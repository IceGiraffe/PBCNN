20-03-22 13:52-INFO-{'session_length': 24, 'height': 32, 'width': 32, 'num_labels': 8, 'learning_rate': 0.0005, 'filter_sizes': [3, 4, 5, 6], 'num_filters': 64, 'filter_sizes_hierarchical': [3, 4, 5], 'num_fitlers_hierarchical': 64, 'is_train': True, 'early_stop': True, 'is_pretrain': True, 'is_time': True, 'is_size': False, 'uncertainty': False, 'is_tuning': False}
20-03-22 13:52-WARNING-From ../utils.py:127: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

20-03-22 13:52-WARNING-From ../model/train.py:79: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.

20-03-22 13:52-WARNING-From ../model/base_model.py:46: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

20-03-22 13:52-WARNING-From ../model/hierarchical_model.py:46: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

20-03-22 13:52-WARNING-From ../model/hierarchical_model.py:46: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.

20-03-22 13:52-WARNING-From ../model/utils/utils.py:25: conv1d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv1D` instead.
20-03-22 13:52-WARNING-Entity <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7fd6d81572d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7fd6d81572d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-22 13:52-WARNING-From ../model/utils/utils.py:44: max_pooling1d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.MaxPooling1D instead.
20-03-22 13:52-WARNING-Entity <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7fd6d81577d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7fd6d81577d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-22 13:52-WARNING-Entity <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7fd6d81100d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7fd6d81100d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-22 13:52-WARNING-Entity <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7fd6d7544910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7fd6d7544910>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-22 13:52-WARNING-Entity <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7fd6d7544910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7fd6d7544910>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-22 13:52-WARNING-Entity <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7fd6d756c910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7fd6d756c910>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-22 13:52-WARNING-Entity <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7fd6d7544d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7fd6d7544d90>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-22 13:52-WARNING-Entity <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7fd6d75399d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7fd6d75399d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-22 13:52-WARNING-From ../model/utils/modules.py:207: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.dropout instead.
20-03-22 13:52-WARNING-Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fd6d754ae50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fd6d754ae50>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-22 13:52-WARNING-Variable *= will be deprecated. Use `var.assign(var * other)` if you want assignment to the variable value or `x = x * y` if you want a new python Tensor object.
20-03-22 13:52-WARNING-Entity <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7fd6d7544f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7fd6d7544f10>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-22 13:52-WARNING-Entity <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7fd6d80a8d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7fd6d80a8d90>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-22 13:52-WARNING-Entity <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7fd6d756ccd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7fd6d756ccd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-22 13:52-WARNING-Entity <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7fd6d7539a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7fd6d7539a90>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-22 13:52-WARNING-Entity <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7fd6d80cf410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7fd6d80cf410>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-22 13:52-WARNING-Entity <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7fd6d74c2850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7fd6d74c2850>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-22 13:52-WARNING-From ../model/utils/modules.py:242: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.dense instead.
20-03-22 13:52-WARNING-Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fd6d74d5090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fd6d74d5090>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-22 13:52-WARNING-From ../model/utils/modules.py:244: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).
20-03-22 13:52-WARNING-Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7fd6d80cfa90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7fd6d80cfa90>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-22 13:52-WARNING-From ../model/utils/modules.py:247: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
20-03-22 13:52-WARNING-Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fd6d756d690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fd6d756d690>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-22 13:52-WARNING-Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fd6d7438e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fd6d7438e90>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-22 13:52-WARNING-From ../model/base_model.py:132: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.

20-03-22 13:52-WARNING-From /root/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/clip_ops.py:286: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
20-03-22 13:52-WARNING-From ../model/train.py:90: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.

20-03-22 13:54-INFO-Epoch 0, Batch 100, Global step 100:
20-03-22 13:54-INFO-training batch loss: 0.1942; avg_loss: 5.6493
20-03-22 13:54-INFO-
20-03-22 13:56-INFO-Epoch 0, Batch 200, Global step 200:
20-03-22 13:56-INFO-training batch loss: 0.0956; avg_loss: 3.0308
20-03-22 13:56-INFO-
20-03-22 13:58-INFO-Epoch 0, Batch 300, Global step 300:
20-03-22 13:58-INFO-training batch loss: 3.6653; avg_loss: 2.6409
20-03-22 13:58-INFO-
20-03-22 14:00-INFO-Epoch 0, Batch 400, Global step 400:
20-03-22 14:00-INFO-training batch loss: 2.4374; avg_loss: 2.1071
20-03-22 14:00-INFO-
20-03-22 14:02-INFO-Epoch 0, Batch 500, Global step 500:
20-03-22 14:02-INFO-training batch loss: 1.1703; avg_loss: 2.9043
20-03-22 14:02-INFO-
20-03-22 14:04-INFO-Epoch 0, Batch 600, Global step 600:
20-03-22 14:04-INFO-training batch loss: 0.7446; avg_loss: 2.5131
20-03-22 14:04-INFO-
20-03-22 14:06-INFO-Epoch 0, Batch 700, Global step 700:
20-03-22 14:06-INFO-training batch loss: 0.8704; avg_loss: 2.2302
20-03-22 14:06-INFO-
20-03-22 14:08-INFO-Epoch 0, Batch 800, Global step 800:
20-03-22 14:08-INFO-training batch loss: 0.4312; avg_loss: 2.0124
20-03-22 14:08-INFO-
20-03-22 14:10-INFO-Epoch 0, Batch 900, Global step 900:
20-03-22 14:10-INFO-training batch loss: 0.0242; avg_loss: 1.8501
20-03-22 14:10-INFO-
20-03-22 14:12-INFO-Epoch 0, Batch 1000, Global step 1000:
20-03-22 14:12-INFO-training batch loss: 0.0780; avg_loss: 1.7218
20-03-22 14:12-INFO-
20-03-22 14:14-INFO-Epoch 0, Batch 1100, Global step 1100:
20-03-22 14:14-INFO-training batch loss: 0.2764; avg_loss: 1.6195
20-03-22 14:14-INFO-
20-03-22 14:16-INFO-Epoch 0, Batch 1200, Global step 1200:
20-03-22 14:16-INFO-training batch loss: 0.1916; avg_loss: 1.5416
20-03-22 14:16-INFO-
20-03-22 14:18-INFO-Epoch 0, Batch 1300, Global step 1300:
20-03-22 14:18-INFO-training batch loss: 0.2035; avg_loss: 1.4682
20-03-22 14:18-INFO-
20-03-22 14:20-INFO-Epoch 0, Batch 1400, Global step 1400:
20-03-22 14:20-INFO-training batch loss: 0.5799; avg_loss: 1.4206
20-03-22 14:20-INFO-
20-03-22 14:22-INFO-Epoch 0, Batch 1500, Global step 1500:
20-03-22 14:22-INFO-training batch loss: 0.6077; avg_loss: 1.3599
20-03-22 14:22-INFO-
20-03-22 14:24-INFO-Epoch 0, Batch 1600, Global step 1600:
20-03-22 14:24-INFO-training batch loss: 0.1311; avg_loss: 1.3329
20-03-22 14:24-INFO-
20-03-22 14:26-INFO-Epoch 0, Batch 1700, Global step 1700:
20-03-22 14:26-INFO-training batch loss: 0.0268; avg_loss: 1.2943
20-03-22 14:26-INFO-
20-03-22 14:28-INFO-Epoch 0, Batch 1800, Global step 1800:
20-03-22 14:28-INFO-training batch loss: 1.6956; avg_loss: 1.2520
20-03-22 14:28-INFO-
20-03-22 14:30-INFO-Epoch 0, Batch 1900, Global step 1900:
20-03-22 14:30-INFO-training batch loss: 0.0678; avg_loss: 1.2633
20-03-22 14:30-INFO-
20-03-22 14:32-INFO-Epoch 0, Batch 2000, Global step 2000:
20-03-22 14:32-INFO-training batch loss: 0.2862; avg_loss: 1.2277
20-03-22 14:32-INFO-
20-03-22 14:35-INFO-Epoch 0, evaluating batch loss: 0.1039; avg_loss: 0.9498
20-03-22 14:35-INFO-
20-03-22 14:36-INFO-Epoch 1, Batch 16, Global step 2100:
20-03-22 14:36-INFO-training batch loss: 0.1243; avg_loss: 0.2172
20-03-22 14:36-INFO-
20-03-22 14:38-INFO-Epoch 1, Batch 116, Global step 2200:
20-03-22 14:38-INFO-training batch loss: 0.3023; avg_loss: 4.0537
20-03-22 14:38-INFO-
20-03-22 14:39-INFO-Epoch 1, Batch 216, Global step 2300:
20-03-22 14:39-INFO-training batch loss: 0.0455; avg_loss: 2.3839
20-03-22 14:39-INFO-
20-03-22 14:41-INFO-Epoch 1, Batch 316, Global step 2400:
20-03-22 14:41-INFO-training batch loss: 0.0485; avg_loss: 2.2118
20-03-22 14:41-INFO-
20-03-22 14:43-INFO-Epoch 1, Batch 416, Global step 2500:
20-03-22 14:43-INFO-training batch loss: 0.1447; avg_loss: 1.8078
20-03-22 14:43-INFO-
20-03-22 14:45-INFO-Epoch 1, Batch 516, Global step 2600:
20-03-22 14:45-INFO-training batch loss: 0.0533; avg_loss: 2.6408
20-03-22 14:45-INFO-
20-03-22 14:47-INFO-Epoch 1, Batch 616, Global step 2700:
20-03-22 14:47-INFO-training batch loss: 0.0388; avg_loss: 2.2975
20-03-22 14:47-INFO-
20-03-22 14:49-INFO-Epoch 1, Batch 716, Global step 2800:
20-03-22 14:49-INFO-training batch loss: 0.0247; avg_loss: 2.0476
20-03-22 14:49-INFO-
20-03-22 14:51-INFO-Epoch 1, Batch 816, Global step 2900:
20-03-22 14:51-INFO-training batch loss: 0.0928; avg_loss: 1.8578
20-03-22 14:51-INFO-
20-03-22 14:53-INFO-Epoch 1, Batch 916, Global step 3000:
20-03-22 14:53-INFO-training batch loss: 0.1580; avg_loss: 1.7205
20-03-22 14:53-INFO-
20-03-22 14:55-INFO-Epoch 1, Batch 1016, Global step 3100:
20-03-22 14:55-INFO-training batch loss: 0.0644; avg_loss: 1.6031
20-03-22 14:55-INFO-
20-03-22 14:57-INFO-Epoch 1, Batch 1116, Global step 3200:
20-03-22 14:57-INFO-training batch loss: 0.1383; avg_loss: 1.5109
20-03-22 14:57-INFO-
20-03-22 14:59-INFO-Epoch 1, Batch 1216, Global step 3300:
20-03-22 14:59-INFO-training batch loss: 0.0713; avg_loss: 1.4528
20-03-22 14:59-INFO-
20-03-22 15:02-INFO-Epoch 1, Batch 1316, Global step 3400:
20-03-22 15:02-INFO-training batch loss: 1.3951; avg_loss: 1.3823
20-03-22 15:02-INFO-
20-03-22 15:04-INFO-Epoch 1, Batch 1416, Global step 3500:
20-03-22 15:04-INFO-training batch loss: 0.4634; avg_loss: 1.3423
20-03-22 15:04-INFO-
20-03-22 15:06-INFO-Epoch 1, Batch 1516, Global step 3600:
20-03-22 15:06-INFO-training batch loss: 0.3045; avg_loss: 1.2859
20-03-22 15:06-INFO-
20-03-22 15:08-INFO-Epoch 1, Batch 1616, Global step 3700:
20-03-22 15:08-INFO-training batch loss: 0.2676; avg_loss: 1.2653
20-03-22 15:08-INFO-
20-03-22 15:10-INFO-Epoch 1, Batch 1716, Global step 3800:
20-03-22 15:10-INFO-training batch loss: 0.0878; avg_loss: 1.2284
20-03-22 15:10-INFO-
20-03-22 15:12-INFO-Epoch 1, Batch 1816, Global step 3900:
20-03-22 15:12-INFO-training batch loss: 0.2329; avg_loss: 1.1872
20-03-22 15:12-INFO-
20-03-22 15:14-INFO-Epoch 1, Batch 1916, Global step 4000:
20-03-22 15:14-INFO-training batch loss: 0.0563; avg_loss: 1.2042
20-03-22 15:14-INFO-
20-03-22 15:16-INFO-Epoch 1, Batch 2016, Global step 4100:
20-03-22 15:16-INFO-training batch loss: 0.1504; avg_loss: 1.1712
20-03-22 15:16-INFO-
20-03-22 15:19-INFO-Epoch 1, evaluating batch loss: 0.1039; avg_loss: 0.9502
20-03-22 15:19-INFO-
20-03-22 15:20-INFO-Epoch 2, Batch 32, Global step 4200:
20-03-22 15:20-INFO-training batch loss: 0.0248; avg_loss: 10.0710
20-03-22 15:20-INFO-
20-03-22 15:22-INFO-Epoch 2, Batch 132, Global step 4300:
20-03-22 15:22-INFO-training batch loss: 0.3699; avg_loss: 3.5973
20-03-22 15:22-INFO-
20-03-22 15:24-INFO-Epoch 2, Batch 232, Global step 4400:
20-03-22 15:24-INFO-training batch loss: 7.1713; avg_loss: 2.3266
20-03-22 15:24-INFO-
20-03-22 15:26-INFO-Epoch 2, Batch 332, Global step 4500:
20-03-22 15:26-INFO-training batch loss: 0.1062; avg_loss: 2.1183
20-03-22 15:26-INFO-
20-03-22 15:28-INFO-Epoch 2, Batch 432, Global step 4600:
20-03-22 15:28-INFO-training batch loss: 0.1525; avg_loss: 1.7615
20-03-22 15:28-INFO-
20-03-22 15:30-INFO-Epoch 2, Batch 532, Global step 4700:
20-03-22 15:30-INFO-training batch loss: 0.0607; avg_loss: 2.5794
20-03-22 15:30-INFO-
20-03-22 15:32-INFO-Epoch 2, Batch 632, Global step 4800:
20-03-22 15:32-INFO-training batch loss: 3.5041; avg_loss: 2.2523
20-03-22 15:32-INFO-
20-03-22 15:34-INFO-Epoch 2, Batch 732, Global step 4900:
20-03-22 15:34-INFO-training batch loss: 0.2520; avg_loss: 2.0095
20-03-22 15:34-INFO-
20-03-22 15:36-INFO-Epoch 2, Batch 832, Global step 5000:
20-03-22 15:36-INFO-training batch loss: 0.0827; avg_loss: 1.8314
20-03-22 15:36-INFO-
20-03-22 15:38-INFO-Epoch 2, Batch 932, Global step 5100:
20-03-22 15:38-INFO-training batch loss: 0.2451; avg_loss: 1.7056
20-03-22 15:38-INFO-
20-03-22 15:40-INFO-Epoch 2, Batch 1032, Global step 5200:
20-03-22 15:40-INFO-training batch loss: 0.0426; avg_loss: 1.5892
20-03-22 15:40-INFO-
20-03-22 15:42-INFO-Epoch 2, Batch 1132, Global step 5300:
20-03-22 15:42-INFO-training batch loss: 0.1283; avg_loss: 1.4988
20-03-22 15:42-INFO-
20-03-22 15:44-INFO-Epoch 2, Batch 1232, Global step 5400:
20-03-22 15:44-INFO-training batch loss: 0.3294; avg_loss: 1.4384
20-03-22 15:44-INFO-
20-03-22 15:46-INFO-Epoch 2, Batch 1332, Global step 5500:
20-03-22 15:46-INFO-training batch loss: 2.8362; avg_loss: 1.3756
20-03-22 15:46-INFO-
20-03-22 15:48-INFO-Epoch 2, Batch 1432, Global step 5600:
20-03-22 15:48-INFO-training batch loss: 0.0654; avg_loss: 1.3317
20-03-22 15:48-INFO-
20-03-22 15:50-INFO-Epoch 2, Batch 1532, Global step 5700:
20-03-22 15:50-INFO-training batch loss: 2.5355; avg_loss: 1.2848
20-03-22 15:50-INFO-
20-03-22 15:52-INFO-Epoch 2, Batch 1632, Global step 5800:
20-03-22 15:52-INFO-training batch loss: 0.2571; avg_loss: 1.2560
20-03-22 15:52-INFO-
20-03-22 15:54-INFO-Epoch 2, Batch 1732, Global step 5900:
20-03-22 15:54-INFO-training batch loss: 2.4813; avg_loss: 1.2208
20-03-22 15:54-INFO-
20-03-22 15:56-INFO-Epoch 2, Batch 1832, Global step 6000:
20-03-22 15:56-INFO-training batch loss: 0.9323; avg_loss: 1.2409
20-03-22 15:56-INFO-
20-03-22 15:58-INFO-Epoch 2, Batch 1932, Global step 6100:
20-03-22 15:58-INFO-training batch loss: 0.4356; avg_loss: 1.1982
20-03-22 15:58-INFO-
20-03-22 16:00-INFO-Epoch 2, Batch 2032, Global step 6200:
20-03-22 16:00-INFO-training batch loss: 0.0310; avg_loss: 1.1739
20-03-22 16:00-INFO-
20-03-22 16:03-INFO-Epoch 2, evaluating batch loss: 0.1039; avg_loss: 0.9503
20-03-22 16:03-INFO-
