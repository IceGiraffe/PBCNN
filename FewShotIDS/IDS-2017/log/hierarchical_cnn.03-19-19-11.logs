20-03-19 19:11-INFO-{'session_length': 24, 'height': 32, 'width': 32, 'num_labels': 13, 'learning_rate': 0.0005, 'filter_sizes': [3, 4, 5, 6], 'num_filters': 64, 'filter_sizes_hierarchical': [3, 4, 5], 'num_fitlers_hierarchical': 64, 'is_train': True, 'early_stop': True, 'is_pretrain': False, 'is_time': False, 'is_size': False, 'uncertainty': False}
20-03-19 19:11-WARNING-From ../utils.py:123: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

20-03-19 19:11-WARNING-From ../model/train.py:79: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.

20-03-19 19:11-WARNING-From ../model/base_model.py:46: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

20-03-19 19:11-WARNING-From ../model/hierarchical_model.py:45: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

20-03-19 19:11-WARNING-From ../model/hierarchical_model.py:45: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.

20-03-19 19:11-WARNING-From ../model/utils/utils.py:25: conv1d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv1D` instead.
20-03-19 19:11-WARNING-Entity <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7efed4a1f510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7efed4a1f510>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-19 19:11-WARNING-From ../model/utils/utils.py:44: max_pooling1d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.MaxPooling1D instead.
20-03-19 19:11-WARNING-Entity <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7efed4a1f610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7efed4a1f610>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-19 19:11-WARNING-Entity <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7efed4a23090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7efed4a23090>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-19 19:11-WARNING-Entity <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7efed4a230d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7efed4a230d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-19 19:11-WARNING-Entity <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7efed4a230d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7efed4a230d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-19 19:11-WARNING-Entity <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7efed49da290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7efed49da290>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-19 19:11-WARNING-Entity <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7efed49eed50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7efed49eed50>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-19 19:11-WARNING-Entity <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7efed4a1f1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7efed4a1f1d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-19 19:11-WARNING-From ../model/utils/modules.py:207: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.dropout instead.
20-03-19 19:11-WARNING-Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7efed4a23150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7efed4a23150>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-19 19:11-WARNING-Variable *= will be deprecated. Use `var.assign(var * other)` if you want assignment to the variable value or `x = x * y` if you want a new python Tensor object.
20-03-19 19:11-WARNING-Entity <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7efed49da450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7efed49da450>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-19 19:11-WARNING-Entity <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7efed3dbb650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7efed3dbb650>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-19 19:11-WARNING-Entity <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7efed3dbbdd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7efed3dbbdd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-19 19:11-WARNING-Entity <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7efed3e0fe90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7efed3e0fe90>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-19 19:11-WARNING-Entity <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7efed3e0fe90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7efed3e0fe90>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-19 19:11-WARNING-Entity <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7efee1410490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7efee1410490>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-19 19:11-WARNING-From ../model/utils/modules.py:242: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.dense instead.
20-03-19 19:11-WARNING-Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7efed3dba410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7efed3dba410>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-19 19:11-WARNING-From ../model/utils/modules.py:244: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).
20-03-19 19:11-WARNING-Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7efee1410490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7efee1410490>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-19 19:11-WARNING-From ../model/utils/modules.py:247: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
20-03-19 19:11-WARNING-Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7efed3d84cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7efed3d84cd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-19 19:11-WARNING-Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7efed3d84cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7efed3d84cd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-19 19:11-WARNING-From ../model/base_model.py:132: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.

20-03-19 19:11-WARNING-From /root/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/clip_ops.py:286: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
20-03-19 19:11-WARNING-From ../model/train.py:90: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.

20-03-19 19:14-INFO-Epoch 0, Batch 100, Global step 100:
20-03-19 19:14-INFO-training batch loss: 0.3439; avg_loss: 0.5658
20-03-19 19:14-INFO-training batch accuracy: 0.8672; avg_accuracy: 0.8198
20-03-19 19:14-INFO-
20-03-19 19:16-INFO-Epoch 0, Batch 200, Global step 200:
20-03-19 19:16-INFO-training batch loss: 0.1659; avg_loss: 0.3894
20-03-19 19:16-INFO-training batch accuracy: 0.9609; avg_accuracy: 0.8781
20-03-19 19:16-INFO-
20-03-19 19:18-INFO-Epoch 0, Batch 300, Global step 300:
20-03-19 19:18-INFO-training batch loss: 0.1563; avg_loss: 0.3208
20-03-19 19:18-INFO-training batch accuracy: 0.9453; avg_accuracy: 0.9004
20-03-19 19:18-INFO-
20-03-19 19:20-INFO-Epoch 0, Batch 400, Global step 400:
20-03-19 19:20-INFO-training batch loss: 0.0962; avg_loss: 0.2775
20-03-19 19:20-INFO-training batch accuracy: 0.9609; avg_accuracy: 0.9129
20-03-19 19:20-INFO-
20-03-19 19:23-INFO-Epoch 0, Batch 500, Global step 500:
20-03-19 19:23-INFO-training batch loss: 0.0602; avg_loss: 0.2505
20-03-19 19:23-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9203
20-03-19 19:23-INFO-
20-03-19 19:25-INFO-Epoch 0, Batch 600, Global step 600:
20-03-19 19:25-INFO-training batch loss: 0.1329; avg_loss: 0.2306
20-03-19 19:25-INFO-training batch accuracy: 0.9453; avg_accuracy: 0.9254
20-03-19 19:25-INFO-
20-03-19 19:27-INFO-Epoch 0, Batch 700, Global step 700:
20-03-19 19:27-INFO-training batch loss: 0.1453; avg_loss: 0.2157
20-03-19 19:27-INFO-training batch accuracy: 0.9375; avg_accuracy: 0.9297
20-03-19 19:27-INFO-
20-03-19 19:29-INFO-Epoch 0, Batch 800, Global step 800:
20-03-19 19:29-INFO-training batch loss: 0.1803; avg_loss: 0.2033
20-03-19 19:29-INFO-training batch accuracy: 0.9609; avg_accuracy: 0.9329
20-03-19 19:29-INFO-
20-03-19 19:31-INFO-Epoch 0, Batch 900, Global step 900:
20-03-19 19:31-INFO-training batch loss: 0.2414; avg_loss: 0.1941
20-03-19 19:31-INFO-training batch accuracy: 0.9297; avg_accuracy: 0.9351
20-03-19 19:31-INFO-
20-03-19 19:33-INFO-Epoch 0, Batch 1000, Global step 1000:
20-03-19 19:33-INFO-training batch loss: 0.0427; avg_loss: 0.1863
20-03-19 19:33-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9369
20-03-19 19:33-INFO-
20-03-19 19:36-INFO-Epoch 0, Batch 1100, Global step 1100:
20-03-19 19:36-INFO-training batch loss: 0.1332; avg_loss: 0.1794
20-03-19 19:36-INFO-training batch accuracy: 0.9453; avg_accuracy: 0.9388
20-03-19 19:36-INFO-
20-03-19 19:38-INFO-Epoch 0, Batch 1200, Global step 1200:
20-03-19 19:38-INFO-training batch loss: 0.1405; avg_loss: 0.1731
20-03-19 19:38-INFO-training batch accuracy: 0.9453; avg_accuracy: 0.9403
20-03-19 19:38-INFO-
20-03-19 19:40-INFO-Epoch 0, Batch 1300, Global step 1300:
20-03-19 19:40-INFO-training batch loss: 0.1273; avg_loss: 0.1687
20-03-19 19:40-INFO-training batch accuracy: 0.9453; avg_accuracy: 0.9414
20-03-19 19:40-INFO-
20-03-19 19:42-INFO-Epoch 0, Batch 1400, Global step 1400:
20-03-19 19:42-INFO-training batch loss: 0.1001; avg_loss: 0.1641
20-03-19 19:42-INFO-training batch accuracy: 0.9609; avg_accuracy: 0.9426
20-03-19 19:42-INFO-
20-03-19 19:44-INFO-Epoch 0, Batch 1500, Global step 1500:
20-03-19 19:44-INFO-training batch loss: 0.1182; avg_loss: 0.1598
20-03-19 19:44-INFO-training batch accuracy: 0.9531; avg_accuracy: 0.9437
20-03-19 19:44-INFO-
20-03-19 19:46-INFO-Epoch 0, Batch 1600, Global step 1600:
20-03-19 19:46-INFO-training batch loss: 0.0559; avg_loss: 0.1561
20-03-19 19:46-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9444
20-03-19 19:46-INFO-
20-03-19 19:48-INFO-Epoch 0, Batch 1700, Global step 1700:
20-03-19 19:48-INFO-training batch loss: 0.0442; avg_loss: 0.1528
20-03-19 19:48-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9451
20-03-19 19:48-INFO-
20-03-19 19:51-INFO-Epoch 0, Batch 1800, Global step 1800:
20-03-19 19:51-INFO-training batch loss: 0.0932; avg_loss: 0.1498
20-03-19 19:51-INFO-training batch accuracy: 0.9531; avg_accuracy: 0.9457
20-03-19 19:51-INFO-
20-03-19 19:53-INFO-Epoch 0, Batch 1900, Global step 1900:
20-03-19 19:53-INFO-training batch loss: 0.0529; avg_loss: 0.1471
20-03-19 19:53-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9462
20-03-19 19:53-INFO-
20-03-19 19:55-INFO-Epoch 0, Batch 2000, Global step 2000:
20-03-19 19:55-INFO-training batch loss: 0.1411; avg_loss: 0.1443
20-03-19 19:55-INFO-training batch accuracy: 0.9375; avg_accuracy: 0.9469
20-03-19 19:55-INFO-
20-03-19 19:57-INFO-training batch accuracy: 0.9048; avg_accuracy: 0.9478
20-03-19 19:57-INFO-
20-03-19 19:59-INFO-Epoch 0, evaluating batch loss: 0.1372; avg_loss: 0.1864
20-03-19 19:59-INFO-evaluating batch accuracy: 0.9615; avg_accuracy: 0.9238

20-03-19 19:59-INFO-
20-03-19 19:59-INFO-Epoch 1, Batch 16, Global step 2100:
20-03-19 19:59-INFO-training batch loss: 0.0836; avg_loss: 0.0853
20-03-19 19:59-INFO-training batch accuracy: 0.9609; avg_accuracy: 0.9019
20-03-19 19:59-INFO-
20-03-19 20:01-INFO-Epoch 1, Batch 116, Global step 2200:
20-03-19 20:01-INFO-training batch loss: 0.0695; avg_loss: 0.0935
20-03-19 20:01-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9499
20-03-19 20:01-INFO-
20-03-19 20:04-INFO-Epoch 1, Batch 216, Global step 2300:
20-03-19 20:04-INFO-training batch loss: 0.0611; avg_loss: 0.0940
20-03-19 20:04-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9533
20-03-19 20:04-INFO-
20-03-19 20:06-INFO-Epoch 1, Batch 316, Global step 2400:
20-03-19 20:06-INFO-training batch loss: 0.1420; avg_loss: 0.0932
20-03-19 20:06-INFO-training batch accuracy: 0.9375; avg_accuracy: 0.9549
20-03-19 20:06-INFO-
20-03-19 20:08-INFO-Epoch 1, Batch 416, Global step 2500:
20-03-19 20:08-INFO-training batch loss: 0.0615; avg_loss: 0.0921
20-03-19 20:08-INFO-training batch accuracy: 0.9531; avg_accuracy: 0.9558
20-03-19 20:08-INFO-
20-03-19 20:10-INFO-Epoch 1, Batch 516, Global step 2600:
20-03-19 20:10-INFO-training batch loss: 0.0705; avg_loss: 0.0920
20-03-19 20:10-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9566
20-03-19 20:10-INFO-
20-03-19 20:12-INFO-Epoch 1, Batch 616, Global step 2700:
20-03-19 20:12-INFO-training batch loss: 0.0930; avg_loss: 0.0921
20-03-19 20:12-INFO-training batch accuracy: 0.9609; avg_accuracy: 0.9568
20-03-19 20:12-INFO-
20-03-19 20:15-INFO-Epoch 1, Batch 716, Global step 2800:
20-03-19 20:15-INFO-training batch loss: 0.0647; avg_loss: 0.0917
20-03-19 20:15-INFO-training batch accuracy: 0.9609; avg_accuracy: 0.9572
20-03-19 20:15-INFO-
20-03-19 20:17-INFO-Epoch 1, Batch 816, Global step 2900:
20-03-19 20:17-INFO-training batch loss: 0.0980; avg_loss: 0.0914
20-03-19 20:17-INFO-training batch accuracy: 0.9609; avg_accuracy: 0.9576
20-03-19 20:17-INFO-
20-03-19 20:19-INFO-Epoch 1, Batch 916, Global step 3000:
20-03-19 20:19-INFO-training batch loss: 0.0742; avg_loss: 0.0914
20-03-19 20:19-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9577
20-03-19 20:19-INFO-
20-03-19 20:21-INFO-Epoch 1, Batch 1016, Global step 3100:
20-03-19 20:21-INFO-training batch loss: 0.0941; avg_loss: 0.0911
20-03-19 20:21-INFO-training batch accuracy: 0.9531; avg_accuracy: 0.9579
20-03-19 20:21-INFO-
20-03-19 20:24-INFO-Epoch 1, Batch 1116, Global step 3200:
20-03-19 20:24-INFO-training batch loss: 0.0853; avg_loss: 0.0909
20-03-19 20:24-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9582
20-03-19 20:24-INFO-
20-03-19 20:26-INFO-Epoch 1, Batch 1216, Global step 3300:
20-03-19 20:26-INFO-training batch loss: 0.0593; avg_loss: 0.0904
20-03-19 20:26-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9584
20-03-19 20:26-INFO-
20-03-19 20:28-INFO-Epoch 1, Batch 1316, Global step 3400:
20-03-19 20:28-INFO-training batch loss: 0.0894; avg_loss: 0.0906
20-03-19 20:28-INFO-training batch accuracy: 0.9375; avg_accuracy: 0.9584
20-03-19 20:28-INFO-
20-03-19 20:30-INFO-Epoch 1, Batch 1416, Global step 3500:
20-03-19 20:30-INFO-training batch loss: 0.0943; avg_loss: 0.0901
20-03-19 20:30-INFO-training batch accuracy: 0.9375; avg_accuracy: 0.9586
20-03-19 20:30-INFO-
20-03-19 20:33-INFO-Epoch 1, Batch 1516, Global step 3600:
20-03-19 20:33-INFO-training batch loss: 0.1247; avg_loss: 0.0899
20-03-19 20:33-INFO-training batch accuracy: 0.9141; avg_accuracy: 0.9586
20-03-19 20:33-INFO-
20-03-19 20:35-INFO-Epoch 1, Batch 1616, Global step 3700:
20-03-19 20:35-INFO-training batch loss: 0.0720; avg_loss: 0.0892
20-03-19 20:35-INFO-training batch accuracy: 0.9609; avg_accuracy: 0.9588
20-03-19 20:35-INFO-
20-03-19 20:37-INFO-Epoch 1, Batch 1716, Global step 3800:
20-03-19 20:37-INFO-training batch loss: 0.0285; avg_loss: 0.0889
20-03-19 20:37-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9589
20-03-19 20:37-INFO-
20-03-19 20:39-INFO-Epoch 1, Batch 1816, Global step 3900:
20-03-19 20:39-INFO-training batch loss: 0.0611; avg_loss: 0.0887
20-03-19 20:39-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9589
20-03-19 20:39-INFO-
20-03-19 20:42-INFO-Epoch 1, Batch 1916, Global step 4000:
20-03-19 20:42-INFO-training batch loss: 0.0528; avg_loss: 0.0884
20-03-19 20:42-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9591
20-03-19 20:42-INFO-
20-03-19 20:44-INFO-Epoch 1, Batch 2016, Global step 4100:
20-03-19 20:44-INFO-training batch loss: 0.0786; avg_loss: 0.0881
20-03-19 20:44-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9593
20-03-19 20:44-INFO-
20-03-19 20:45-INFO-training batch accuracy: 0.9048; avg_accuracy: 0.9598
20-03-19 20:45-INFO-
20-03-19 20:48-INFO-Epoch 1, evaluating batch loss: 0.1040; avg_loss: 0.1602
20-03-19 20:48-INFO-evaluating batch accuracy: 0.9615; avg_accuracy: 0.9254

20-03-19 20:48-INFO-
20-03-19 20:48-INFO-Epoch 2, Batch 32, Global step 4200:
20-03-19 20:48-INFO-training batch loss: 0.0801; avg_loss: 0.0761
20-03-19 20:48-INFO-training batch accuracy: 0.9531; avg_accuracy: 0.9338
20-03-19 20:48-INFO-
20-03-19 20:50-INFO-Epoch 2, Batch 132, Global step 4300:
20-03-19 20:50-INFO-training batch loss: 0.0944; avg_loss: 0.0799
20-03-19 20:50-INFO-training batch accuracy: 0.9609; avg_accuracy: 0.9541
20-03-19 20:50-INFO-
20-03-19 20:53-INFO-Epoch 2, Batch 232, Global step 4400:
20-03-19 20:53-INFO-training batch loss: 0.0362; avg_loss: 0.0801
20-03-19 20:53-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9575
20-03-19 20:53-INFO-
20-03-19 20:55-INFO-Epoch 2, Batch 332, Global step 4500:
20-03-19 20:55-INFO-training batch loss: 0.0352; avg_loss: 0.0796
20-03-19 20:55-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9592
20-03-19 20:55-INFO-
20-03-19 20:57-INFO-Epoch 2, Batch 432, Global step 4600:
20-03-19 20:57-INFO-training batch loss: 0.1177; avg_loss: 0.0795
20-03-19 20:57-INFO-training batch accuracy: 0.9453; avg_accuracy: 0.9598
20-03-19 20:57-INFO-
20-03-19 20:59-INFO-Epoch 2, Batch 532, Global step 4700:
20-03-19 20:59-INFO-training batch loss: 0.1322; avg_loss: 0.0794
20-03-19 20:59-INFO-training batch accuracy: 0.9297; avg_accuracy: 0.9607
20-03-19 20:59-INFO-
20-03-19 21:02-INFO-Epoch 2, Batch 632, Global step 4800:
20-03-19 21:02-INFO-training batch loss: 0.1257; avg_loss: 0.0801
20-03-19 21:02-INFO-training batch accuracy: 0.9297; avg_accuracy: 0.9607
20-03-19 21:02-INFO-
20-03-19 21:04-INFO-Epoch 2, Batch 732, Global step 4900:
20-03-19 21:04-INFO-training batch loss: 0.0700; avg_loss: 0.0800
20-03-19 21:04-INFO-training batch accuracy: 0.9531; avg_accuracy: 0.9610
20-03-19 21:04-INFO-
20-03-19 21:06-INFO-Epoch 2, Batch 832, Global step 5000:
20-03-19 21:06-INFO-training batch loss: 0.0839; avg_loss: 0.0798
20-03-19 21:06-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9612
20-03-19 21:06-INFO-
20-03-19 21:08-INFO-Epoch 2, Batch 932, Global step 5100:
20-03-19 21:08-INFO-training batch loss: 0.1217; avg_loss: 0.0798
20-03-19 21:08-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9612
20-03-19 21:08-INFO-
20-03-19 21:11-INFO-Epoch 2, Batch 1032, Global step 5200:
20-03-19 21:11-INFO-training batch loss: 0.0731; avg_loss: 0.0798
20-03-19 21:11-INFO-training batch accuracy: 0.9609; avg_accuracy: 0.9614
20-03-19 21:11-INFO-
20-03-19 21:13-INFO-Epoch 2, Batch 1132, Global step 5300:
20-03-19 21:13-INFO-training batch loss: 0.0761; avg_loss: 0.0797
20-03-19 21:13-INFO-training batch accuracy: 0.9453; avg_accuracy: 0.9616
20-03-19 21:13-INFO-
20-03-19 21:15-INFO-Epoch 2, Batch 1232, Global step 5400:
20-03-19 21:15-INFO-training batch loss: 0.0490; avg_loss: 0.0793
20-03-19 21:15-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9619
20-03-19 21:15-INFO-
20-03-19 21:18-INFO-Epoch 2, Batch 1332, Global step 5500:
20-03-19 21:18-INFO-training batch loss: 0.0736; avg_loss: 0.0792
20-03-19 21:18-INFO-training batch accuracy: 0.9609; avg_accuracy: 0.9619
20-03-19 21:18-INFO-
20-03-19 21:20-INFO-Epoch 2, Batch 1432, Global step 5600:
20-03-19 21:20-INFO-training batch loss: 0.1073; avg_loss: 0.0789
20-03-19 21:20-INFO-training batch accuracy: 0.9453; avg_accuracy: 0.9621
20-03-19 21:20-INFO-
20-03-19 21:22-INFO-Epoch 2, Batch 1532, Global step 5700:
20-03-19 21:22-INFO-training batch loss: 0.0820; avg_loss: 0.0787
20-03-19 21:22-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9622
20-03-19 21:22-INFO-
20-03-19 21:24-INFO-Epoch 2, Batch 1632, Global step 5800:
20-03-19 21:24-INFO-training batch loss: 0.0703; avg_loss: 0.0780
20-03-19 21:24-INFO-training batch accuracy: 0.9609; avg_accuracy: 0.9625
20-03-19 21:24-INFO-
20-03-19 21:27-INFO-Epoch 2, Batch 1732, Global step 5900:
20-03-19 21:27-INFO-training batch loss: 0.0485; avg_loss: 0.0779
20-03-19 21:27-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9625
20-03-19 21:27-INFO-
20-03-19 21:29-INFO-Epoch 2, Batch 1832, Global step 6000:
20-03-19 21:29-INFO-training batch loss: 0.0688; avg_loss: 0.0776
20-03-19 21:29-INFO-training batch accuracy: 0.9531; avg_accuracy: 0.9626
20-03-19 21:29-INFO-
20-03-19 21:31-INFO-Epoch 2, Batch 1932, Global step 6100:
20-03-19 21:31-INFO-training batch loss: 0.0607; avg_loss: 0.0772
20-03-19 21:31-INFO-training batch accuracy: 0.9609; avg_accuracy: 0.9629
20-03-19 21:31-INFO-
20-03-19 21:33-INFO-Epoch 2, Batch 2032, Global step 6200:
20-03-19 21:33-INFO-training batch loss: 0.0724; avg_loss: 0.0769
20-03-19 21:33-INFO-training batch accuracy: 0.9531; avg_accuracy: 0.9630
20-03-19 21:33-INFO-
20-03-19 21:35-INFO-training batch accuracy: 0.9048; avg_accuracy: 0.9635
20-03-19 21:35-INFO-
20-03-19 21:37-INFO-Epoch 2, evaluating batch loss: 0.0698; avg_loss: 0.1186
20-03-19 21:37-INFO-evaluating batch accuracy: 0.9615; avg_accuracy: 0.9369

20-03-19 21:37-INFO-
20-03-19 21:38-INFO-Epoch 3, Batch 48, Global step 6300:
20-03-19 21:38-INFO-training batch loss: 0.0864; avg_loss: 0.0628
20-03-19 21:38-INFO-training batch accuracy: 0.9609; avg_accuracy: 0.9526
20-03-19 21:38-INFO-
20-03-19 21:40-INFO-Epoch 3, Batch 148, Global step 6400:
20-03-19 21:40-INFO-training batch loss: 0.0517; avg_loss: 0.0658
20-03-19 21:40-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9629
20-03-19 21:40-INFO-
20-03-19 21:42-INFO-Epoch 3, Batch 248, Global step 6500:
20-03-19 21:42-INFO-training batch loss: 0.0984; avg_loss: 0.0680
20-03-19 21:42-INFO-training batch accuracy: 0.9531; avg_accuracy: 0.9644
20-03-19 21:42-INFO-
20-03-19 21:45-INFO-Epoch 3, Batch 348, Global step 6600:
20-03-19 21:45-INFO-training batch loss: 0.0622; avg_loss: 0.0676
20-03-19 21:45-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9651
20-03-19 21:45-INFO-
20-03-19 21:47-INFO-Epoch 3, Batch 448, Global step 6700:
20-03-19 21:47-INFO-training batch loss: 0.0626; avg_loss: 0.0678
20-03-19 21:47-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9653
20-03-19 21:47-INFO-
20-03-19 21:49-INFO-Epoch 3, Batch 548, Global step 6800:
20-03-19 21:49-INFO-training batch loss: 0.0888; avg_loss: 0.0674
20-03-19 21:49-INFO-training batch accuracy: 0.9453; avg_accuracy: 0.9660
20-03-19 21:49-INFO-
20-03-19 21:51-INFO-Epoch 3, Batch 648, Global step 6900:
20-03-19 21:51-INFO-training batch loss: 0.0484; avg_loss: 0.0673
20-03-19 21:51-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9664
20-03-19 21:51-INFO-
20-03-19 21:54-INFO-Epoch 3, Batch 748, Global step 7000:
20-03-19 21:54-INFO-training batch loss: 0.0489; avg_loss: 0.0669
20-03-19 21:54-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9669
20-03-19 21:54-INFO-
20-03-19 21:56-INFO-Epoch 3, Batch 848, Global step 7100:
20-03-19 21:56-INFO-training batch loss: 0.0529; avg_loss: 0.0661
20-03-19 21:56-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9676
20-03-19 21:56-INFO-
20-03-19 21:58-INFO-Epoch 3, Batch 948, Global step 7200:
20-03-19 21:58-INFO-training batch loss: 0.0704; avg_loss: 0.0656
20-03-19 21:58-INFO-training batch accuracy: 0.9609; avg_accuracy: 0.9678
20-03-19 21:58-INFO-
20-03-19 22:00-INFO-Epoch 3, Batch 1048, Global step 7300:
20-03-19 22:00-INFO-training batch loss: 0.0362; avg_loss: 0.0650
20-03-19 22:00-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9681
20-03-19 22:00-INFO-
20-03-19 22:03-INFO-Epoch 3, Batch 1148, Global step 7400:
20-03-19 22:03-INFO-training batch loss: 0.0641; avg_loss: 0.0640
20-03-19 22:03-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9685
20-03-19 22:03-INFO-
20-03-19 22:05-INFO-Epoch 3, Batch 1248, Global step 7500:
20-03-19 22:05-INFO-training batch loss: 0.0484; avg_loss: 0.0636
20-03-19 22:05-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9686
20-03-19 22:05-INFO-
20-03-19 22:07-INFO-Epoch 3, Batch 1348, Global step 7600:
20-03-19 22:07-INFO-training batch loss: 0.0329; avg_loss: 0.0631
20-03-19 22:07-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9688
20-03-19 22:07-INFO-
20-03-19 22:10-INFO-Epoch 3, Batch 1448, Global step 7700:
20-03-19 22:10-INFO-training batch loss: 0.0478; avg_loss: 0.0623
20-03-19 22:10-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9692
20-03-19 22:10-INFO-
20-03-19 22:12-INFO-Epoch 3, Batch 1548, Global step 7800:
20-03-19 22:12-INFO-training batch loss: 0.0455; avg_loss: 0.0617
20-03-19 22:12-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9695
20-03-19 22:12-INFO-
20-03-19 22:14-INFO-Epoch 3, Batch 1648, Global step 7900:
20-03-19 22:14-INFO-training batch loss: 0.0871; avg_loss: 0.0611
20-03-19 22:14-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9698
20-03-19 22:14-INFO-
20-03-19 22:17-INFO-Epoch 3, Batch 1748, Global step 8000:
20-03-19 22:17-INFO-training batch loss: 0.0419; avg_loss: 0.0604
20-03-19 22:17-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9702
20-03-19 22:17-INFO-
20-03-19 22:19-INFO-Epoch 3, Batch 1848, Global step 8100:
20-03-19 22:19-INFO-training batch loss: 0.0755; avg_loss: 0.0598
20-03-19 22:19-INFO-training batch accuracy: 0.9531; avg_accuracy: 0.9705
20-03-19 22:19-INFO-
20-03-19 22:21-INFO-Epoch 3, Batch 1948, Global step 8200:
20-03-19 22:21-INFO-training batch loss: 0.0345; avg_loss: 0.0590
20-03-19 22:21-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9710
20-03-19 22:21-INFO-
20-03-19 22:23-INFO-Epoch 3, Batch 2048, Global step 8300:
20-03-19 22:23-INFO-training batch loss: 0.0673; avg_loss: 0.0584
20-03-19 22:23-INFO-training batch accuracy: 0.9609; avg_accuracy: 0.9713
20-03-19 22:23-INFO-
20-03-19 22:24-INFO-training batch accuracy: 0.9286; avg_accuracy: 0.9717
20-03-19 22:24-INFO-
20-03-19 22:26-INFO-Epoch 3, evaluating batch loss: 0.0488; avg_loss: 0.0853
20-03-19 22:26-INFO-evaluating batch accuracy: 0.9808; avg_accuracy: 0.9531

20-03-19 22:26-INFO-
20-03-19 22:28-INFO-Epoch 4, Batch 64, Global step 8400:
20-03-19 22:28-INFO-training batch loss: 0.0398; avg_loss: 0.0409
20-03-19 22:28-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9648
20-03-19 22:28-INFO-
20-03-19 22:30-INFO-Epoch 4, Batch 164, Global step 8500:
20-03-19 22:30-INFO-training batch loss: 0.0248; avg_loss: 0.0434
20-03-19 22:30-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9717
20-03-19 22:30-INFO-
20-03-19 22:32-INFO-Epoch 4, Batch 264, Global step 8600:
20-03-19 22:32-INFO-training batch loss: 0.0586; avg_loss: 0.0450
20-03-19 22:32-INFO-training batch accuracy: 0.9609; avg_accuracy: 0.9738
20-03-19 22:32-INFO-
20-03-19 22:35-INFO-Epoch 4, Batch 364, Global step 8700:
20-03-19 22:35-INFO-training batch loss: 0.0832; avg_loss: 0.0456
20-03-19 22:35-INFO-training batch accuracy: 0.9375; avg_accuracy: 0.9738
20-03-19 22:35-INFO-
20-03-19 22:37-INFO-Epoch 4, Batch 464, Global step 8800:
20-03-19 22:37-INFO-training batch loss: 0.0143; avg_loss: 0.0456
20-03-19 22:37-INFO-training batch accuracy: 1.0000; avg_accuracy: 0.9742
20-03-19 22:37-INFO-
20-03-19 22:39-INFO-Epoch 4, Batch 564, Global step 8900:
20-03-19 22:39-INFO-training batch loss: 0.0426; avg_loss: 0.0458
20-03-19 22:39-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9747
20-03-19 22:39-INFO-
20-03-19 22:42-INFO-Epoch 4, Batch 664, Global step 9000:
20-03-19 22:42-INFO-training batch loss: 0.0275; avg_loss: 0.0467
20-03-19 22:42-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9746
20-03-19 22:42-INFO-
20-03-19 22:44-INFO-Epoch 4, Batch 764, Global step 9100:
20-03-19 22:44-INFO-training batch loss: 0.0445; avg_loss: 0.0466
20-03-19 22:44-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9750
20-03-19 22:44-INFO-
20-03-19 22:46-INFO-Epoch 4, Batch 864, Global step 9200:
20-03-19 22:46-INFO-training batch loss: 0.0723; avg_loss: 0.0465
20-03-19 22:46-INFO-training batch accuracy: 0.9609; avg_accuracy: 0.9752
20-03-19 22:46-INFO-
20-03-19 22:48-INFO-Epoch 4, Batch 964, Global step 9300:
20-03-19 22:48-INFO-training batch loss: 0.0692; avg_loss: 0.0468
20-03-19 22:48-INFO-training batch accuracy: 0.9531; avg_accuracy: 0.9753
20-03-19 22:48-INFO-
20-03-19 22:51-INFO-Epoch 4, Batch 1064, Global step 9400:
20-03-19 22:51-INFO-training batch loss: 0.0336; avg_loss: 0.0472
20-03-19 22:51-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9753
20-03-19 22:51-INFO-
20-03-19 22:53-INFO-Epoch 4, Batch 1164, Global step 9500:
20-03-19 22:53-INFO-training batch loss: 0.0463; avg_loss: 0.0467
20-03-19 22:53-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9755
20-03-19 22:53-INFO-
20-03-19 22:55-INFO-Epoch 4, Batch 1264, Global step 9600:
20-03-19 22:55-INFO-training batch loss: 0.0440; avg_loss: 0.0465
20-03-19 22:55-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9756
20-03-19 22:55-INFO-
20-03-19 22:58-INFO-Epoch 4, Batch 1364, Global step 9700:
20-03-19 22:58-INFO-training batch loss: 0.0690; avg_loss: 0.0468
20-03-19 22:58-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9756
20-03-19 22:58-INFO-
20-03-19 23:00-INFO-Epoch 4, Batch 1464, Global step 9800:
20-03-19 23:00-INFO-training batch loss: 0.0398; avg_loss: 0.0470
20-03-19 23:00-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9756
20-03-19 23:00-INFO-
20-03-19 23:02-INFO-Epoch 4, Batch 1564, Global step 9900:
20-03-19 23:02-INFO-training batch loss: 0.0253; avg_loss: 0.0467
20-03-19 23:02-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9757
20-03-19 23:02-INFO-
20-03-19 23:05-INFO-Epoch 4, Batch 1664, Global step 10000:
20-03-19 23:05-INFO-training batch loss: 0.0479; avg_loss: 0.0465
20-03-19 23:05-INFO-training batch accuracy: 0.9609; avg_accuracy: 0.9759
20-03-19 23:05-INFO-
20-03-19 23:07-INFO-Epoch 4, Batch 1764, Global step 10100:
20-03-19 23:07-INFO-training batch loss: 0.0477; avg_loss: 0.0462
20-03-19 23:07-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9760
20-03-19 23:07-INFO-
20-03-19 23:09-INFO-Epoch 4, Batch 1864, Global step 10200:
20-03-19 23:09-INFO-training batch loss: 0.0425; avg_loss: 0.0460
20-03-19 23:09-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9762
20-03-19 23:09-INFO-
20-03-19 23:11-INFO-Epoch 4, Batch 1964, Global step 10300:
20-03-19 23:11-INFO-training batch loss: 0.0321; avg_loss: 0.0457
20-03-19 23:11-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9763
20-03-19 23:11-INFO-
20-03-19 23:13-INFO-Epoch 4, Batch 2064, Global step 10400:
20-03-19 23:13-INFO-training batch loss: 0.0371; avg_loss: 0.0459
20-03-19 23:13-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9764
20-03-19 23:13-INFO-
20-03-19 23:14-INFO-training batch accuracy: 0.9286; avg_accuracy: 0.9768
20-03-19 23:14-INFO-
20-03-19 23:16-INFO-Epoch 4, evaluating batch loss: 0.0770; avg_loss: 0.0838
20-03-19 23:16-INFO-evaluating batch accuracy: 0.9712; avg_accuracy: 0.9578

20-03-19 23:16-INFO-
20-03-19 23:17-INFO-Epoch 5, Batch 80, Global step 10500:
20-03-19 23:17-INFO-training batch loss: 0.0340; avg_loss: 0.0435
20-03-19 23:17-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9676
20-03-19 23:17-INFO-
20-03-19 23:19-INFO-Epoch 5, Batch 180, Global step 10600:
20-03-19 23:19-INFO-training batch loss: 0.0379; avg_loss: 0.0451
20-03-19 23:19-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9726
20-03-19 23:19-INFO-
20-03-19 23:21-INFO-Epoch 5, Batch 280, Global step 10700:
20-03-19 23:21-INFO-training batch loss: 0.0409; avg_loss: 0.0447
20-03-19 23:21-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9745
20-03-19 23:21-INFO-
20-03-19 23:23-INFO-Epoch 5, Batch 380, Global step 10800:
20-03-19 23:23-INFO-training batch loss: 0.0228; avg_loss: 0.0450
20-03-19 23:23-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9748
20-03-19 23:23-INFO-
20-03-19 23:25-INFO-Epoch 5, Batch 480, Global step 10900:
20-03-19 23:25-INFO-training batch loss: 0.0487; avg_loss: 0.0453
20-03-19 23:25-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9751
20-03-19 23:25-INFO-
20-03-19 23:27-INFO-Epoch 5, Batch 580, Global step 11000:
20-03-19 23:27-INFO-training batch loss: 0.0486; avg_loss: 0.0447
20-03-19 23:27-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9757
20-03-19 23:27-INFO-
20-03-19 23:30-INFO-Epoch 5, Batch 680, Global step 11100:
20-03-19 23:30-INFO-training batch loss: 0.0242; avg_loss: 0.0450
20-03-19 23:30-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9756
20-03-19 23:30-INFO-
20-03-19 23:32-INFO-Epoch 5, Batch 780, Global step 11200:
20-03-19 23:32-INFO-training batch loss: 0.1034; avg_loss: 0.0452
20-03-19 23:32-INFO-training batch accuracy: 0.9375; avg_accuracy: 0.9758
20-03-19 23:32-INFO-
20-03-19 23:34-INFO-Epoch 5, Batch 880, Global step 11300:
20-03-19 23:34-INFO-training batch loss: 0.0614; avg_loss: 0.0451
20-03-19 23:34-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9760
20-03-19 23:34-INFO-
20-03-19 23:36-INFO-Epoch 5, Batch 980, Global step 11400:
20-03-19 23:36-INFO-training batch loss: 0.0444; avg_loss: 0.0454
20-03-19 23:36-INFO-training batch accuracy: 0.9609; avg_accuracy: 0.9761
20-03-19 23:36-INFO-
20-03-19 23:38-INFO-Epoch 5, Batch 1080, Global step 11500:
20-03-19 23:38-INFO-training batch loss: 0.0585; avg_loss: 0.0454
20-03-19 23:38-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9762
20-03-19 23:38-INFO-
20-03-19 23:40-INFO-Epoch 5, Batch 1180, Global step 11600:
20-03-19 23:40-INFO-training batch loss: 0.0313; avg_loss: 0.0449
20-03-19 23:40-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9765
20-03-19 23:40-INFO-
20-03-19 23:42-INFO-Epoch 5, Batch 1280, Global step 11700:
20-03-19 23:42-INFO-training batch loss: 0.0529; avg_loss: 0.0449
20-03-19 23:42-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9765
20-03-19 23:42-INFO-
20-03-19 23:44-INFO-Epoch 5, Batch 1380, Global step 11800:
20-03-19 23:44-INFO-training batch loss: 0.0303; avg_loss: 0.0447
20-03-19 23:44-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9766
20-03-19 23:44-INFO-
20-03-19 23:46-INFO-Epoch 5, Batch 1480, Global step 11900:
20-03-19 23:46-INFO-training batch loss: 0.0384; avg_loss: 0.0445
20-03-19 23:46-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9768
20-03-19 23:46-INFO-
20-03-19 23:48-INFO-Epoch 5, Batch 1580, Global step 12000:
20-03-19 23:48-INFO-training batch loss: 0.0262; avg_loss: 0.0445
20-03-19 23:48-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9768
20-03-19 23:48-INFO-
20-03-19 23:50-INFO-Epoch 5, Batch 1680, Global step 12100:
20-03-19 23:50-INFO-training batch loss: 0.0362; avg_loss: 0.0444
20-03-19 23:50-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9768
20-03-19 23:50-INFO-
20-03-19 23:52-INFO-Epoch 5, Batch 1780, Global step 12200:
20-03-19 23:52-INFO-training batch loss: 0.0478; avg_loss: 0.0441
20-03-19 23:52-INFO-training batch accuracy: 0.9609; avg_accuracy: 0.9769
20-03-19 23:52-INFO-
20-03-19 23:54-INFO-Epoch 5, Batch 1880, Global step 12300:
20-03-19 23:54-INFO-training batch loss: 0.0198; avg_loss: 0.0439
20-03-19 23:54-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9771
20-03-19 23:54-INFO-
20-03-19 23:56-INFO-Epoch 5, Batch 1980, Global step 12400:
20-03-19 23:56-INFO-training batch loss: 0.0187; avg_loss: 0.0437
20-03-19 23:56-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9772
20-03-19 23:56-INFO-
20-03-19 23:58-INFO-Epoch 5, Batch 2080, Global step 12500:
20-03-19 23:58-INFO-training batch loss: 0.0494; avg_loss: 0.0435
20-03-19 23:58-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9773
20-03-19 23:58-INFO-
20-03-19 23:58-INFO-training batch accuracy: 0.9524; avg_accuracy: 0.9778
20-03-19 23:58-INFO-
20-03-20 00:00-INFO-Epoch 5, evaluating batch loss: 0.0475; avg_loss: 0.0725
20-03-20 00:00-INFO-evaluating batch accuracy: 0.9808; avg_accuracy: 0.9613

20-03-20 00:00-INFO-
20-03-20 00:00-WARNING-From /root/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to delete files with this prefix.
20-03-20 00:02-INFO-Epoch 6, Batch 96, Global step 12600:
20-03-20 00:02-INFO-training batch loss: 0.0129; avg_loss: 0.0382
20-03-20 00:02-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9696
20-03-20 00:02-INFO-
20-03-20 00:05-INFO-Epoch 6, Batch 196, Global step 12700:
20-03-20 00:05-INFO-training batch loss: 0.0476; avg_loss: 0.0410
20-03-20 00:05-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9740
20-03-20 00:05-INFO-
20-03-20 00:07-INFO-Epoch 6, Batch 296, Global step 12800:
20-03-20 00:07-INFO-training batch loss: 0.0310; avg_loss: 0.0406
20-03-20 00:07-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9757
20-03-20 00:07-INFO-
20-03-20 00:09-INFO-Epoch 6, Batch 396, Global step 12900:
20-03-20 00:09-INFO-training batch loss: 0.0295; avg_loss: 0.0411
20-03-20 00:09-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9761
20-03-20 00:09-INFO-
20-03-20 00:11-INFO-Epoch 6, Batch 496, Global step 13000:
20-03-20 00:11-INFO-training batch loss: 0.0254; avg_loss: 0.0414
20-03-20 00:11-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9766
20-03-20 00:11-INFO-
20-03-20 00:13-INFO-Epoch 6, Batch 596, Global step 13100:
20-03-20 00:13-INFO-training batch loss: 0.0177; avg_loss: 0.0409
20-03-20 00:13-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9771
20-03-20 00:13-INFO-
20-03-20 00:15-INFO-Epoch 6, Batch 696, Global step 13200:
20-03-20 00:15-INFO-training batch loss: 0.0246; avg_loss: 0.0415
20-03-20 00:15-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9770
20-03-20 00:15-INFO-
20-03-20 00:17-INFO-Epoch 6, Batch 796, Global step 13300:
20-03-20 00:17-INFO-training batch loss: 0.0308; avg_loss: 0.0414
20-03-20 00:17-INFO-training batch accuracy: 1.0000; avg_accuracy: 0.9772
20-03-20 00:17-INFO-
20-03-20 00:19-INFO-Epoch 6, Batch 896, Global step 13400:
20-03-20 00:19-INFO-training batch loss: 0.0374; avg_loss: 0.0417
20-03-20 00:19-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9772
20-03-20 00:19-INFO-
20-03-20 00:21-INFO-Epoch 6, Batch 996, Global step 13500:
20-03-20 00:21-INFO-training batch loss: 0.0345; avg_loss: 0.0418
20-03-20 00:21-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9773
20-03-20 00:21-INFO-
20-03-20 00:23-INFO-Epoch 6, Batch 1096, Global step 13600:
20-03-20 00:23-INFO-training batch loss: 0.0215; avg_loss: 0.0417
20-03-20 00:23-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9774
20-03-20 00:23-INFO-
20-03-20 00:25-INFO-Epoch 6, Batch 1196, Global step 13700:
20-03-20 00:25-INFO-training batch loss: 0.0375; avg_loss: 0.0415
20-03-20 00:25-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9775
20-03-20 00:25-INFO-
20-03-20 00:28-INFO-Epoch 6, Batch 1296, Global step 13800:
20-03-20 00:28-INFO-training batch loss: 0.0358; avg_loss: 0.0416
20-03-20 00:28-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9775
20-03-20 00:28-INFO-
20-03-20 00:30-INFO-Epoch 6, Batch 1396, Global step 13900:
20-03-20 00:30-INFO-training batch loss: 0.0122; avg_loss: 0.0414
20-03-20 00:30-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9775
20-03-20 00:30-INFO-
20-03-20 00:32-INFO-Epoch 6, Batch 1496, Global step 14000:
20-03-20 00:32-INFO-training batch loss: 0.0422; avg_loss: 0.0418
20-03-20 00:32-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9775
20-03-20 00:32-INFO-
20-03-20 00:34-INFO-Epoch 6, Batch 1596, Global step 14100:
20-03-20 00:34-INFO-training batch loss: 0.0323; avg_loss: 0.0417
20-03-20 00:34-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9775
20-03-20 00:34-INFO-
20-03-20 00:36-INFO-Epoch 6, Batch 1696, Global step 14200:
20-03-20 00:36-INFO-training batch loss: 0.0729; avg_loss: 0.0416
20-03-20 00:36-INFO-training batch accuracy: 0.9453; avg_accuracy: 0.9776
20-03-20 00:36-INFO-
20-03-20 00:38-INFO-Epoch 6, Batch 1796, Global step 14300:
20-03-20 00:38-INFO-training batch loss: 0.0437; avg_loss: 0.0413
20-03-20 00:38-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9777
20-03-20 00:38-INFO-
20-03-20 00:40-INFO-Epoch 6, Batch 1896, Global step 14400:
20-03-20 00:40-INFO-training batch loss: 0.0179; avg_loss: 0.0411
20-03-20 00:40-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9779
20-03-20 00:40-INFO-
20-03-20 00:42-INFO-Epoch 6, Batch 1996, Global step 14500:
20-03-20 00:42-INFO-training batch loss: 0.0201; avg_loss: 0.0409
20-03-20 00:42-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9780
20-03-20 00:42-INFO-
20-03-20 00:44-INFO-training batch accuracy: 0.9524; avg_accuracy: 0.9785
20-03-20 00:44-INFO-
20-03-20 00:46-INFO-Epoch 6, evaluating batch loss: 0.0443; avg_loss: 0.0675
20-03-20 00:46-INFO-evaluating batch accuracy: 0.9808; avg_accuracy: 0.9638

20-03-20 00:46-INFO-
20-03-20 00:46-INFO-Epoch 7, Batch 12, Global step 14600:
20-03-20 00:46-INFO-training batch loss: 0.0450; avg_loss: 0.0309
20-03-20 00:46-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9023
20-03-20 00:46-INFO-
20-03-20 00:48-INFO-Epoch 7, Batch 112, Global step 14700:
20-03-20 00:48-INFO-training batch loss: 0.0456; avg_loss: 0.0353
20-03-20 00:48-INFO-training batch accuracy: 0.9531; avg_accuracy: 0.9726
20-03-20 00:48-INFO-
20-03-20 00:50-INFO-Epoch 7, Batch 212, Global step 14800:
20-03-20 00:50-INFO-training batch loss: 0.0392; avg_loss: 0.0368
20-03-20 00:50-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9760
20-03-20 00:50-INFO-
20-03-20 00:52-INFO-Epoch 7, Batch 312, Global step 14900:
20-03-20 00:52-INFO-training batch loss: 0.1759; avg_loss: 0.0378
20-03-20 00:52-INFO-training batch accuracy: 0.9141; avg_accuracy: 0.9773
20-03-20 00:52-INFO-
20-03-20 00:54-INFO-Epoch 7, Batch 412, Global step 15000:
20-03-20 00:54-INFO-training batch loss: 0.0213; avg_loss: 0.0377
20-03-20 00:54-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9776
20-03-20 00:54-INFO-
20-03-20 00:56-INFO-Epoch 7, Batch 512, Global step 15100:
20-03-20 00:56-INFO-training batch loss: 0.0327; avg_loss: 0.0380
20-03-20 00:56-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9780
20-03-20 00:56-INFO-
20-03-20 00:58-INFO-Epoch 7, Batch 612, Global step 15200:
20-03-20 00:58-INFO-training batch loss: 0.0170; avg_loss: 0.0382
20-03-20 00:58-INFO-training batch accuracy: 1.0000; avg_accuracy: 0.9781
20-03-20 00:58-INFO-
20-03-20 01:00-INFO-Epoch 7, Batch 712, Global step 15300:
20-03-20 01:00-INFO-training batch loss: 0.0315; avg_loss: 0.0387
20-03-20 01:00-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9781
20-03-20 01:00-INFO-
20-03-20 01:02-INFO-Epoch 7, Batch 812, Global step 15400:
20-03-20 01:02-INFO-training batch loss: 0.0451; avg_loss: 0.0387
20-03-20 01:02-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9783
20-03-20 01:02-INFO-
20-03-20 01:04-INFO-Epoch 7, Batch 912, Global step 15500:
20-03-20 01:04-INFO-training batch loss: 0.0247; avg_loss: 0.0395
20-03-20 01:04-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9781
20-03-20 01:04-INFO-
20-03-20 01:06-INFO-Epoch 7, Batch 1012, Global step 15600:
20-03-20 01:06-INFO-training batch loss: 0.0363; avg_loss: 0.0395
20-03-20 01:06-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9783
20-03-20 01:06-INFO-
20-03-20 01:08-INFO-Epoch 7, Batch 1112, Global step 15700:
20-03-20 01:08-INFO-training batch loss: 0.0380; avg_loss: 0.0399
20-03-20 01:08-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9782
20-03-20 01:08-INFO-
20-03-20 01:10-INFO-Epoch 7, Batch 1212, Global step 15800:
20-03-20 01:10-INFO-training batch loss: 0.0206; avg_loss: 0.0401
20-03-20 01:10-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9782
20-03-20 01:10-INFO-
20-03-20 01:12-INFO-Epoch 7, Batch 1312, Global step 15900:
20-03-20 01:12-INFO-training batch loss: 0.0768; avg_loss: 0.0407
20-03-20 01:12-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9780
20-03-20 01:12-INFO-
20-03-20 01:14-INFO-Epoch 7, Batch 1412, Global step 16000:
20-03-20 01:14-INFO-training batch loss: 0.0318; avg_loss: 0.0405
20-03-20 01:14-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9782
20-03-20 01:14-INFO-
20-03-20 01:17-INFO-Epoch 7, Batch 1512, Global step 16100:
20-03-20 01:17-INFO-training batch loss: 0.0391; avg_loss: 0.0405
20-03-20 01:17-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9782
20-03-20 01:17-INFO-
20-03-20 01:19-INFO-Epoch 7, Batch 1612, Global step 16200:
20-03-20 01:19-INFO-training batch loss: 0.0463; avg_loss: 0.0403
20-03-20 01:19-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9783
20-03-20 01:19-INFO-
20-03-20 01:21-INFO-Epoch 7, Batch 1712, Global step 16300:
20-03-20 01:21-INFO-training batch loss: 0.0221; avg_loss: 0.0401
20-03-20 01:21-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9784
20-03-20 01:21-INFO-
20-03-20 01:23-INFO-Epoch 7, Batch 1812, Global step 16400:
20-03-20 01:23-INFO-training batch loss: 0.0235; avg_loss: 0.0398
20-03-20 01:23-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9785
20-03-20 01:23-INFO-
20-03-20 01:25-INFO-Epoch 7, Batch 1912, Global step 16500:
20-03-20 01:25-INFO-training batch loss: 0.1040; avg_loss: 0.0396
20-03-20 01:25-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9788
20-03-20 01:25-INFO-
20-03-20 01:27-INFO-Epoch 7, Batch 2012, Global step 16600:
20-03-20 01:27-INFO-training batch loss: 0.0407; avg_loss: 0.0395
20-03-20 01:27-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9788
20-03-20 01:27-INFO-
20-03-20 01:28-INFO-training batch accuracy: 0.9286; avg_accuracy: 0.9792
20-03-20 01:28-INFO-
20-03-20 01:30-INFO-Epoch 7, evaluating batch loss: 0.0465; avg_loss: 0.0711
20-03-20 01:30-INFO-evaluating batch accuracy: 0.9712; avg_accuracy: 0.9638

20-03-20 01:30-INFO-
20-03-20 01:31-INFO-Epoch 8, Batch 28, Global step 16700:
20-03-20 01:31-INFO-training batch loss: 0.0686; avg_loss: 0.0374
20-03-20 01:31-INFO-training batch accuracy: 0.9609; avg_accuracy: 0.9442
20-03-20 01:31-INFO-
20-03-20 01:33-INFO-Epoch 8, Batch 128, Global step 16800:
20-03-20 01:33-INFO-training batch loss: 0.0818; avg_loss: 0.0372
20-03-20 01:33-INFO-training batch accuracy: 0.9531; avg_accuracy: 0.9729
20-03-20 01:33-INFO-
20-03-20 01:35-INFO-Epoch 8, Batch 228, Global step 16900:
20-03-20 01:35-INFO-training batch loss: 0.0575; avg_loss: 0.0378
20-03-20 01:35-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9760
20-03-20 01:35-INFO-
20-03-20 01:37-INFO-Epoch 8, Batch 328, Global step 17000:
20-03-20 01:37-INFO-training batch loss: 0.0140; avg_loss: 0.0380
20-03-20 01:37-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9766
20-03-20 01:37-INFO-
20-03-20 01:39-INFO-Epoch 8, Batch 428, Global step 17100:
20-03-20 01:39-INFO-training batch loss: 0.0543; avg_loss: 0.0379
20-03-20 01:39-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9769
20-03-20 01:39-INFO-
20-03-20 01:41-INFO-Epoch 8, Batch 528, Global step 17200:
20-03-20 01:41-INFO-training batch loss: 0.0289; avg_loss: 0.0377
20-03-20 01:41-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9778
20-03-20 01:41-INFO-
20-03-20 01:43-INFO-Epoch 8, Batch 628, Global step 17300:
20-03-20 01:43-INFO-training batch loss: 0.0436; avg_loss: 0.0374
20-03-20 01:43-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9784
20-03-20 01:43-INFO-
20-03-20 01:45-INFO-Epoch 8, Batch 728, Global step 17400:
20-03-20 01:45-INFO-training batch loss: 0.0581; avg_loss: 0.0375
20-03-20 01:45-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9786
20-03-20 01:45-INFO-
20-03-20 01:47-INFO-Epoch 8, Batch 828, Global step 17500:
20-03-20 01:47-INFO-training batch loss: 0.0202; avg_loss: 0.0376
20-03-20 01:47-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9788
20-03-20 01:47-INFO-
20-03-20 01:49-INFO-Epoch 8, Batch 928, Global step 17600:
20-03-20 01:49-INFO-training batch loss: 0.0202; avg_loss: 0.0379
20-03-20 01:49-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9787
20-03-20 01:49-INFO-
20-03-20 01:51-INFO-Epoch 8, Batch 1028, Global step 17700:
20-03-20 01:51-INFO-training batch loss: 0.0293; avg_loss: 0.0381
20-03-20 01:51-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9789
20-03-20 01:51-INFO-
20-03-20 01:53-INFO-Epoch 8, Batch 1128, Global step 17800:
20-03-20 01:53-INFO-training batch loss: 0.0321; avg_loss: 0.0380
20-03-20 01:53-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9789
20-03-20 01:53-INFO-
20-03-20 01:55-INFO-Epoch 8, Batch 1228, Global step 17900:
20-03-20 01:55-INFO-training batch loss: 0.0498; avg_loss: 0.0380
20-03-20 01:55-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9790
20-03-20 01:55-INFO-
20-03-20 01:57-INFO-Epoch 8, Batch 1328, Global step 18000:
20-03-20 01:57-INFO-training batch loss: 0.0067; avg_loss: 0.0385
20-03-20 01:57-INFO-training batch accuracy: 1.0000; avg_accuracy: 0.9789
20-03-20 01:57-INFO-
20-03-20 01:59-INFO-Epoch 8, Batch 1428, Global step 18100:
20-03-20 01:59-INFO-training batch loss: 0.0255; avg_loss: 0.0384
20-03-20 01:59-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9791
20-03-20 01:59-INFO-
20-03-20 02:01-INFO-Epoch 8, Batch 1528, Global step 18200:
20-03-20 02:01-INFO-training batch loss: 0.0386; avg_loss: 0.0385
20-03-20 02:01-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9791
20-03-20 02:01-INFO-
20-03-20 02:03-INFO-Epoch 8, Batch 1628, Global step 18300:
20-03-20 02:03-INFO-training batch loss: 0.0480; avg_loss: 0.0383
20-03-20 02:03-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9792
20-03-20 02:03-INFO-
20-03-20 02:06-INFO-Epoch 8, Batch 1728, Global step 18400:
20-03-20 02:06-INFO-training batch loss: 0.0600; avg_loss: 0.0382
20-03-20 02:06-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9794
20-03-20 02:06-INFO-
20-03-20 02:08-INFO-Epoch 8, Batch 1828, Global step 18500:
20-03-20 02:08-INFO-training batch loss: 0.0260; avg_loss: 0.0380
20-03-20 02:08-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9795
20-03-20 02:08-INFO-
20-03-20 02:10-INFO-Epoch 8, Batch 1928, Global step 18600:
20-03-20 02:10-INFO-training batch loss: 0.0394; avg_loss: 0.0378
20-03-20 02:10-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9798
20-03-20 02:10-INFO-
20-03-20 02:12-INFO-Epoch 8, Batch 2028, Global step 18700:
20-03-20 02:12-INFO-training batch loss: 0.0402; avg_loss: 0.0376
20-03-20 02:12-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9800
20-03-20 02:12-INFO-
20-03-20 02:13-INFO-training batch accuracy: 0.9524; avg_accuracy: 0.9804
20-03-20 02:13-INFO-
20-03-20 02:15-INFO-Epoch 8, evaluating batch loss: 0.0549; avg_loss: 0.0686
20-03-20 02:15-INFO-evaluating batch accuracy: 0.9712; avg_accuracy: 0.9691

20-03-20 02:15-INFO-
20-03-20 02:16-INFO-Epoch 9, Batch 44, Global step 18800:
20-03-20 02:16-INFO-training batch loss: 0.0359; avg_loss: 0.0353
20-03-20 02:16-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9609
20-03-20 02:16-INFO-
20-03-20 02:18-INFO-Epoch 9, Batch 144, Global step 18900:
20-03-20 02:18-INFO-training batch loss: 0.0164; avg_loss: 0.0347
20-03-20 02:18-INFO-training batch accuracy: 1.0000; avg_accuracy: 0.9757
20-03-20 02:18-INFO-
20-03-20 02:20-INFO-Epoch 9, Batch 244, Global step 19000:
20-03-20 02:20-INFO-training batch loss: 0.0392; avg_loss: 0.0353
20-03-20 02:20-INFO-training batch accuracy: 0.9609; avg_accuracy: 0.9785
20-03-20 02:20-INFO-
20-03-20 02:22-INFO-Epoch 9, Batch 344, Global step 19100:
20-03-20 02:22-INFO-training batch loss: 0.0402; avg_loss: 0.0357
20-03-20 02:22-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9794
20-03-20 02:22-INFO-
20-03-20 02:24-INFO-Epoch 9, Batch 444, Global step 19200:
20-03-20 02:24-INFO-training batch loss: 0.0289; avg_loss: 0.0361
20-03-20 02:24-INFO-training batch accuracy: 1.0000; avg_accuracy: 0.9797
20-03-20 02:24-INFO-
20-03-20 02:26-INFO-Epoch 9, Batch 544, Global step 19300:
20-03-20 02:26-INFO-training batch loss: 0.0316; avg_loss: 0.0356
20-03-20 02:26-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9807
20-03-20 02:26-INFO-
20-03-20 02:28-INFO-Epoch 9, Batch 644, Global step 19400:
20-03-20 02:28-INFO-training batch loss: 0.0232; avg_loss: 0.0359
20-03-20 02:28-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9809
20-03-20 02:28-INFO-
20-03-20 02:30-INFO-Epoch 9, Batch 744, Global step 19500:
20-03-20 02:30-INFO-training batch loss: 0.0582; avg_loss: 0.0361
20-03-20 02:30-INFO-training batch accuracy: 0.9609; avg_accuracy: 0.9810
20-03-20 02:30-INFO-
20-03-20 02:32-INFO-Epoch 9, Batch 844, Global step 19600:
20-03-20 02:32-INFO-training batch loss: 0.0259; avg_loss: 0.0360
20-03-20 02:32-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9811
20-03-20 02:32-INFO-
20-03-20 02:34-INFO-Epoch 9, Batch 944, Global step 19700:
20-03-20 02:34-INFO-training batch loss: 0.0428; avg_loss: 0.0363
20-03-20 02:34-INFO-training batch accuracy: 0.9609; avg_accuracy: 0.9812
20-03-20 02:34-INFO-
20-03-20 02:36-INFO-Epoch 9, Batch 1044, Global step 19800:
20-03-20 02:36-INFO-training batch loss: 0.0634; avg_loss: 0.0363
20-03-20 02:36-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9815
20-03-20 02:36-INFO-
20-03-20 02:38-INFO-Epoch 9, Batch 1144, Global step 19900:
20-03-20 02:38-INFO-training batch loss: 0.0930; avg_loss: 0.0361
20-03-20 02:38-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9817
20-03-20 02:38-INFO-
20-03-20 02:40-INFO-Epoch 9, Batch 1244, Global step 20000:
20-03-20 02:40-INFO-training batch loss: 0.0239; avg_loss: 0.0360
20-03-20 02:40-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9819
20-03-20 02:40-INFO-
20-03-20 02:42-INFO-Epoch 9, Batch 1344, Global step 20100:
20-03-20 02:42-INFO-training batch loss: 0.0397; avg_loss: 0.0359
20-03-20 02:42-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9821
20-03-20 02:42-INFO-
20-03-20 02:44-INFO-Epoch 9, Batch 1444, Global step 20200:
20-03-20 02:44-INFO-training batch loss: 0.0423; avg_loss: 0.0361
20-03-20 02:44-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9822
20-03-20 02:44-INFO-
20-03-20 02:46-INFO-Epoch 9, Batch 1544, Global step 20300:
20-03-20 02:46-INFO-training batch loss: 0.0262; avg_loss: 0.0362
20-03-20 02:46-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9822
20-03-20 02:46-INFO-
20-03-20 02:48-INFO-Epoch 9, Batch 1644, Global step 20400:
20-03-20 02:48-INFO-training batch loss: 0.0209; avg_loss: 0.0361
20-03-20 02:48-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9824
20-03-20 02:48-INFO-
20-03-20 02:50-INFO-Epoch 9, Batch 1744, Global step 20500:
20-03-20 02:50-INFO-training batch loss: 0.0576; avg_loss: 0.0360
20-03-20 02:50-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9825
20-03-20 02:50-INFO-
20-03-20 02:53-INFO-Epoch 9, Batch 1844, Global step 20600:
20-03-20 02:53-INFO-training batch loss: 0.0205; avg_loss: 0.0358
20-03-20 02:53-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9827
20-03-20 02:53-INFO-
20-03-20 02:55-INFO-Epoch 9, Batch 1944, Global step 20700:
20-03-20 02:55-INFO-training batch loss: 0.0210; avg_loss: 0.0355
20-03-20 02:55-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9830
20-03-20 02:55-INFO-
20-03-20 02:57-INFO-Epoch 9, Batch 2044, Global step 20800:
20-03-20 02:57-INFO-training batch loss: 0.0584; avg_loss: 0.0353
20-03-20 02:57-INFO-training batch accuracy: 0.9531; avg_accuracy: 0.9832
20-03-20 02:57-INFO-
20-03-20 02:57-INFO-training batch accuracy: 0.9524; avg_accuracy: 0.9837
20-03-20 02:57-INFO-
20-03-20 02:59-INFO-Epoch 9, evaluating batch loss: 0.0460; avg_loss: 0.0581
20-03-20 02:59-INFO-evaluating batch accuracy: 0.9904; avg_accuracy: 0.9752

20-03-20 02:59-INFO-
20-03-20 03:01-INFO-Epoch 10, Batch 60, Global step 20900:
20-03-20 03:01-INFO-training batch loss: 0.0720; avg_loss: 0.0318
20-03-20 03:01-INFO-training batch accuracy: 0.9609; avg_accuracy: 0.9693
20-03-20 03:01-INFO-
20-03-20 03:03-INFO-Epoch 10, Batch 160, Global step 21000:
20-03-20 03:03-INFO-training batch loss: 0.0187; avg_loss: 0.0355
20-03-20 03:03-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9781
20-03-20 03:03-INFO-
20-03-20 03:05-INFO-Epoch 10, Batch 260, Global step 21100:
20-03-20 03:05-INFO-training batch loss: 0.0681; avg_loss: 0.0341
20-03-20 03:05-INFO-training batch accuracy: 0.9609; avg_accuracy: 0.9813
20-03-20 03:05-INFO-
20-03-20 03:07-INFO-Epoch 10, Batch 360, Global step 21200:
20-03-20 03:07-INFO-training batch loss: 0.0311; avg_loss: 0.0333
20-03-20 03:07-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9827
20-03-20 03:07-INFO-
20-03-20 03:09-INFO-Epoch 10, Batch 460, Global step 21300:
20-03-20 03:09-INFO-training batch loss: 0.0098; avg_loss: 0.0329
20-03-20 03:09-INFO-training batch accuracy: 1.0000; avg_accuracy: 0.9835
20-03-20 03:09-INFO-
20-03-20 03:11-INFO-Epoch 10, Batch 560, Global step 21400:
20-03-20 03:11-INFO-training batch loss: 0.0100; avg_loss: 0.0321
20-03-20 03:11-INFO-training batch accuracy: 1.0000; avg_accuracy: 0.9843
20-03-20 03:11-INFO-
20-03-20 03:13-INFO-Epoch 10, Batch 660, Global step 21500:
20-03-20 03:13-INFO-training batch loss: 0.0085; avg_loss: 0.0318
20-03-20 03:13-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9847
20-03-20 03:13-INFO-
20-03-20 03:15-INFO-Epoch 10, Batch 760, Global step 21600:
20-03-20 03:15-INFO-training batch loss: 0.0122; avg_loss: 0.0318
20-03-20 03:15-INFO-training batch accuracy: 1.0000; avg_accuracy: 0.9849
20-03-20 03:15-INFO-
20-03-20 03:17-INFO-Epoch 10, Batch 860, Global step 21700:
20-03-20 03:17-INFO-training batch loss: 0.0287; avg_loss: 0.0325
20-03-20 03:17-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9849
20-03-20 03:17-INFO-
20-03-20 03:19-INFO-Epoch 10, Batch 960, Global step 21800:
20-03-20 03:19-INFO-training batch loss: 0.0180; avg_loss: 0.0328
20-03-20 03:19-INFO-training batch accuracy: 1.0000; avg_accuracy: 0.9850
20-03-20 03:19-INFO-
20-03-20 03:21-INFO-Epoch 10, Batch 1060, Global step 21900:
20-03-20 03:21-INFO-training batch loss: 0.0129; avg_loss: 0.0328
20-03-20 03:21-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9852
20-03-20 03:21-INFO-
20-03-20 03:23-INFO-Epoch 10, Batch 1160, Global step 22000:
20-03-20 03:23-INFO-training batch loss: 0.0203; avg_loss: 0.0324
20-03-20 03:23-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9854
20-03-20 03:23-INFO-
20-03-20 03:25-INFO-Epoch 10, Batch 1260, Global step 22100:
20-03-20 03:25-INFO-training batch loss: 0.0234; avg_loss: 0.0324
20-03-20 03:25-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9854
20-03-20 03:25-INFO-
20-03-20 03:27-INFO-Epoch 10, Batch 1360, Global step 22200:
20-03-20 03:27-INFO-training batch loss: 0.0218; avg_loss: 0.0323
20-03-20 03:27-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9856
20-03-20 03:27-INFO-
20-03-20 03:29-INFO-Epoch 10, Batch 1460, Global step 22300:
20-03-20 03:29-INFO-training batch loss: 0.0249; avg_loss: 0.0324
20-03-20 03:29-INFO-training batch accuracy: 1.0000; avg_accuracy: 0.9855
20-03-20 03:29-INFO-
20-03-20 03:31-INFO-Epoch 10, Batch 1560, Global step 22400:
20-03-20 03:31-INFO-training batch loss: 0.0274; avg_loss: 0.0324
20-03-20 03:31-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9856
20-03-20 03:31-INFO-
20-03-20 03:33-INFO-Epoch 10, Batch 1660, Global step 22500:
20-03-20 03:33-INFO-training batch loss: 0.0263; avg_loss: 0.0323
20-03-20 03:33-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9857
20-03-20 03:33-INFO-
20-03-20 03:36-INFO-Epoch 10, Batch 1760, Global step 22600:
20-03-20 03:36-INFO-training batch loss: 0.0185; avg_loss: 0.0322
20-03-20 03:36-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9859
20-03-20 03:36-INFO-
20-03-20 03:38-INFO-Epoch 10, Batch 1860, Global step 22700:
20-03-20 03:38-INFO-training batch loss: 0.0130; avg_loss: 0.0322
20-03-20 03:38-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9860
20-03-20 03:38-INFO-
20-03-20 03:40-INFO-Epoch 10, Batch 1960, Global step 22800:
20-03-20 03:40-INFO-training batch loss: 0.0012; avg_loss: 0.0321
20-03-20 03:40-INFO-training batch accuracy: 1.0000; avg_accuracy: 0.9861
20-03-20 03:40-INFO-
20-03-20 03:42-INFO-Epoch 10, Batch 2060, Global step 22900:
20-03-20 03:42-INFO-training batch loss: 0.0107; avg_loss: 0.0320
20-03-20 03:42-INFO-training batch accuracy: 1.0000; avg_accuracy: 0.9862
20-03-20 03:42-INFO-
20-03-20 03:42-INFO-training batch accuracy: 0.9762; avg_accuracy: 0.9866
20-03-20 03:42-INFO-
20-03-20 03:44-INFO-Epoch 10, evaluating batch loss: 0.0537; avg_loss: 0.0614
20-03-20 03:44-INFO-evaluating batch accuracy: 0.9712; avg_accuracy: 0.9775

20-03-20 03:44-INFO-
20-03-20 03:46-INFO-Epoch 11, Batch 76, Global step 23000:
20-03-20 03:46-INFO-training batch loss: 0.0364; avg_loss: 0.0301
20-03-20 03:46-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9741
20-03-20 03:46-INFO-
20-03-20 03:48-INFO-Epoch 11, Batch 176, Global step 23100:
20-03-20 03:48-INFO-training batch loss: 0.0273; avg_loss: 0.0317
20-03-20 03:48-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9806
20-03-20 03:48-INFO-
20-03-20 03:50-INFO-Epoch 11, Batch 276, Global step 23200:
20-03-20 03:50-INFO-training batch loss: 0.0323; avg_loss: 0.0320
20-03-20 03:50-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9828
20-03-20 03:50-INFO-
20-03-20 03:52-INFO-Epoch 11, Batch 376, Global step 23300:
20-03-20 03:52-INFO-training batch loss: 0.0054; avg_loss: 0.0322
20-03-20 03:52-INFO-training batch accuracy: 1.0000; avg_accuracy: 0.9839
20-03-20 03:52-INFO-
20-03-20 03:54-INFO-Epoch 11, Batch 476, Global step 23400:
20-03-20 03:54-INFO-training batch loss: 0.0168; avg_loss: 0.0318
20-03-20 03:54-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9848
20-03-20 03:54-INFO-
20-03-20 03:56-INFO-Epoch 11, Batch 576, Global step 23500:
20-03-20 03:56-INFO-training batch loss: 0.0267; avg_loss: 0.0311
20-03-20 03:56-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9855
20-03-20 03:56-INFO-
20-03-20 03:58-INFO-Epoch 11, Batch 676, Global step 23600:
20-03-20 03:58-INFO-training batch loss: 0.0391; avg_loss: 0.0311
20-03-20 03:58-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9857
20-03-20 03:58-INFO-
20-03-20 04:00-INFO-Epoch 11, Batch 776, Global step 23700:
20-03-20 04:00-INFO-training batch loss: 0.0206; avg_loss: 0.0312
20-03-20 04:00-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9858
20-03-20 04:00-INFO-
20-03-20 04:02-INFO-Epoch 11, Batch 876, Global step 23800:
20-03-20 04:02-INFO-training batch loss: 0.0522; avg_loss: 0.0311
20-03-20 04:02-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9859
20-03-20 04:02-INFO-
20-03-20 04:04-INFO-Epoch 11, Batch 976, Global step 23900:
20-03-20 04:04-INFO-training batch loss: 0.0403; avg_loss: 0.0310
20-03-20 04:04-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9862
20-03-20 04:04-INFO-
20-03-20 04:06-INFO-Epoch 11, Batch 1076, Global step 24000:
20-03-20 04:06-INFO-training batch loss: 0.0712; avg_loss: 0.0312
20-03-20 04:06-INFO-training batch accuracy: 0.9609; avg_accuracy: 0.9862
20-03-20 04:06-INFO-
20-03-20 04:08-INFO-Epoch 11, Batch 1176, Global step 24100:
20-03-20 04:08-INFO-training batch loss: 0.0096; avg_loss: 0.0310
20-03-20 04:08-INFO-training batch accuracy: 1.0000; avg_accuracy: 0.9862
20-03-20 04:08-INFO-
20-03-20 04:10-INFO-Epoch 11, Batch 1276, Global step 24200:
20-03-20 04:10-INFO-training batch loss: 0.0282; avg_loss: 0.0311
20-03-20 04:10-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9863
20-03-20 04:10-INFO-
20-03-20 04:12-INFO-Epoch 11, Batch 1376, Global step 24300:
20-03-20 04:12-INFO-training batch loss: 0.0228; avg_loss: 0.0309
20-03-20 04:12-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9864
20-03-20 04:12-INFO-
20-03-20 04:14-INFO-Epoch 11, Batch 1476, Global step 24400:
20-03-20 04:14-INFO-training batch loss: 0.0120; avg_loss: 0.0310
20-03-20 04:14-INFO-training batch accuracy: 1.0000; avg_accuracy: 0.9865
20-03-20 04:14-INFO-
20-03-20 04:16-INFO-Epoch 11, Batch 1576, Global step 24500:
20-03-20 04:16-INFO-training batch loss: 0.0574; avg_loss: 0.0312
20-03-20 04:16-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9865
20-03-20 04:16-INFO-
20-03-20 04:19-INFO-Epoch 11, Batch 1676, Global step 24600:
20-03-20 04:19-INFO-training batch loss: 0.0493; avg_loss: 0.0311
20-03-20 04:19-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9866
20-03-20 04:19-INFO-
20-03-20 04:21-INFO-Epoch 11, Batch 1776, Global step 24700:
20-03-20 04:21-INFO-training batch loss: 0.0051; avg_loss: 0.0311
20-03-20 04:21-INFO-training batch accuracy: 1.0000; avg_accuracy: 0.9866
20-03-20 04:21-INFO-
20-03-20 04:23-INFO-Epoch 11, Batch 1876, Global step 24800:
20-03-20 04:23-INFO-training batch loss: 0.0057; avg_loss: 0.0309
20-03-20 04:23-INFO-training batch accuracy: 1.0000; avg_accuracy: 0.9867
20-03-20 04:23-INFO-
20-03-20 04:25-INFO-Epoch 11, Batch 1976, Global step 24900:
20-03-20 04:25-INFO-training batch loss: 0.0159; avg_loss: 0.0308
20-03-20 04:25-INFO-training batch accuracy: 1.0000; avg_accuracy: 0.9868
20-03-20 04:25-INFO-
20-03-20 04:27-INFO-Epoch 11, Batch 2076, Global step 25000:
20-03-20 04:27-INFO-training batch loss: 0.0756; avg_loss: 0.0309
20-03-20 04:27-INFO-training batch accuracy: 0.9609; avg_accuracy: 0.9869
20-03-20 04:27-INFO-
20-03-20 04:27-INFO-training batch accuracy: 0.9762; avg_accuracy: 0.9873
20-03-20 04:27-INFO-
20-03-20 04:29-INFO-Epoch 11, evaluating batch loss: 0.0380; avg_loss: 0.0535
20-03-20 04:29-INFO-evaluating batch accuracy: 0.9808; avg_accuracy: 0.9797

20-03-20 04:29-INFO-
20-03-20 04:31-INFO-Epoch 12, Batch 92, Global step 25100:
20-03-20 04:31-INFO-training batch loss: 0.0479; avg_loss: 0.0278
20-03-20 04:31-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9776
20-03-20 04:31-INFO-
20-03-20 04:33-INFO-Epoch 12, Batch 192, Global step 25200:
20-03-20 04:33-INFO-training batch loss: 0.0182; avg_loss: 0.0284
20-03-20 04:33-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9833
20-03-20 04:33-INFO-
20-03-20 04:35-INFO-Epoch 12, Batch 292, Global step 25300:
20-03-20 04:35-INFO-training batch loss: 0.0078; avg_loss: 0.0289
20-03-20 04:35-INFO-training batch accuracy: 1.0000; avg_accuracy: 0.9847
20-03-20 04:35-INFO-
20-03-20 04:37-INFO-Epoch 12, Batch 392, Global step 25400:
20-03-20 04:37-INFO-training batch loss: 0.0210; avg_loss: 0.0293
20-03-20 04:37-INFO-training batch accuracy: 1.0000; avg_accuracy: 0.9858
20-03-20 04:37-INFO-
20-03-20 04:39-INFO-Epoch 12, Batch 492, Global step 25500:
20-03-20 04:39-INFO-training batch loss: 0.0201; avg_loss: 0.0291
20-03-20 04:39-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9862
20-03-20 04:39-INFO-
20-03-20 04:41-INFO-Epoch 12, Batch 592, Global step 25600:
20-03-20 04:41-INFO-training batch loss: 0.0236; avg_loss: 0.0292
20-03-20 04:41-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9864
20-03-20 04:41-INFO-
20-03-20 04:43-INFO-Epoch 12, Batch 692, Global step 25700:
20-03-20 04:43-INFO-training batch loss: 0.0347; avg_loss: 0.0294
20-03-20 04:43-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9865
20-03-20 04:43-INFO-
20-03-20 04:45-INFO-Epoch 12, Batch 792, Global step 25800:
20-03-20 04:45-INFO-training batch loss: 0.0382; avg_loss: 0.0294
20-03-20 04:45-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9867
20-03-20 04:45-INFO-
20-03-20 04:47-INFO-Epoch 12, Batch 892, Global step 25900:
20-03-20 04:47-INFO-training batch loss: 0.0555; avg_loss: 0.0297
20-03-20 04:47-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9867
20-03-20 04:47-INFO-
20-03-20 04:49-INFO-Epoch 12, Batch 992, Global step 26000:
20-03-20 04:49-INFO-training batch loss: 0.0248; avg_loss: 0.0297
20-03-20 04:49-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9868
20-03-20 04:49-INFO-
20-03-20 04:51-INFO-Epoch 12, Batch 1092, Global step 26100:
20-03-20 04:51-INFO-training batch loss: 0.0242; avg_loss: 0.0296
20-03-20 04:51-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9870
20-03-20 04:51-INFO-
20-03-20 04:53-INFO-Epoch 12, Batch 1192, Global step 26200:
20-03-20 04:53-INFO-training batch loss: 0.0379; avg_loss: 0.0294
20-03-20 04:53-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9871
20-03-20 04:53-INFO-
20-03-20 04:55-INFO-Epoch 12, Batch 1292, Global step 26300:
20-03-20 04:55-INFO-training batch loss: 0.0074; avg_loss: 0.0297
20-03-20 04:55-INFO-training batch accuracy: 1.0000; avg_accuracy: 0.9871
20-03-20 04:55-INFO-
20-03-20 04:57-INFO-Epoch 12, Batch 1392, Global step 26400:
20-03-20 04:57-INFO-training batch loss: 0.0467; avg_loss: 0.0296
20-03-20 04:57-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9872
20-03-20 04:57-INFO-
20-03-20 04:59-INFO-Epoch 12, Batch 1492, Global step 26500:
20-03-20 04:59-INFO-training batch loss: 0.0363; avg_loss: 0.0295
20-03-20 04:59-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9872
20-03-20 04:59-INFO-
20-03-20 05:01-INFO-Epoch 12, Batch 1592, Global step 26600:
20-03-20 05:01-INFO-training batch loss: 0.0165; avg_loss: 0.0295
20-03-20 05:01-INFO-training batch accuracy: 1.0000; avg_accuracy: 0.9873
20-03-20 05:01-INFO-
20-03-20 05:03-INFO-Epoch 12, Batch 1692, Global step 26700:
20-03-20 05:03-INFO-training batch loss: 0.0082; avg_loss: 0.0294
20-03-20 05:03-INFO-training batch accuracy: 1.0000; avg_accuracy: 0.9874
20-03-20 05:03-INFO-
20-03-20 05:05-INFO-Epoch 12, Batch 1792, Global step 26800:
20-03-20 05:05-INFO-training batch loss: 0.0020; avg_loss: 0.0293
20-03-20 05:05-INFO-training batch accuracy: 1.0000; avg_accuracy: 0.9874
20-03-20 05:05-INFO-
20-03-20 05:07-INFO-Epoch 12, Batch 1892, Global step 26900:
20-03-20 05:07-INFO-training batch loss: 0.0153; avg_loss: 0.0291
20-03-20 05:07-INFO-training batch accuracy: 1.0000; avg_accuracy: 0.9876
20-03-20 05:07-INFO-
20-03-20 05:09-INFO-Epoch 12, Batch 1992, Global step 27000:
20-03-20 05:09-INFO-training batch loss: 0.0085; avg_loss: 0.0291
20-03-20 05:09-INFO-training batch accuracy: 1.0000; avg_accuracy: 0.9877
20-03-20 05:09-INFO-
20-03-20 05:11-INFO-training batch accuracy: 0.9762; avg_accuracy: 0.9881
20-03-20 05:11-INFO-
20-03-20 05:13-INFO-Epoch 12, evaluating batch loss: 0.0319; avg_loss: 0.0555
20-03-20 05:13-INFO-evaluating batch accuracy: 0.9808; avg_accuracy: 0.9786

20-03-20 05:13-INFO-
20-03-20 05:13-INFO-Epoch 13, Batch 8, Global step 27100:
20-03-20 05:13-INFO-training batch loss: 0.0168; avg_loss: 0.0319
20-03-20 05:13-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.8633
20-03-20 05:13-INFO-
20-03-20 05:15-INFO-Epoch 13, Batch 108, Global step 27200:
20-03-20 05:15-INFO-training batch loss: 0.0076; avg_loss: 0.0301
20-03-20 05:15-INFO-training batch accuracy: 1.0000; avg_accuracy: 0.9774
20-03-20 05:15-INFO-
20-03-20 05:17-INFO-Epoch 13, Batch 208, Global step 27300:
20-03-20 05:17-INFO-training batch loss: 0.0353; avg_loss: 0.0302
20-03-20 05:17-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9829
20-03-20 05:17-INFO-
20-03-20 05:19-INFO-Epoch 13, Batch 308, Global step 27400:
20-03-20 05:19-INFO-training batch loss: 0.0230; avg_loss: 0.0299
20-03-20 05:19-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9845
20-03-20 05:19-INFO-
20-03-20 05:21-INFO-Epoch 13, Batch 408, Global step 27500:
20-03-20 05:21-INFO-training batch loss: 0.0358; avg_loss: 0.0300
20-03-20 05:21-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9854
20-03-20 05:21-INFO-
20-03-20 05:23-INFO-Epoch 13, Batch 508, Global step 27600:
20-03-20 05:23-INFO-training batch loss: 0.0451; avg_loss: 0.0294
20-03-20 05:23-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9861
20-03-20 05:23-INFO-
20-03-20 05:25-INFO-Epoch 13, Batch 608, Global step 27700:
20-03-20 05:25-INFO-training batch loss: 0.0169; avg_loss: 0.0290
20-03-20 05:25-INFO-training batch accuracy: 1.0000; avg_accuracy: 0.9864
20-03-20 05:25-INFO-
20-03-20 05:27-INFO-Epoch 13, Batch 708, Global step 27800:
20-03-20 05:27-INFO-training batch loss: 0.0387; avg_loss: 0.0292
20-03-20 05:27-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9865
20-03-20 05:27-INFO-
20-03-20 05:29-INFO-Epoch 13, Batch 808, Global step 27900:
20-03-20 05:29-INFO-training batch loss: 0.0420; avg_loss: 0.0292
20-03-20 05:29-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9867
20-03-20 05:29-INFO-
20-03-20 05:31-INFO-Epoch 13, Batch 908, Global step 28000:
20-03-20 05:31-INFO-training batch loss: 0.0421; avg_loss: 0.0292
20-03-20 05:31-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9868
20-03-20 05:31-INFO-
20-03-20 05:33-INFO-Epoch 13, Batch 1008, Global step 28100:
20-03-20 05:33-INFO-training batch loss: 0.0103; avg_loss: 0.0292
20-03-20 05:33-INFO-training batch accuracy: 1.0000; avg_accuracy: 0.9870
20-03-20 05:33-INFO-
20-03-20 05:35-INFO-Epoch 13, Batch 1108, Global step 28200:
20-03-20 05:35-INFO-training batch loss: 0.0334; avg_loss: 0.0290
20-03-20 05:35-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9872
20-03-20 05:35-INFO-
20-03-20 05:37-INFO-Epoch 13, Batch 1208, Global step 28300:
20-03-20 05:37-INFO-training batch loss: 0.0076; avg_loss: 0.0289
20-03-20 05:37-INFO-training batch accuracy: 1.0000; avg_accuracy: 0.9873
20-03-20 05:37-INFO-
20-03-20 05:39-INFO-Epoch 13, Batch 1308, Global step 28400:
20-03-20 05:39-INFO-training batch loss: 0.0196; avg_loss: 0.0292
20-03-20 05:39-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9873
20-03-20 05:39-INFO-
20-03-20 05:41-INFO-Epoch 13, Batch 1408, Global step 28500:
20-03-20 05:41-INFO-training batch loss: 0.0191; avg_loss: 0.0291
20-03-20 05:41-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9874
20-03-20 05:41-INFO-
20-03-20 05:43-INFO-Epoch 13, Batch 1508, Global step 28600:
20-03-20 05:43-INFO-training batch loss: 0.0244; avg_loss: 0.0291
20-03-20 05:43-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9874
20-03-20 05:43-INFO-
20-03-20 05:45-INFO-Epoch 13, Batch 1608, Global step 28700:
20-03-20 05:45-INFO-training batch loss: 0.0177; avg_loss: 0.0292
20-03-20 05:45-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9874
20-03-20 05:45-INFO-
20-03-20 05:47-INFO-Epoch 13, Batch 1708, Global step 28800:
20-03-20 05:47-INFO-training batch loss: 0.0203; avg_loss: 0.0293
20-03-20 05:47-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9875
20-03-20 05:47-INFO-
20-03-20 05:49-INFO-Epoch 13, Batch 1808, Global step 28900:
20-03-20 05:49-INFO-training batch loss: 0.0080; avg_loss: 0.0293
20-03-20 05:49-INFO-training batch accuracy: 1.0000; avg_accuracy: 0.9875
20-03-20 05:49-INFO-
20-03-20 05:51-INFO-Epoch 13, Batch 1908, Global step 29000:
20-03-20 05:51-INFO-training batch loss: 0.0810; avg_loss: 0.0292
20-03-20 05:51-INFO-training batch accuracy: 0.9609; avg_accuracy: 0.9876
20-03-20 05:51-INFO-
20-03-20 05:53-INFO-Epoch 13, Batch 2008, Global step 29100:
20-03-20 05:53-INFO-training batch loss: 0.0185; avg_loss: 0.0290
20-03-20 05:53-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9878
20-03-20 05:53-INFO-
20-03-20 05:55-INFO-training batch accuracy: 0.9762; avg_accuracy: 0.9882
20-03-20 05:55-INFO-
20-03-20 05:57-INFO-Epoch 13, evaluating batch loss: 0.0501; avg_loss: 0.0484
20-03-20 05:57-INFO-evaluating batch accuracy: 0.9808; avg_accuracy: 0.9817

20-03-20 05:57-INFO-
20-03-20 05:57-INFO-Epoch 14, Batch 24, Global step 29200:
20-03-20 05:57-INFO-training batch loss: 0.0073; avg_loss: 0.0287
20-03-20 05:57-INFO-training batch accuracy: 1.0000; avg_accuracy: 0.9463
20-03-20 05:57-INFO-
20-03-20 05:59-INFO-Epoch 14, Batch 124, Global step 29300:
20-03-20 05:59-INFO-training batch loss: 0.0204; avg_loss: 0.0258
20-03-20 05:59-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9810
20-03-20 05:59-INFO-
20-03-20 06:01-INFO-Epoch 14, Batch 224, Global step 29400:
20-03-20 06:01-INFO-training batch loss: 0.0123; avg_loss: 0.0268
20-03-20 06:01-INFO-training batch accuracy: 1.0000; avg_accuracy: 0.9845
20-03-20 06:01-INFO-
20-03-20 06:03-INFO-Epoch 14, Batch 324, Global step 29500:
20-03-20 06:03-INFO-training batch loss: 0.0145; avg_loss: 0.0267
20-03-20 06:03-INFO-training batch accuracy: 1.0000; avg_accuracy: 0.9861
20-03-20 06:03-INFO-
20-03-20 06:05-INFO-Epoch 14, Batch 424, Global step 29600:
20-03-20 06:05-INFO-training batch loss: 0.0175; avg_loss: 0.0275
20-03-20 06:05-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9864
20-03-20 06:05-INFO-
20-03-20 06:07-INFO-Epoch 14, Batch 524, Global step 29700:
20-03-20 06:07-INFO-training batch loss: 0.0298; avg_loss: 0.0274
20-03-20 06:07-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9870
20-03-20 06:07-INFO-
20-03-20 06:09-INFO-Epoch 14, Batch 624, Global step 29800:
20-03-20 06:09-INFO-training batch loss: 0.0292; avg_loss: 0.0272
20-03-20 06:09-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9873
20-03-20 06:09-INFO-
20-03-20 06:11-INFO-Epoch 14, Batch 724, Global step 29900:
20-03-20 06:11-INFO-training batch loss: 0.0336; avg_loss: 0.0279
20-03-20 06:11-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9872
20-03-20 06:11-INFO-
20-03-20 06:13-INFO-Epoch 14, Batch 824, Global step 30000:
20-03-20 06:13-INFO-training batch loss: 0.0396; avg_loss: 0.0280
20-03-20 06:13-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9873
20-03-20 06:13-INFO-
20-03-20 06:15-INFO-Epoch 14, Batch 924, Global step 30100:
20-03-20 06:15-INFO-training batch loss: 0.0025; avg_loss: 0.0283
20-03-20 06:15-INFO-training batch accuracy: 1.0000; avg_accuracy: 0.9874
20-03-20 06:15-INFO-
20-03-20 06:17-INFO-Epoch 14, Batch 1024, Global step 30200:
20-03-20 06:17-INFO-training batch loss: 0.0576; avg_loss: 0.0283
20-03-20 06:17-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9875
20-03-20 06:17-INFO-
20-03-20 06:19-INFO-Epoch 14, Batch 1124, Global step 30300:
20-03-20 06:19-INFO-training batch loss: 0.0077; avg_loss: 0.0285
20-03-20 06:19-INFO-training batch accuracy: 1.0000; avg_accuracy: 0.9876
20-03-20 06:19-INFO-
20-03-20 06:21-INFO-Epoch 14, Batch 1224, Global step 30400:
20-03-20 06:21-INFO-training batch loss: 0.0263; avg_loss: 0.0284
20-03-20 06:21-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9877
20-03-20 06:21-INFO-
20-03-20 06:23-INFO-Epoch 14, Batch 1324, Global step 30500:
20-03-20 06:23-INFO-training batch loss: 0.0304; avg_loss: 0.0286
20-03-20 06:23-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9877
20-03-20 06:23-INFO-
20-03-20 06:25-INFO-Epoch 14, Batch 1424, Global step 30600:
20-03-20 06:25-INFO-training batch loss: 0.0435; avg_loss: 0.0285
20-03-20 06:25-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9878
20-03-20 06:25-INFO-
20-03-20 06:27-INFO-Epoch 14, Batch 1524, Global step 30700:
20-03-20 06:27-INFO-training batch loss: 0.0379; avg_loss: 0.0286
20-03-20 06:27-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9878
20-03-20 06:27-INFO-
20-03-20 06:30-INFO-Epoch 14, Batch 1624, Global step 30800:
20-03-20 06:30-INFO-training batch loss: 0.0089; avg_loss: 0.0284
20-03-20 06:30-INFO-training batch accuracy: 1.0000; avg_accuracy: 0.9878
20-03-20 06:30-INFO-
20-03-20 06:32-INFO-Epoch 14, Batch 1724, Global step 30900:
20-03-20 06:32-INFO-training batch loss: 0.0267; avg_loss: 0.0284
20-03-20 06:32-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9879
20-03-20 06:32-INFO-
20-03-20 06:34-INFO-Epoch 14, Batch 1824, Global step 31000:
20-03-20 06:34-INFO-training batch loss: 0.0124; avg_loss: 0.0284
20-03-20 06:34-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9879
20-03-20 06:34-INFO-
20-03-20 06:36-INFO-Epoch 14, Batch 1924, Global step 31100:
20-03-20 06:36-INFO-training batch loss: 0.0190; avg_loss: 0.0282
20-03-20 06:36-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9881
20-03-20 06:36-INFO-
20-03-20 06:38-INFO-Epoch 14, Batch 2024, Global step 31200:
20-03-20 06:38-INFO-training batch loss: 0.0154; avg_loss: 0.0280
20-03-20 06:38-INFO-training batch accuracy: 1.0000; avg_accuracy: 0.9882
20-03-20 06:38-INFO-
20-03-20 06:39-INFO-training batch accuracy: 0.9762; avg_accuracy: 0.9886
20-03-20 06:39-INFO-
20-03-20 06:41-INFO-Epoch 14, evaluating batch loss: 0.0428; avg_loss: 0.0479
20-03-20 06:41-INFO-evaluating batch accuracy: 0.9808; avg_accuracy: 0.9802

20-03-20 06:41-INFO-
20-03-20 06:42-INFO-Epoch 15, Batch 40, Global step 31300:
20-03-20 06:42-INFO-training batch loss: 0.0589; avg_loss: 0.0245
20-03-20 06:42-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9654
20-03-20 06:42-INFO-
20-03-20 06:44-INFO-Epoch 15, Batch 140, Global step 31400:
20-03-20 06:44-INFO-training batch loss: 0.0576; avg_loss: 0.0261
20-03-20 06:44-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9818
20-03-20 06:44-INFO-
20-03-20 06:45-INFO-Epoch 15, Batch 240, Global step 31500:
20-03-20 06:45-INFO-training batch loss: 0.0212; avg_loss: 0.0263
20-03-20 06:45-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9849
20-03-20 06:45-INFO-
20-03-20 06:47-INFO-Epoch 15, Batch 340, Global step 31600:
20-03-20 06:47-INFO-training batch loss: 0.0190; avg_loss: 0.0265
20-03-20 06:47-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9860
20-03-20 06:47-INFO-
20-03-20 06:49-INFO-Epoch 15, Batch 440, Global step 31700:
20-03-20 06:49-INFO-training batch loss: 0.0421; avg_loss: 0.0269
20-03-20 06:49-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9863
20-03-20 06:49-INFO-
20-03-20 06:51-INFO-Epoch 15, Batch 540, Global step 31800:
20-03-20 06:51-INFO-training batch loss: 0.0041; avg_loss: 0.0266
20-03-20 06:51-INFO-training batch accuracy: 1.0000; avg_accuracy: 0.9870
20-03-20 06:51-INFO-
20-03-20 06:53-INFO-Epoch 15, Batch 640, Global step 31900:
20-03-20 06:53-INFO-training batch loss: 0.0237; avg_loss: 0.0269
20-03-20 06:53-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9870
20-03-20 06:53-INFO-
20-03-20 06:55-INFO-Epoch 15, Batch 740, Global step 32000:
20-03-20 06:55-INFO-training batch loss: 0.0094; avg_loss: 0.0271
20-03-20 06:55-INFO-training batch accuracy: 1.0000; avg_accuracy: 0.9872
20-03-20 06:55-INFO-
20-03-20 06:58-INFO-Epoch 15, Batch 840, Global step 32100:
20-03-20 06:58-INFO-training batch loss: 0.0107; avg_loss: 0.0272
20-03-20 06:58-INFO-training batch accuracy: 1.0000; avg_accuracy: 0.9873
20-03-20 06:58-INFO-
20-03-20 07:00-INFO-Epoch 15, Batch 940, Global step 32200:
20-03-20 07:00-INFO-training batch loss: 0.0417; avg_loss: 0.0276
20-03-20 07:00-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9874
20-03-20 07:00-INFO-
20-03-20 07:02-INFO-Epoch 15, Batch 1040, Global step 32300:
20-03-20 07:02-INFO-training batch loss: 0.0248; avg_loss: 0.0275
20-03-20 07:02-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9877
20-03-20 07:02-INFO-
20-03-20 07:04-INFO-Epoch 15, Batch 1140, Global step 32400:
20-03-20 07:04-INFO-training batch loss: 0.0546; avg_loss: 0.0276
20-03-20 07:04-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9877
20-03-20 07:04-INFO-
20-03-20 07:06-INFO-Epoch 15, Batch 1240, Global step 32500:
20-03-20 07:06-INFO-training batch loss: 0.0591; avg_loss: 0.0277
20-03-20 07:06-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9878
20-03-20 07:06-INFO-
20-03-20 07:08-INFO-Epoch 15, Batch 1340, Global step 32600:
20-03-20 07:08-INFO-training batch loss: 0.0642; avg_loss: 0.0276
20-03-20 07:08-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9879
20-03-20 07:08-INFO-
20-03-20 07:10-INFO-Epoch 15, Batch 1440, Global step 32700:
20-03-20 07:10-INFO-training batch loss: 0.0157; avg_loss: 0.0276
20-03-20 07:10-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9880
20-03-20 07:10-INFO-
20-03-20 07:12-INFO-Epoch 15, Batch 1540, Global step 32800:
20-03-20 07:12-INFO-training batch loss: 0.0125; avg_loss: 0.0276
20-03-20 07:12-INFO-training batch accuracy: 1.0000; avg_accuracy: 0.9881
20-03-20 07:12-INFO-
20-03-20 07:14-INFO-Epoch 15, Batch 1640, Global step 32900:
20-03-20 07:14-INFO-training batch loss: 0.0309; avg_loss: 0.0274
20-03-20 07:14-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9882
20-03-20 07:14-INFO-
20-03-20 07:16-INFO-Epoch 15, Batch 1740, Global step 33000:
20-03-20 07:16-INFO-training batch loss: 0.0293; avg_loss: 0.0274
20-03-20 07:16-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9883
20-03-20 07:16-INFO-
20-03-20 07:18-INFO-Epoch 15, Batch 1840, Global step 33100:
20-03-20 07:18-INFO-training batch loss: 0.0346; avg_loss: 0.0275
20-03-20 07:18-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9883
20-03-20 07:18-INFO-
20-03-20 07:20-INFO-Epoch 15, Batch 1940, Global step 33200:
20-03-20 07:20-INFO-training batch loss: 0.0199; avg_loss: 0.0274
20-03-20 07:20-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9884
20-03-20 07:20-INFO-
20-03-20 07:22-INFO-Epoch 15, Batch 2040, Global step 33300:
20-03-20 07:22-INFO-training batch loss: 0.0141; avg_loss: 0.0274
20-03-20 07:22-INFO-training batch accuracy: 1.0000; avg_accuracy: 0.9885
20-03-20 07:22-INFO-
20-03-20 07:23-INFO-training batch accuracy: 0.9762; avg_accuracy: 0.9889
20-03-20 07:23-INFO-
20-03-20 07:25-INFO-Epoch 15, evaluating batch loss: 0.0470; avg_loss: 0.0471
20-03-20 07:25-INFO-evaluating batch accuracy: 0.9808; avg_accuracy: 0.9832

20-03-20 07:25-INFO-
20-03-20 07:26-INFO-Epoch 16, Batch 56, Global step 33400:
20-03-20 07:26-INFO-training batch loss: 0.0098; avg_loss: 0.0253
20-03-20 07:26-INFO-training batch accuracy: 1.0000; avg_accuracy: 0.9718
20-03-20 07:26-INFO-
20-03-20 07:28-INFO-Epoch 16, Batch 156, Global step 33500:
20-03-20 07:28-INFO-training batch loss: 0.0156; avg_loss: 0.0263
20-03-20 07:28-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9831
20-03-20 07:28-INFO-
20-03-20 07:30-INFO-Epoch 16, Batch 256, Global step 33600:
20-03-20 07:30-INFO-training batch loss: 0.0094; avg_loss: 0.0267
20-03-20 07:30-INFO-training batch accuracy: 1.0000; avg_accuracy: 0.9856
20-03-20 07:30-INFO-
20-03-20 07:32-INFO-Epoch 16, Batch 356, Global step 33700:
20-03-20 07:32-INFO-training batch loss: 0.0267; avg_loss: 0.0263
20-03-20 07:32-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9869
20-03-20 07:32-INFO-
20-03-20 07:34-INFO-Epoch 16, Batch 456, Global step 33800:
20-03-20 07:34-INFO-training batch loss: 0.0316; avg_loss: 0.0267
20-03-20 07:34-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9874
20-03-20 07:34-INFO-
20-03-20 07:36-INFO-Epoch 16, Batch 556, Global step 33900:
20-03-20 07:36-INFO-training batch loss: 0.0545; avg_loss: 0.0265
20-03-20 07:36-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9879
20-03-20 07:36-INFO-
20-03-20 07:38-INFO-Epoch 16, Batch 656, Global step 34000:
20-03-20 07:38-INFO-training batch loss: 0.0328; avg_loss: 0.0266
20-03-20 07:38-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9880
20-03-20 07:38-INFO-
20-03-20 07:40-INFO-Epoch 16, Batch 756, Global step 34100:
20-03-20 07:40-INFO-training batch loss: 0.0082; avg_loss: 0.0267
20-03-20 07:40-INFO-training batch accuracy: 1.0000; avg_accuracy: 0.9881
20-03-20 07:40-INFO-
20-03-20 07:42-INFO-Epoch 16, Batch 856, Global step 34200:
20-03-20 07:42-INFO-training batch loss: 0.0415; avg_loss: 0.0270
20-03-20 07:42-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9881
20-03-20 07:42-INFO-
20-03-20 07:44-INFO-Epoch 16, Batch 956, Global step 34300:
20-03-20 07:44-INFO-training batch loss: 0.0214; avg_loss: 0.0270
20-03-20 07:44-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9883
20-03-20 07:44-INFO-
20-03-20 07:46-INFO-Epoch 16, Batch 1056, Global step 34400:
20-03-20 07:46-INFO-training batch loss: 0.0143; avg_loss: 0.0271
20-03-20 07:46-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9884
20-03-20 07:46-INFO-
20-03-20 07:48-INFO-Epoch 16, Batch 1156, Global step 34500:
20-03-20 07:48-INFO-training batch loss: 0.0736; avg_loss: 0.0271
20-03-20 07:48-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9884
20-03-20 07:48-INFO-
20-03-20 07:50-INFO-Epoch 16, Batch 1256, Global step 34600:
20-03-20 07:50-INFO-training batch loss: 0.0119; avg_loss: 0.0271
20-03-20 07:50-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9884
20-03-20 07:50-INFO-
20-03-20 07:52-INFO-Epoch 16, Batch 1356, Global step 34700:
20-03-20 07:52-INFO-training batch loss: 0.0462; avg_loss: 0.0271
20-03-20 07:52-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9885
20-03-20 07:52-INFO-
20-03-20 07:54-INFO-Epoch 16, Batch 1456, Global step 34800:
20-03-20 07:54-INFO-training batch loss: 0.0617; avg_loss: 0.0271
20-03-20 07:54-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9885
20-03-20 07:54-INFO-
20-03-20 07:56-INFO-Epoch 16, Batch 1556, Global step 34900:
20-03-20 07:56-INFO-training batch loss: 0.0266; avg_loss: 0.0271
20-03-20 07:56-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9886
20-03-20 07:56-INFO-
20-03-20 07:58-INFO-Epoch 16, Batch 1656, Global step 35000:
20-03-20 07:58-INFO-training batch loss: 0.0519; avg_loss: 0.0270
20-03-20 07:58-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9887
20-03-20 07:58-INFO-
20-03-20 08:00-INFO-Epoch 16, Batch 1756, Global step 35100:
20-03-20 08:00-INFO-training batch loss: 0.0324; avg_loss: 0.0270
20-03-20 08:00-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9887
20-03-20 08:00-INFO-
20-03-20 08:02-INFO-Epoch 16, Batch 1856, Global step 35200:
20-03-20 08:02-INFO-training batch loss: 0.0095; avg_loss: 0.0270
20-03-20 08:02-INFO-training batch accuracy: 1.0000; avg_accuracy: 0.9887
20-03-20 08:02-INFO-
20-03-20 08:04-INFO-Epoch 16, Batch 1956, Global step 35300:
20-03-20 08:04-INFO-training batch loss: 0.0118; avg_loss: 0.0268
20-03-20 08:04-INFO-training batch accuracy: 1.0000; avg_accuracy: 0.9888
20-03-20 08:04-INFO-
20-03-20 08:06-INFO-Epoch 16, Batch 2056, Global step 35400:
20-03-20 08:06-INFO-training batch loss: 0.0061; avg_loss: 0.0267
20-03-20 08:06-INFO-training batch accuracy: 1.0000; avg_accuracy: 0.9888
20-03-20 08:06-INFO-
20-03-20 08:06-INFO-training batch accuracy: 0.9762; avg_accuracy: 0.9893
20-03-20 08:06-INFO-
20-03-20 08:08-INFO-Epoch 16, evaluating batch loss: 0.0411; avg_loss: 0.0427
20-03-20 08:08-INFO-evaluating batch accuracy: 0.9808; avg_accuracy: 0.9834

20-03-20 08:08-INFO-
20-03-20 08:10-INFO-Epoch 17, Batch 72, Global step 35500:
20-03-20 08:10-INFO-training batch loss: 0.0355; avg_loss: 0.0245
20-03-20 08:10-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9750
20-03-20 08:10-INFO-
20-03-20 08:12-INFO-Epoch 17, Batch 172, Global step 35600:
20-03-20 08:12-INFO-training batch loss: 0.0226; avg_loss: 0.0255
20-03-20 08:12-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9833
20-03-20 08:12-INFO-
20-03-20 08:14-INFO-Epoch 17, Batch 272, Global step 35700:
20-03-20 08:14-INFO-training batch loss: 0.0150; avg_loss: 0.0259
20-03-20 08:14-INFO-training batch accuracy: 1.0000; avg_accuracy: 0.9856
20-03-20 08:14-INFO-
20-03-20 08:16-INFO-Epoch 17, Batch 372, Global step 35800:
20-03-20 08:16-INFO-training batch loss: 0.0176; avg_loss: 0.0255
20-03-20 08:16-INFO-training batch accuracy: 1.0000; avg_accuracy: 0.9869
20-03-20 08:16-INFO-
20-03-20 08:18-INFO-Epoch 17, Batch 472, Global step 35900:
20-03-20 08:18-INFO-training batch loss: 0.0429; avg_loss: 0.0257
20-03-20 08:18-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9874
20-03-20 08:18-INFO-
20-03-20 08:20-INFO-Epoch 17, Batch 572, Global step 36000:
20-03-20 08:20-INFO-training batch loss: 0.0235; avg_loss: 0.0264
20-03-20 08:20-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9875
20-03-20 08:20-INFO-
20-03-20 08:22-INFO-Epoch 17, Batch 672, Global step 36100:
20-03-20 08:22-INFO-training batch loss: 0.0033; avg_loss: 0.0268
20-03-20 08:22-INFO-training batch accuracy: 1.0000; avg_accuracy: 0.9876
20-03-20 08:22-INFO-
20-03-20 08:24-INFO-Epoch 17, Batch 772, Global step 36200:
20-03-20 08:24-INFO-training batch loss: 0.0069; avg_loss: 0.0272
20-03-20 08:24-INFO-training batch accuracy: 1.0000; avg_accuracy: 0.9877
20-03-20 08:24-INFO-
20-03-20 08:26-INFO-Epoch 17, Batch 872, Global step 36300:
20-03-20 08:26-INFO-training batch loss: 0.0408; avg_loss: 0.0273
20-03-20 08:26-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9878
20-03-20 08:26-INFO-
20-03-20 08:28-INFO-Epoch 17, Batch 972, Global step 36400:
20-03-20 08:28-INFO-training batch loss: 0.0106; avg_loss: 0.0273
20-03-20 08:28-INFO-training batch accuracy: 1.0000; avg_accuracy: 0.9879
20-03-20 08:28-INFO-
20-03-20 08:30-INFO-Epoch 17, Batch 1072, Global step 36500:
20-03-20 08:30-INFO-training batch loss: 0.0372; avg_loss: 0.0273
20-03-20 08:30-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9881
20-03-20 08:30-INFO-
20-03-20 08:32-INFO-Epoch 17, Batch 1172, Global step 36600:
20-03-20 08:32-INFO-training batch loss: 0.0337; avg_loss: 0.0271
20-03-20 08:32-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9882
20-03-20 08:32-INFO-
20-03-20 08:34-INFO-Epoch 17, Batch 1272, Global step 36700:
20-03-20 08:34-INFO-training batch loss: 0.0328; avg_loss: 0.0274
20-03-20 08:34-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9881
20-03-20 08:34-INFO-
20-03-20 08:36-INFO-Epoch 17, Batch 1372, Global step 36800:
20-03-20 08:36-INFO-training batch loss: 0.0224; avg_loss: 0.0273
20-03-20 08:36-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9882
20-03-20 08:36-INFO-
20-03-20 08:38-INFO-Epoch 17, Batch 1472, Global step 36900:
20-03-20 08:38-INFO-training batch loss: 0.0511; avg_loss: 0.0273
20-03-20 08:38-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9883
20-03-20 08:38-INFO-
20-03-20 08:40-INFO-Epoch 17, Batch 1572, Global step 37000:
20-03-20 08:40-INFO-training batch loss: 0.0551; avg_loss: 0.0271
20-03-20 08:40-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9884
20-03-20 08:40-INFO-
20-03-20 08:42-INFO-Epoch 17, Batch 1672, Global step 37100:
20-03-20 08:42-INFO-training batch loss: 0.0215; avg_loss: 0.0271
20-03-20 08:42-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9884
20-03-20 08:42-INFO-
20-03-20 08:44-INFO-Epoch 17, Batch 1772, Global step 37200:
20-03-20 08:44-INFO-training batch loss: 0.0282; avg_loss: 0.0271
20-03-20 08:44-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9885
20-03-20 08:44-INFO-
20-03-20 08:46-INFO-Epoch 17, Batch 1872, Global step 37300:
20-03-20 08:46-INFO-training batch loss: 0.0109; avg_loss: 0.0270
20-03-20 08:46-INFO-training batch accuracy: 1.0000; avg_accuracy: 0.9886
20-03-20 08:46-INFO-
20-03-20 08:48-INFO-Epoch 17, Batch 1972, Global step 37400:
20-03-20 08:48-INFO-training batch loss: 0.0019; avg_loss: 0.0269
20-03-20 08:48-INFO-training batch accuracy: 1.0000; avg_accuracy: 0.9887
20-03-20 08:48-INFO-
20-03-20 08:50-INFO-Epoch 17, Batch 2072, Global step 37500:
20-03-20 08:50-INFO-training batch loss: 0.4618; avg_loss: 0.0276
20-03-20 08:50-INFO-training batch accuracy: 0.9375; avg_accuracy: 0.9886
20-03-20 08:50-INFO-
20-03-20 08:50-INFO-training batch accuracy: 0.9524; avg_accuracy: 0.9889
20-03-20 08:50-INFO-
20-03-20 08:52-INFO-Epoch 17, evaluating batch loss: 0.0430; avg_loss: 0.0550
20-03-20 08:52-INFO-evaluating batch accuracy: 0.9808; avg_accuracy: 0.9781

20-03-20 08:52-INFO-
20-03-20 08:54-INFO-Epoch 18, Batch 88, Global step 37600:
20-03-20 08:54-INFO-training batch loss: 0.0371; avg_loss: 0.0301
20-03-20 08:54-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9763
20-03-20 08:54-INFO-
20-03-20 08:56-INFO-Epoch 18, Batch 188, Global step 37700:
20-03-20 08:56-INFO-training batch loss: 0.0392; avg_loss: 0.0562
20-03-20 08:56-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9734
20-03-20 08:56-INFO-
20-03-20 08:58-INFO-Epoch 18, Batch 288, Global step 37800:
20-03-20 08:58-INFO-training batch loss: 0.0600; avg_loss: 0.0592
20-03-20 08:58-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9735
20-03-20 08:58-INFO-
20-03-20 09:00-INFO-Epoch 18, Batch 388, Global step 37900:
20-03-20 09:00-INFO-training batch loss: 0.0711; avg_loss: 0.0592
20-03-20 09:00-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9743
20-03-20 09:00-INFO-
20-03-20 09:02-INFO-Epoch 18, Batch 488, Global step 38000:
20-03-20 09:02-INFO-training batch loss: 0.0312; avg_loss: 0.0577
20-03-20 09:02-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9753
20-03-20 09:02-INFO-
20-03-20 09:04-INFO-Epoch 18, Batch 588, Global step 38100:
20-03-20 09:04-INFO-training batch loss: 0.0237; avg_loss: 0.0522
20-03-20 09:04-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9777
20-03-20 09:04-INFO-
20-03-20 09:06-INFO-Epoch 18, Batch 688, Global step 38200:
20-03-20 09:06-INFO-training batch loss: 0.0632; avg_loss: 0.0484
20-03-20 09:06-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9793
20-03-20 09:06-INFO-
20-03-20 09:08-INFO-Epoch 18, Batch 788, Global step 38300:
20-03-20 09:08-INFO-training batch loss: 0.0159; avg_loss: 0.0456
20-03-20 09:08-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9805
20-03-20 09:08-INFO-
20-03-20 09:10-INFO-Epoch 18, Batch 888, Global step 38400:
20-03-20 09:10-INFO-training batch loss: 0.0081; avg_loss: 0.0434
20-03-20 09:10-INFO-training batch accuracy: 1.0000; avg_accuracy: 0.9815
20-03-20 09:10-INFO-
20-03-20 09:12-INFO-Epoch 18, Batch 988, Global step 38500:
20-03-20 09:12-INFO-training batch loss: 0.0147; avg_loss: 0.0416
20-03-20 09:12-INFO-training batch accuracy: 1.0000; avg_accuracy: 0.9824
20-03-20 09:12-INFO-
20-03-20 09:14-INFO-Epoch 18, Batch 1088, Global step 38600:
20-03-20 09:14-INFO-training batch loss: 0.0060; avg_loss: 0.0403
20-03-20 09:14-INFO-training batch accuracy: 1.0000; avg_accuracy: 0.9831
20-03-20 09:14-INFO-
20-03-20 09:16-INFO-Epoch 18, Batch 1188, Global step 38700:
20-03-20 09:16-INFO-training batch loss: 0.0334; avg_loss: 0.0390
20-03-20 09:16-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9836
20-03-20 09:16-INFO-
20-03-20 09:18-INFO-Epoch 18, Batch 1288, Global step 38800:
20-03-20 09:18-INFO-training batch loss: 0.0200; avg_loss: 0.0382
20-03-20 09:18-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9839
20-03-20 09:18-INFO-
20-03-20 09:20-INFO-Epoch 18, Batch 1388, Global step 38900:
20-03-20 09:20-INFO-training batch loss: 0.0258; avg_loss: 0.0376
20-03-20 09:20-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9842
20-03-20 09:20-INFO-
20-03-20 09:22-INFO-Epoch 18, Batch 1488, Global step 39000:
20-03-20 09:22-INFO-training batch loss: 0.0405; avg_loss: 0.0367
20-03-20 09:22-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9847
20-03-20 09:22-INFO-
20-03-20 09:24-INFO-Epoch 18, Batch 1588, Global step 39100:
20-03-20 09:24-INFO-training batch loss: 0.0386; avg_loss: 0.0361
20-03-20 09:24-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9850
20-03-20 09:24-INFO-
20-03-20 09:26-INFO-Epoch 18, Batch 1688, Global step 39200:
20-03-20 09:26-INFO-training batch loss: 0.0100; avg_loss: 0.0355
20-03-20 09:26-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9852
20-03-20 09:26-INFO-
20-03-20 09:28-INFO-Epoch 18, Batch 1788, Global step 39300:
20-03-20 09:28-INFO-training batch loss: 0.0212; avg_loss: 0.0351
20-03-20 09:28-INFO-training batch accuracy: 1.0000; avg_accuracy: 0.9854
20-03-20 09:28-INFO-
20-03-20 09:30-INFO-Epoch 18, Batch 1888, Global step 39400:
20-03-20 09:30-INFO-training batch loss: 0.0242; avg_loss: 0.0344
20-03-20 09:30-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9857
20-03-20 09:30-INFO-
20-03-20 09:32-INFO-Epoch 18, Batch 1988, Global step 39500:
20-03-20 09:32-INFO-training batch loss: 0.0059; avg_loss: 0.0338
20-03-20 09:32-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9860
20-03-20 09:32-INFO-
20-03-20 09:34-INFO-training batch accuracy: 0.9524; avg_accuracy: 0.9865
20-03-20 09:34-INFO-
20-03-20 09:36-INFO-Epoch 18, evaluating batch loss: 0.1072; avg_loss: 0.1080
20-03-20 09:36-INFO-evaluating batch accuracy: 0.9519; avg_accuracy: 0.9576

20-03-20 09:36-INFO-
