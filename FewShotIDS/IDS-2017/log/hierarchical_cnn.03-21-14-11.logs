20-03-21 14:11-INFO-{'session_length': 24, 'height': 32, 'width': 32, 'num_labels': 13, 'learning_rate': 0.0005, 'filter_sizes': [3, 4, 5, 6], 'num_filters': 64, 'filter_sizes_hierarchical': [3, 4, 5], 'num_fitlers_hierarchical': 64, 'is_train': True, 'early_stop': True, 'is_pretrain': False, 'is_time': False, 'is_size': False, 'uncertainty': False, 'is_tuning': True}
20-03-21 14:11-WARNING-From ../utils.py:127: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

20-03-21 14:11-WARNING-From ../model/train.py:79: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.

20-03-21 14:11-WARNING-From ../model/base_model.py:46: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

20-03-21 14:11-WARNING-From ../model/hierarchical_model.py:46: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

20-03-21 14:11-WARNING-From ../model/hierarchical_model.py:46: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.

20-03-21 14:11-WARNING-From ../model/utils/utils.py:25: conv1d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv1D` instead.
20-03-21 14:11-WARNING-Entity <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7f3cb26724d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7f3cb26724d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-21 14:11-WARNING-From ../model/utils/utils.py:44: max_pooling1d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.MaxPooling1D instead.
20-03-21 14:11-WARNING-Entity <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7f3cb26724d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7f3cb26724d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-21 14:11-WARNING-Entity <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7f3cb2679650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7f3cb2679650>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-21 14:11-WARNING-Entity <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7f3cb25e6a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7f3cb25e6a90>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-21 14:11-WARNING-Entity <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7f3cb2672790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7f3cb2672790>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-21 14:11-WARNING-Entity <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7f3cb2679350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7f3cb2679350>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-21 14:11-WARNING-Entity <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7f3cb2679510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7f3cb2679510>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-21 14:11-WARNING-Entity <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7f3cb25e6290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7f3cb25e6290>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-21 14:11-WARNING-From ../model/utils/modules.py:207: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.dropout instead.
20-03-21 14:11-WARNING-Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7f3cb25eb210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7f3cb25eb210>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-21 14:11-WARNING-Variable *= will be deprecated. Use `var.assign(var * other)` if you want assignment to the variable value or `x = x * y` if you want a new python Tensor object.
20-03-21 14:11-WARNING-Entity <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7f3cb1a5d290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7f3cb1a5d290>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-21 14:11-WARNING-Entity <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7f3cb25d4c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7f3cb25d4c50>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-21 14:11-WARNING-Entity <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7f3cb19a3a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7f3cb19a3a50>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-21 14:11-WARNING-Entity <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7f3cb19a3810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7f3cb19a3810>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-21 14:11-WARNING-Entity <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7f3cb25d4950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7f3cb25d4950>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-21 14:11-WARNING-Entity <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7f3cb25d4c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7f3cb25d4c50>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-21 14:11-WARNING-From ../model/utils/modules.py:242: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.dense instead.
20-03-21 14:11-WARNING-Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f3cb19f0cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f3cb19f0cd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-21 14:11-WARNING-From ../model/utils/modules.py:244: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).
20-03-21 14:11-WARNING-Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f3cb1a5d890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f3cb1a5d890>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-21 14:11-WARNING-From ../model/utils/modules.py:247: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
20-03-21 14:11-WARNING-Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f3cb1a8aed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f3cb1a8aed0>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-21 14:11-WARNING-Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f3cb1a8aed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f3cb1a8aed0>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-21 14:11-WARNING-From ../model/base_model.py:132: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.

20-03-21 14:11-WARNING-From /root/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/clip_ops.py:286: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
20-03-21 14:11-WARNING-From ../model/train.py:88: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.

20-03-21 14:11-WARNING-From /root/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
20-03-21 14:11-INFO-Restoring parameters from /root/data/mentali/IDS/ckpt/hierarchical_cnn_pretrain/hierarchical_cnn-6252
20-03-21 14:13-INFO-Epoch 0, Batch 100, Global step 100:
20-03-21 14:13-INFO-training batch loss: 0.5674; avg_loss: 1.1747
20-03-21 14:13-INFO-training batch accuracy: 0.8203; avg_accuracy: 0.5641
20-03-21 14:13-INFO-
20-03-21 14:15-INFO-Epoch 0, Batch 200, Global step 200:
20-03-21 14:15-INFO-training batch loss: 0.2662; avg_loss: 0.7600
20-03-21 14:15-INFO-training batch accuracy: 0.9219; avg_accuracy: 0.7331
20-03-21 14:15-INFO-
20-03-21 14:16-INFO-Epoch 0, Batch 300, Global step 300:
20-03-21 14:16-INFO-training batch loss: 0.1925; avg_loss: 0.5914
20-03-21 14:16-INFO-training batch accuracy: 0.9375; avg_accuracy: 0.7949
20-03-21 14:16-INFO-
20-03-21 14:18-INFO-Epoch 0, Batch 400, Global step 400:
20-03-21 14:18-INFO-training batch loss: 0.2018; avg_loss: 0.4945
20-03-21 14:18-INFO-training batch accuracy: 0.9375; avg_accuracy: 0.8302
20-03-21 14:18-INFO-
20-03-21 14:20-INFO-Epoch 0, Batch 500, Global step 500:
20-03-21 14:20-INFO-training batch loss: 0.1213; avg_loss: 0.4330
20-03-21 14:20-INFO-training batch accuracy: 0.9531; avg_accuracy: 0.8514
20-03-21 14:20-INFO-
20-03-21 14:22-INFO-Epoch 0, Batch 600, Global step 600:
20-03-21 14:22-INFO-training batch loss: 0.1973; avg_loss: 0.3894
20-03-21 14:22-INFO-training batch accuracy: 0.9297; avg_accuracy: 0.8665
20-03-21 14:22-INFO-
20-03-21 14:24-INFO-Epoch 0, Batch 700, Global step 700:
20-03-21 14:24-INFO-training batch loss: 0.2240; avg_loss: 0.3574
20-03-21 14:24-INFO-training batch accuracy: 0.9297; avg_accuracy: 0.8778
20-03-21 14:24-INFO-
20-03-21 14:26-INFO-Epoch 0, Batch 800, Global step 800:
20-03-21 14:26-INFO-training batch loss: 0.2401; avg_loss: 0.3331
20-03-21 14:26-INFO-training batch accuracy: 0.9453; avg_accuracy: 0.8861
20-03-21 14:26-INFO-
20-03-21 14:28-INFO-Epoch 0, Batch 900, Global step 900:
20-03-21 14:28-INFO-training batch loss: 0.2635; avg_loss: 0.3145
20-03-21 14:28-INFO-training batch accuracy: 0.9297; avg_accuracy: 0.8923
20-03-21 14:28-INFO-
20-03-21 14:30-INFO-Epoch 0, Batch 1000, Global step 1000:
20-03-21 14:30-INFO-training batch loss: 0.1099; avg_loss: 0.2982
20-03-21 14:30-INFO-training batch accuracy: 0.9531; avg_accuracy: 0.8977
20-03-21 14:30-INFO-
20-03-21 14:32-INFO-Epoch 0, Batch 1100, Global step 1100:
20-03-21 14:32-INFO-training batch loss: 0.1731; avg_loss: 0.2844
20-03-21 14:32-INFO-training batch accuracy: 0.9375; avg_accuracy: 0.9023
20-03-21 14:32-INFO-
20-03-21 14:34-INFO-Epoch 0, Batch 1200, Global step 1200:
20-03-21 14:34-INFO-training batch loss: 0.1889; avg_loss: 0.2723
20-03-21 14:34-INFO-training batch accuracy: 0.9297; avg_accuracy: 0.9062
20-03-21 14:34-INFO-
20-03-21 14:36-INFO-Epoch 0, Batch 1300, Global step 1300:
20-03-21 14:36-INFO-training batch loss: 0.1614; avg_loss: 0.2630
20-03-21 14:36-INFO-training batch accuracy: 0.9375; avg_accuracy: 0.9093
20-03-21 14:36-INFO-
20-03-21 14:38-INFO-Epoch 0, Batch 1400, Global step 1400:
20-03-21 14:38-INFO-training batch loss: 0.1154; avg_loss: 0.2540
20-03-21 14:38-INFO-training batch accuracy: 0.9531; avg_accuracy: 0.9121
20-03-21 14:38-INFO-
20-03-21 14:40-INFO-Epoch 0, Batch 1500, Global step 1500:
20-03-21 14:40-INFO-training batch loss: 0.0938; avg_loss: 0.2462
20-03-21 14:40-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9145
20-03-21 14:40-INFO-
20-03-21 14:42-INFO-Epoch 0, Batch 1600, Global step 1600:
20-03-21 14:42-INFO-training batch loss: 0.0693; avg_loss: 0.2391
20-03-21 14:42-INFO-training batch accuracy: 0.9609; avg_accuracy: 0.9166
20-03-21 14:42-INFO-
20-03-21 14:44-INFO-Epoch 0, Batch 1700, Global step 1700:
20-03-21 14:44-INFO-training batch loss: 0.0947; avg_loss: 0.2328
20-03-21 14:44-INFO-training batch accuracy: 0.9531; avg_accuracy: 0.9184
20-03-21 14:44-INFO-
20-03-21 14:46-INFO-Epoch 0, Batch 1800, Global step 1800:
20-03-21 14:46-INFO-training batch loss: 0.1243; avg_loss: 0.2269
20-03-21 14:46-INFO-training batch accuracy: 0.9297; avg_accuracy: 0.9201
20-03-21 14:46-INFO-
20-03-21 14:48-INFO-Epoch 0, Batch 1900, Global step 1900:
20-03-21 14:48-INFO-training batch loss: 0.1160; avg_loss: 0.2215
20-03-21 14:48-INFO-training batch accuracy: 0.9453; avg_accuracy: 0.9218
20-03-21 14:48-INFO-
20-03-21 14:50-INFO-Epoch 0, Batch 2000, Global step 2000:
20-03-21 14:50-INFO-training batch loss: 0.1622; avg_loss: 0.2162
20-03-21 14:50-INFO-training batch accuracy: 0.9453; avg_accuracy: 0.9234
20-03-21 14:50-INFO-
20-03-21 14:52-INFO-training batch accuracy: 0.9048; avg_accuracy: 0.9249
20-03-21 14:52-INFO-
20-03-21 14:54-INFO-Epoch 0, evaluating batch loss: 0.2043; avg_loss: 0.2411
20-03-21 14:54-INFO-evaluating batch accuracy: 0.9423; avg_accuracy: 0.9135

20-03-21 14:54-INFO-
20-03-21 14:54-INFO-Epoch 1, Batch 16, Global step 2100:
20-03-21 14:54-INFO-training batch loss: 0.0968; avg_loss: 0.1047
20-03-21 14:54-INFO-training batch accuracy: 0.9531; avg_accuracy: 0.8916
20-03-21 14:54-INFO-
20-03-21 14:56-INFO-Epoch 1, Batch 116, Global step 2200:
20-03-21 14:56-INFO-training batch loss: 0.0826; avg_loss: 0.1156
20-03-21 14:56-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9414
20-03-21 14:56-INFO-
20-03-21 14:58-INFO-Epoch 1, Batch 216, Global step 2300:
20-03-21 14:58-INFO-training batch loss: 0.1025; avg_loss: 0.1164
20-03-21 14:58-INFO-training batch accuracy: 0.9531; avg_accuracy: 0.9463
20-03-21 14:58-INFO-
20-03-21 15:00-INFO-Epoch 1, Batch 316, Global step 2400:
20-03-21 15:00-INFO-training batch loss: 0.1733; avg_loss: 0.1166
20-03-21 15:00-INFO-training batch accuracy: 0.9297; avg_accuracy: 0.9484
20-03-21 15:00-INFO-
20-03-21 15:02-INFO-Epoch 1, Batch 416, Global step 2500:
20-03-21 15:02-INFO-training batch loss: 0.1061; avg_loss: 0.1175
20-03-21 15:02-INFO-training batch accuracy: 0.9375; avg_accuracy: 0.9490
20-03-21 15:02-INFO-
20-03-21 15:04-INFO-Epoch 1, Batch 516, Global step 2600:
20-03-21 15:04-INFO-training batch loss: 0.0863; avg_loss: 0.1175
20-03-21 15:04-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9496
20-03-21 15:04-INFO-
20-03-21 15:06-INFO-Epoch 1, Batch 616, Global step 2700:
20-03-21 15:06-INFO-training batch loss: 0.1102; avg_loss: 0.1172
20-03-21 15:06-INFO-training batch accuracy: 0.9531; avg_accuracy: 0.9502
20-03-21 15:06-INFO-
20-03-21 15:08-INFO-Epoch 1, Batch 716, Global step 2800:
20-03-21 15:08-INFO-training batch loss: 0.1098; avg_loss: 0.1163
20-03-21 15:08-INFO-training batch accuracy: 0.9609; avg_accuracy: 0.9508
20-03-21 15:08-INFO-
20-03-21 15:10-INFO-Epoch 1, Batch 816, Global step 2900:
20-03-21 15:10-INFO-training batch loss: 0.0957; avg_loss: 0.1162
20-03-21 15:10-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9511
20-03-21 15:10-INFO-
20-03-21 15:12-INFO-Epoch 1, Batch 916, Global step 3000:
20-03-21 15:12-INFO-training batch loss: 0.1061; avg_loss: 0.1158
20-03-21 15:12-INFO-training batch accuracy: 0.9609; avg_accuracy: 0.9514
20-03-21 15:12-INFO-
20-03-21 15:14-INFO-Epoch 1, Batch 1016, Global step 3100:
20-03-21 15:14-INFO-training batch loss: 0.1291; avg_loss: 0.1156
20-03-21 15:14-INFO-training batch accuracy: 0.9609; avg_accuracy: 0.9516
20-03-21 15:14-INFO-
20-03-21 15:16-INFO-Epoch 1, Batch 1116, Global step 3200:
20-03-21 15:16-INFO-training batch loss: 0.1484; avg_loss: 0.1153
20-03-21 15:16-INFO-training batch accuracy: 0.9375; avg_accuracy: 0.9519
20-03-21 15:16-INFO-
20-03-21 15:18-INFO-Epoch 1, Batch 1216, Global step 3300:
20-03-21 15:18-INFO-training batch loss: 0.0634; avg_loss: 0.1146
20-03-21 15:18-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9522
20-03-21 15:18-INFO-
20-03-21 15:20-INFO-Epoch 1, Batch 1316, Global step 3400:
20-03-21 15:20-INFO-training batch loss: 0.1127; avg_loss: 0.1147
20-03-21 15:20-INFO-training batch accuracy: 0.9609; avg_accuracy: 0.9523
20-03-21 15:20-INFO-
20-03-21 15:22-INFO-Epoch 1, Batch 1416, Global step 3500:
20-03-21 15:22-INFO-training batch loss: 0.1055; avg_loss: 0.1140
20-03-21 15:22-INFO-training batch accuracy: 0.9453; avg_accuracy: 0.9526
20-03-21 15:22-INFO-
20-03-21 15:24-INFO-Epoch 1, Batch 1516, Global step 3600:
20-03-21 15:24-INFO-training batch loss: 0.1760; avg_loss: 0.1137
20-03-21 15:24-INFO-training batch accuracy: 0.9297; avg_accuracy: 0.9528
20-03-21 15:24-INFO-
20-03-21 15:26-INFO-Epoch 1, Batch 1616, Global step 3700:
20-03-21 15:26-INFO-training batch loss: 0.0755; avg_loss: 0.1126
20-03-21 15:26-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9531
20-03-21 15:26-INFO-
20-03-21 15:28-INFO-Epoch 1, Batch 1716, Global step 3800:
20-03-21 15:28-INFO-training batch loss: 0.0457; avg_loss: 0.1122
20-03-21 15:28-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9532
20-03-21 15:28-INFO-
20-03-21 15:30-INFO-Epoch 1, Batch 1816, Global step 3900:
20-03-21 15:30-INFO-training batch loss: 0.0901; avg_loss: 0.1117
20-03-21 15:30-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9534
20-03-21 15:30-INFO-
20-03-21 15:32-INFO-Epoch 1, Batch 1916, Global step 4000:
20-03-21 15:32-INFO-training batch loss: 0.0624; avg_loss: 0.1111
20-03-21 15:32-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9537
20-03-21 15:32-INFO-
20-03-21 15:34-INFO-Epoch 1, Batch 2016, Global step 4100:
20-03-21 15:34-INFO-training batch loss: 0.1023; avg_loss: 0.1106
20-03-21 15:34-INFO-training batch accuracy: 0.9609; avg_accuracy: 0.9539
20-03-21 15:34-INFO-
20-03-21 15:36-INFO-training batch accuracy: 0.9048; avg_accuracy: 0.9544
20-03-21 15:36-INFO-
20-03-21 15:38-INFO-Epoch 1, evaluating batch loss: 0.1473; avg_loss: 0.1945
20-03-21 15:38-INFO-evaluating batch accuracy: 0.9712; avg_accuracy: 0.9290

20-03-21 15:38-INFO-
20-03-21 15:38-INFO-Epoch 2, Batch 32, Global step 4200:
20-03-21 15:38-INFO-training batch loss: 0.0898; avg_loss: 0.0896
20-03-21 15:38-INFO-training batch accuracy: 0.9531; avg_accuracy: 0.9314
20-03-21 15:38-INFO-
20-03-21 15:40-INFO-Epoch 2, Batch 132, Global step 4300:
20-03-21 15:40-INFO-training batch loss: 0.1033; avg_loss: 0.0975
20-03-21 15:40-INFO-training batch accuracy: 0.9453; avg_accuracy: 0.9511
20-03-21 15:40-INFO-
20-03-21 15:42-INFO-Epoch 2, Batch 232, Global step 4400:
20-03-21 15:42-INFO-training batch loss: 0.0232; avg_loss: 0.0983
20-03-21 15:42-INFO-training batch accuracy: 1.0000; avg_accuracy: 0.9532
20-03-21 15:42-INFO-
20-03-21 15:44-INFO-Epoch 2, Batch 332, Global step 4500:
20-03-21 15:44-INFO-training batch loss: 0.0481; avg_loss: 0.0987
20-03-21 15:44-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9548
20-03-21 15:44-INFO-
20-03-21 15:46-INFO-Epoch 2, Batch 432, Global step 4600:
20-03-21 15:46-INFO-training batch loss: 0.1443; avg_loss: 0.0995
20-03-21 15:46-INFO-training batch accuracy: 0.9531; avg_accuracy: 0.9551
20-03-21 15:46-INFO-
20-03-21 15:48-INFO-Epoch 2, Batch 532, Global step 4700:
20-03-21 15:48-INFO-training batch loss: 0.1679; avg_loss: 0.0992
20-03-21 15:48-INFO-training batch accuracy: 0.9141; avg_accuracy: 0.9562
20-03-21 15:48-INFO-
20-03-21 15:50-INFO-Epoch 2, Batch 632, Global step 4800:
20-03-21 15:50-INFO-training batch loss: 0.1162; avg_loss: 0.0992
20-03-21 15:50-INFO-training batch accuracy: 0.9453; avg_accuracy: 0.9565
20-03-21 15:50-INFO-
20-03-21 15:52-INFO-Epoch 2, Batch 732, Global step 4900:
20-03-21 15:52-INFO-training batch loss: 0.0870; avg_loss: 0.0989
20-03-21 15:52-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9570
20-03-21 15:52-INFO-
20-03-21 15:54-INFO-Epoch 2, Batch 832, Global step 5000:
20-03-21 15:54-INFO-training batch loss: 0.0588; avg_loss: 0.0985
20-03-21 15:54-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9573
20-03-21 15:54-INFO-
20-03-21 15:56-INFO-Epoch 2, Batch 932, Global step 5100:
20-03-21 15:56-INFO-training batch loss: 0.1215; avg_loss: 0.0987
20-03-21 15:56-INFO-training batch accuracy: 0.9609; avg_accuracy: 0.9573
20-03-21 15:56-INFO-
20-03-21 15:58-INFO-Epoch 2, Batch 1032, Global step 5200:
20-03-21 15:58-INFO-training batch loss: 0.0848; avg_loss: 0.0985
20-03-21 15:58-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9577
20-03-21 15:58-INFO-
20-03-21 16:00-INFO-Epoch 2, Batch 1132, Global step 5300:
20-03-21 16:00-INFO-training batch loss: 0.0890; avg_loss: 0.0984
20-03-21 16:00-INFO-training batch accuracy: 0.9609; avg_accuracy: 0.9580
20-03-21 16:00-INFO-
20-03-21 16:02-INFO-Epoch 2, Batch 1232, Global step 5400:
20-03-21 16:02-INFO-training batch loss: 0.0899; avg_loss: 0.0981
20-03-21 16:02-INFO-training batch accuracy: 0.9531; avg_accuracy: 0.9581
20-03-21 16:02-INFO-
20-03-21 16:04-INFO-Epoch 2, Batch 1332, Global step 5500:
20-03-21 16:04-INFO-training batch loss: 0.0769; avg_loss: 0.0977
20-03-21 16:04-INFO-training batch accuracy: 0.9609; avg_accuracy: 0.9584
20-03-21 16:04-INFO-
20-03-21 16:06-INFO-Epoch 2, Batch 1432, Global step 5600:
20-03-21 16:06-INFO-training batch loss: 0.1118; avg_loss: 0.0973
20-03-21 16:06-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9586
20-03-21 16:06-INFO-
20-03-21 16:08-INFO-Epoch 2, Batch 1532, Global step 5700:
20-03-21 16:08-INFO-training batch loss: 0.1179; avg_loss: 0.0971
20-03-21 16:08-INFO-training batch accuracy: 0.9531; avg_accuracy: 0.9587
20-03-21 16:08-INFO-
20-03-21 16:10-INFO-Epoch 2, Batch 1632, Global step 5800:
20-03-21 16:10-INFO-training batch loss: 0.0947; avg_loss: 0.0965
20-03-21 16:10-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9591
20-03-21 16:10-INFO-
20-03-21 16:12-INFO-Epoch 2, Batch 1732, Global step 5900:
20-03-21 16:12-INFO-training batch loss: 0.0447; avg_loss: 0.0960
20-03-21 16:12-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9593
20-03-21 16:12-INFO-
20-03-21 16:14-INFO-Epoch 2, Batch 1832, Global step 6000:
20-03-21 16:14-INFO-training batch loss: 0.0835; avg_loss: 0.0957
20-03-21 16:14-INFO-training batch accuracy: 0.9609; avg_accuracy: 0.9595
20-03-21 16:14-INFO-
20-03-21 16:17-INFO-Epoch 2, Batch 1932, Global step 6100:
20-03-21 16:17-INFO-training batch loss: 0.0822; avg_loss: 0.0952
20-03-21 16:17-INFO-training batch accuracy: 0.9609; avg_accuracy: 0.9597
20-03-21 16:17-INFO-
20-03-21 16:19-INFO-Epoch 2, Batch 2032, Global step 6200:
20-03-21 16:19-INFO-training batch loss: 0.0861; avg_loss: 0.0947
20-03-21 16:19-INFO-training batch accuracy: 0.9453; avg_accuracy: 0.9600
20-03-21 16:19-INFO-
20-03-21 16:20-INFO-training batch accuracy: 0.9048; avg_accuracy: 0.9604
20-03-21 16:20-INFO-
20-03-21 16:22-INFO-Epoch 2, evaluating batch loss: 0.1148; avg_loss: 0.1479
20-03-21 16:22-INFO-evaluating batch accuracy: 0.9519; avg_accuracy: 0.9431

20-03-21 16:22-INFO-
20-03-21 16:23-INFO-Epoch 3, Batch 48, Global step 6300:
20-03-21 16:23-INFO-training batch loss: 0.1134; avg_loss: 0.0794
20-03-21 16:23-INFO-training batch accuracy: 0.9531; avg_accuracy: 0.9465
20-03-21 16:23-INFO-
20-03-21 16:24-INFO-Epoch 3, Batch 148, Global step 6400:
20-03-21 16:24-INFO-training batch loss: 0.0538; avg_loss: 0.0832
20-03-21 16:24-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9584
20-03-21 16:24-INFO-
20-03-21 16:26-INFO-Epoch 3, Batch 248, Global step 6500:
20-03-21 16:26-INFO-training batch loss: 0.0981; avg_loss: 0.0823
20-03-21 16:26-INFO-training batch accuracy: 0.9453; avg_accuracy: 0.9613
20-03-21 16:26-INFO-
20-03-21 16:28-INFO-Epoch 3, Batch 348, Global step 6600:
20-03-21 16:28-INFO-training batch loss: 0.0983; avg_loss: 0.0823
20-03-21 16:28-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9629
20-03-21 16:28-INFO-
20-03-21 16:30-INFO-Epoch 3, Batch 448, Global step 6700:
20-03-21 16:30-INFO-training batch loss: 0.0521; avg_loss: 0.0836
20-03-21 16:30-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9634
20-03-21 16:30-INFO-
20-03-21 16:32-INFO-Epoch 3, Batch 548, Global step 6800:
20-03-21 16:32-INFO-training batch loss: 0.1070; avg_loss: 0.0828
20-03-21 16:32-INFO-training batch accuracy: 0.9531; avg_accuracy: 0.9639
20-03-21 16:32-INFO-
20-03-21 16:34-INFO-Epoch 3, Batch 648, Global step 6900:
20-03-21 16:34-INFO-training batch loss: 0.1733; avg_loss: 0.0833
20-03-21 16:34-INFO-training batch accuracy: 0.9531; avg_accuracy: 0.9643
20-03-21 16:34-INFO-
20-03-21 16:37-INFO-Epoch 3, Batch 748, Global step 7000:
20-03-21 16:37-INFO-training batch loss: 0.0602; avg_loss: 0.0831
20-03-21 16:37-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9649
20-03-21 16:37-INFO-
20-03-21 16:39-INFO-Epoch 3, Batch 848, Global step 7100:
20-03-21 16:39-INFO-training batch loss: 0.0467; avg_loss: 0.0829
20-03-21 16:39-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9651
20-03-21 16:39-INFO-
20-03-21 16:41-INFO-Epoch 3, Batch 948, Global step 7200:
20-03-21 16:41-INFO-training batch loss: 0.1041; avg_loss: 0.0828
20-03-21 16:41-INFO-training batch accuracy: 0.9453; avg_accuracy: 0.9655
20-03-21 16:41-INFO-
20-03-21 16:43-INFO-Epoch 3, Batch 1048, Global step 7300:
20-03-21 16:43-INFO-training batch loss: 0.0384; avg_loss: 0.0824
20-03-21 16:43-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9658
20-03-21 16:43-INFO-
20-03-21 16:45-INFO-Epoch 3, Batch 1148, Global step 7400:
20-03-21 16:45-INFO-training batch loss: 0.0919; avg_loss: 0.0814
20-03-21 16:45-INFO-training batch accuracy: 0.9531; avg_accuracy: 0.9664
20-03-21 16:45-INFO-
20-03-21 16:47-INFO-Epoch 3, Batch 1248, Global step 7500:
20-03-21 16:47-INFO-training batch loss: 0.0625; avg_loss: 0.0804
20-03-21 16:47-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9670
20-03-21 16:47-INFO-
20-03-21 16:49-INFO-Epoch 3, Batch 1348, Global step 7600:
20-03-21 16:49-INFO-training batch loss: 0.0667; avg_loss: 0.0797
20-03-21 16:49-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9674
20-03-21 16:49-INFO-
20-03-21 16:51-INFO-Epoch 3, Batch 1448, Global step 7700:
20-03-21 16:51-INFO-training batch loss: 0.0616; avg_loss: 0.0796
20-03-21 16:51-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9676
20-03-21 16:51-INFO-
20-03-21 16:53-INFO-Epoch 3, Batch 1548, Global step 7800:
20-03-21 16:53-INFO-training batch loss: 0.0835; avg_loss: 0.0788
20-03-21 16:53-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9680
20-03-21 16:53-INFO-
20-03-21 16:55-INFO-Epoch 3, Batch 1648, Global step 7900:
20-03-21 16:55-INFO-training batch loss: 0.0702; avg_loss: 0.0781
20-03-21 16:55-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9683
20-03-21 16:55-INFO-
20-03-21 16:57-INFO-Epoch 3, Batch 1748, Global step 8000:
20-03-21 16:57-INFO-training batch loss: 0.0454; avg_loss: 0.0773
20-03-21 16:57-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9686
20-03-21 16:57-INFO-
20-03-21 16:59-INFO-Epoch 3, Batch 1848, Global step 8100:
20-03-21 16:59-INFO-training batch loss: 0.1021; avg_loss: 0.0767
20-03-21 16:59-INFO-training batch accuracy: 0.9531; avg_accuracy: 0.9690
20-03-21 16:59-INFO-
20-03-21 17:01-INFO-Epoch 3, Batch 1948, Global step 8200:
20-03-21 17:01-INFO-training batch loss: 0.0468; avg_loss: 0.0757
20-03-21 17:01-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9695
20-03-21 17:01-INFO-
20-03-21 17:03-INFO-Epoch 3, Batch 2048, Global step 8300:
20-03-21 17:03-INFO-training batch loss: 0.0738; avg_loss: 0.0751
20-03-21 17:03-INFO-training batch accuracy: 0.9609; avg_accuracy: 0.9698
20-03-21 17:03-INFO-
20-03-21 17:04-INFO-training batch accuracy: 0.9524; avg_accuracy: 0.9704
20-03-21 17:04-INFO-
20-03-21 17:06-INFO-Epoch 3, evaluating batch loss: 0.0896; avg_loss: 0.1047
20-03-21 17:06-INFO-evaluating batch accuracy: 0.9615; avg_accuracy: 0.9630

20-03-21 17:06-INFO-
20-03-21 17:07-INFO-Epoch 4, Batch 64, Global step 8400:
20-03-21 17:07-INFO-training batch loss: 0.0542; avg_loss: 0.0507
20-03-21 17:07-INFO-training batch accuracy: 0.9609; avg_accuracy: 0.9645
20-03-21 17:07-INFO-
20-03-21 17:09-INFO-Epoch 4, Batch 164, Global step 8500:
20-03-21 17:09-INFO-training batch loss: 0.0490; avg_loss: 0.0549
20-03-21 17:09-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9727
20-03-21 17:09-INFO-
20-03-21 17:11-INFO-Epoch 4, Batch 264, Global step 8600:
20-03-21 17:11-INFO-training batch loss: 0.0674; avg_loss: 0.0555
20-03-21 17:11-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9751
20-03-21 17:11-INFO-
20-03-21 17:13-INFO-Epoch 4, Batch 364, Global step 8700:
20-03-21 17:13-INFO-training batch loss: 0.0974; avg_loss: 0.0553
20-03-21 17:13-INFO-training batch accuracy: 0.9375; avg_accuracy: 0.9762
20-03-21 17:13-INFO-
20-03-21 17:15-INFO-Epoch 4, Batch 464, Global step 8800:
20-03-21 17:15-INFO-training batch loss: 0.0151; avg_loss: 0.0558
20-03-21 17:15-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9764
20-03-21 17:15-INFO-
20-03-21 17:17-INFO-Epoch 4, Batch 564, Global step 8900:
20-03-21 17:17-INFO-training batch loss: 0.0456; avg_loss: 0.0559
20-03-21 17:17-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9766
20-03-21 17:17-INFO-
20-03-21 17:19-INFO-Epoch 4, Batch 664, Global step 9000:
20-03-21 17:19-INFO-training batch loss: 0.0550; avg_loss: 0.0561
20-03-21 17:19-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9770
20-03-21 17:19-INFO-
20-03-21 17:21-INFO-Epoch 4, Batch 764, Global step 9100:
20-03-21 17:21-INFO-training batch loss: 0.0365; avg_loss: 0.0562
20-03-21 17:21-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9772
20-03-21 17:21-INFO-
20-03-21 17:23-INFO-Epoch 4, Batch 864, Global step 9200:
20-03-21 17:23-INFO-training batch loss: 0.0937; avg_loss: 0.0560
20-03-21 17:23-INFO-training batch accuracy: 0.9609; avg_accuracy: 0.9775
20-03-21 17:23-INFO-
20-03-21 17:25-INFO-Epoch 4, Batch 964, Global step 9300:
20-03-21 17:25-INFO-training batch loss: 0.0503; avg_loss: 0.0557
20-03-21 17:25-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9777
20-03-21 17:25-INFO-
20-03-21 17:27-INFO-Epoch 4, Batch 1064, Global step 9400:
20-03-21 17:27-INFO-training batch loss: 0.0300; avg_loss: 0.0558
20-03-21 17:27-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9777
20-03-21 17:27-INFO-
20-03-21 17:29-INFO-Epoch 4, Batch 1164, Global step 9500:
20-03-21 17:29-INFO-training batch loss: 0.0377; avg_loss: 0.0553
20-03-21 17:29-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9779
20-03-21 17:29-INFO-
20-03-21 17:31-INFO-Epoch 4, Batch 1264, Global step 9600:
20-03-21 17:31-INFO-training batch loss: 0.0317; avg_loss: 0.0549
20-03-21 17:31-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9780
20-03-21 17:31-INFO-
20-03-21 17:33-INFO-Epoch 4, Batch 1364, Global step 9700:
20-03-21 17:33-INFO-training batch loss: 0.0650; avg_loss: 0.0550
20-03-21 17:33-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9781
20-03-21 17:33-INFO-
20-03-21 17:35-INFO-Epoch 4, Batch 1464, Global step 9800:
20-03-21 17:35-INFO-training batch loss: 0.0432; avg_loss: 0.0550
20-03-21 17:35-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9781
20-03-21 17:35-INFO-
20-03-21 17:37-INFO-Epoch 4, Batch 1564, Global step 9900:
20-03-21 17:37-INFO-training batch loss: 0.0338; avg_loss: 0.0544
20-03-21 17:37-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9783
20-03-21 17:37-INFO-
20-03-21 17:39-INFO-Epoch 4, Batch 1664, Global step 10000:
20-03-21 17:39-INFO-training batch loss: 0.0603; avg_loss: 0.0542
20-03-21 17:39-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9785
20-03-21 17:39-INFO-
20-03-21 17:41-INFO-Epoch 4, Batch 1764, Global step 10100:
20-03-21 17:41-INFO-training batch loss: 0.0570; avg_loss: 0.0540
20-03-21 17:41-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9786
20-03-21 17:41-INFO-
20-03-21 17:43-INFO-Epoch 4, Batch 1864, Global step 10200:
20-03-21 17:43-INFO-training batch loss: 0.0420; avg_loss: 0.0535
20-03-21 17:43-INFO-training batch accuracy: 1.0000; avg_accuracy: 0.9788
20-03-21 17:43-INFO-
20-03-21 17:45-INFO-Epoch 4, Batch 1964, Global step 10300:
20-03-21 17:45-INFO-training batch loss: 0.0426; avg_loss: 0.0531
20-03-21 17:45-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9791
20-03-21 17:45-INFO-
20-03-21 17:47-INFO-Epoch 4, Batch 2064, Global step 10400:
20-03-21 17:47-INFO-training batch loss: 0.0383; avg_loss: 0.0530
20-03-21 17:47-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9791
20-03-21 17:47-INFO-
20-03-21 17:48-INFO-training batch accuracy: 0.9762; avg_accuracy: 0.9796
20-03-21 17:48-INFO-
20-03-21 17:50-INFO-Epoch 4, evaluating batch loss: 0.0701; avg_loss: 0.0975
20-03-21 17:50-INFO-evaluating batch accuracy: 0.9808; avg_accuracy: 0.9615

20-03-21 17:50-INFO-
20-03-21 17:51-INFO-Epoch 5, Batch 80, Global step 10500:
20-03-21 17:51-INFO-training batch loss: 0.0126; avg_loss: 0.0395
20-03-21 17:51-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9729
20-03-21 17:51-INFO-
20-03-21 17:53-INFO-Epoch 5, Batch 180, Global step 10600:
20-03-21 17:53-INFO-training batch loss: 0.0467; avg_loss: 0.0444
20-03-21 17:53-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9770
20-03-21 17:53-INFO-
20-03-21 17:55-INFO-Epoch 5, Batch 280, Global step 10700:
20-03-21 17:55-INFO-training batch loss: 0.0288; avg_loss: 0.0450
20-03-21 17:55-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9784
20-03-21 17:55-INFO-
20-03-21 17:57-INFO-Epoch 5, Batch 380, Global step 10800:
20-03-21 17:57-INFO-training batch loss: 0.0216; avg_loss: 0.0453
20-03-21 17:57-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9795
20-03-21 17:57-INFO-
20-03-21 17:59-INFO-Epoch 5, Batch 480, Global step 10900:
20-03-21 17:59-INFO-training batch loss: 0.0793; avg_loss: 0.0460
20-03-21 17:59-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9800
20-03-21 17:59-INFO-
20-03-21 18:01-INFO-Epoch 5, Batch 580, Global step 11000:
20-03-21 18:01-INFO-training batch loss: 0.0451; avg_loss: 0.0457
20-03-21 18:01-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9802
20-03-21 18:01-INFO-
20-03-21 18:03-INFO-Epoch 5, Batch 680, Global step 11100:
20-03-21 18:03-INFO-training batch loss: 0.0243; avg_loss: 0.0459
20-03-21 18:03-INFO-training batch accuracy: 1.0000; avg_accuracy: 0.9804
20-03-21 18:03-INFO-
20-03-21 18:05-INFO-Epoch 5, Batch 780, Global step 11200:
20-03-21 18:05-INFO-training batch loss: 0.0530; avg_loss: 0.0463
20-03-21 18:05-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9806
20-03-21 18:05-INFO-
20-03-21 18:07-INFO-Epoch 5, Batch 880, Global step 11300:
20-03-21 18:07-INFO-training batch loss: 0.0575; avg_loss: 0.0467
20-03-21 18:07-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9804
20-03-21 18:07-INFO-
20-03-21 18:09-INFO-Epoch 5, Batch 980, Global step 11400:
20-03-21 18:09-INFO-training batch loss: 0.0131; avg_loss: 0.0467
20-03-21 18:09-INFO-training batch accuracy: 1.0000; avg_accuracy: 0.9807
20-03-21 18:09-INFO-
20-03-21 18:11-INFO-Epoch 5, Batch 1080, Global step 11500:
20-03-21 18:11-INFO-training batch loss: 0.0533; avg_loss: 0.0471
20-03-21 18:11-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9807
20-03-21 18:11-INFO-
20-03-21 18:13-INFO-Epoch 5, Batch 1180, Global step 11600:
20-03-21 18:13-INFO-training batch loss: 0.0237; avg_loss: 0.0472
20-03-21 18:13-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9807
20-03-21 18:13-INFO-
20-03-21 18:15-INFO-Epoch 5, Batch 1280, Global step 11700:
20-03-21 18:15-INFO-training batch loss: 0.0282; avg_loss: 0.0473
20-03-21 18:15-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9808
20-03-21 18:15-INFO-
20-03-21 18:17-INFO-Epoch 5, Batch 1380, Global step 11800:
20-03-21 18:17-INFO-training batch loss: 0.0655; avg_loss: 0.0473
20-03-21 18:17-INFO-training batch accuracy: 0.9609; avg_accuracy: 0.9808
20-03-21 18:17-INFO-
20-03-21 18:19-INFO-Epoch 5, Batch 1480, Global step 11900:
20-03-21 18:19-INFO-training batch loss: 0.0409; avg_loss: 0.0471
20-03-21 18:19-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9810
20-03-21 18:19-INFO-
20-03-21 18:21-INFO-Epoch 5, Batch 1580, Global step 12000:
20-03-21 18:21-INFO-training batch loss: 0.0277; avg_loss: 0.0469
20-03-21 18:21-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9811
20-03-21 18:21-INFO-
20-03-21 18:23-INFO-Epoch 5, Batch 1680, Global step 12100:
20-03-21 18:23-INFO-training batch loss: 0.0276; avg_loss: 0.0466
20-03-21 18:23-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9812
20-03-21 18:23-INFO-
20-03-21 18:25-INFO-Epoch 5, Batch 1780, Global step 12200:
20-03-21 18:25-INFO-training batch loss: 0.0450; avg_loss: 0.0465
20-03-21 18:25-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9813
20-03-21 18:25-INFO-
20-03-21 18:27-INFO-Epoch 5, Batch 1880, Global step 12300:
20-03-21 18:27-INFO-training batch loss: 0.0284; avg_loss: 0.0460
20-03-21 18:27-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9816
20-03-21 18:27-INFO-
20-03-21 18:30-INFO-Epoch 5, Batch 1980, Global step 12400:
20-03-21 18:30-INFO-training batch loss: 0.0177; avg_loss: 0.0460
20-03-21 18:30-INFO-training batch accuracy: 1.0000; avg_accuracy: 0.9817
20-03-21 18:30-INFO-
20-03-21 18:32-INFO-Epoch 5, Batch 2080, Global step 12500:
20-03-21 18:32-INFO-training batch loss: 0.0531; avg_loss: 0.0461
20-03-21 18:32-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9818
20-03-21 18:32-INFO-
20-03-21 18:32-INFO-training batch accuracy: 0.9762; avg_accuracy: 0.9822
20-03-21 18:32-INFO-
20-03-21 18:34-INFO-Epoch 5, evaluating batch loss: 0.0714; avg_loss: 0.0893
20-03-21 18:34-INFO-evaluating batch accuracy: 0.9808; avg_accuracy: 0.9656

20-03-21 18:34-INFO-
20-03-21 18:34-WARNING-From /root/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to delete files with this prefix.
20-03-21 18:35-INFO-Epoch 6, Batch 96, Global step 12600:
20-03-21 18:35-INFO-training batch loss: 0.0130; avg_loss: 0.0434
20-03-21 18:35-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9726
20-03-21 18:35-INFO-
20-03-21 18:37-INFO-Epoch 6, Batch 196, Global step 12700:
20-03-21 18:37-INFO-training batch loss: 0.0488; avg_loss: 0.0448
20-03-21 18:37-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9774
20-03-21 18:37-INFO-
20-03-21 18:39-INFO-Epoch 6, Batch 296, Global step 12800:
20-03-21 18:39-INFO-training batch loss: 0.0177; avg_loss: 0.0438
20-03-21 18:39-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9798
20-03-21 18:39-INFO-
20-03-21 18:41-INFO-Epoch 6, Batch 396, Global step 12900:
20-03-21 18:41-INFO-training batch loss: 0.0284; avg_loss: 0.0436
20-03-21 18:41-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9807
20-03-21 18:41-INFO-
20-03-21 18:43-INFO-Epoch 6, Batch 496, Global step 13000:
20-03-21 18:43-INFO-training batch loss: 0.0200; avg_loss: 0.0431
20-03-21 18:43-INFO-training batch accuracy: 1.0000; avg_accuracy: 0.9813
20-03-21 18:43-INFO-
20-03-21 18:46-INFO-Epoch 6, Batch 596, Global step 13100:
20-03-21 18:46-INFO-training batch loss: 0.0209; avg_loss: 0.0427
20-03-21 18:46-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9818
20-03-21 18:46-INFO-
20-03-21 18:48-INFO-Epoch 6, Batch 696, Global step 13200:
20-03-21 18:48-INFO-training batch loss: 0.0175; avg_loss: 0.0429
20-03-21 18:48-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9819
20-03-21 18:48-INFO-
20-03-21 18:50-INFO-Epoch 6, Batch 796, Global step 13300:
20-03-21 18:50-INFO-training batch loss: 0.0562; avg_loss: 0.0432
20-03-21 18:50-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9818
20-03-21 18:50-INFO-
20-03-21 18:52-INFO-Epoch 6, Batch 896, Global step 13400:
20-03-21 18:52-INFO-training batch loss: 0.0510; avg_loss: 0.0436
20-03-21 18:52-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9818
20-03-21 18:52-INFO-
20-03-21 18:54-INFO-Epoch 6, Batch 996, Global step 13500:
20-03-21 18:54-INFO-training batch loss: 0.0451; avg_loss: 0.0438
20-03-21 18:54-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9819
20-03-21 18:54-INFO-
20-03-21 18:56-INFO-Epoch 6, Batch 1096, Global step 13600:
20-03-21 18:56-INFO-training batch loss: 0.0376; avg_loss: 0.0440
20-03-21 18:56-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9820
20-03-21 18:56-INFO-
20-03-21 18:58-INFO-Epoch 6, Batch 1196, Global step 13700:
20-03-21 18:58-INFO-training batch loss: 0.0750; avg_loss: 0.0439
20-03-21 18:58-INFO-training batch accuracy: 0.9609; avg_accuracy: 0.9820
20-03-21 18:58-INFO-
20-03-21 19:00-INFO-Epoch 6, Batch 1296, Global step 13800:
20-03-21 19:00-INFO-training batch loss: 0.0201; avg_loss: 0.0439
20-03-21 19:00-INFO-training batch accuracy: 1.0000; avg_accuracy: 0.9821
20-03-21 19:00-INFO-
20-03-21 19:02-INFO-Epoch 6, Batch 1396, Global step 13900:
20-03-21 19:02-INFO-training batch loss: 0.0138; avg_loss: 0.0440
20-03-21 19:02-INFO-training batch accuracy: 1.0000; avg_accuracy: 0.9821
20-03-21 19:02-INFO-
20-03-21 19:04-INFO-Epoch 6, Batch 1496, Global step 14000:
20-03-21 19:04-INFO-training batch loss: 0.0375; avg_loss: 0.0439
20-03-21 19:04-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9822
20-03-21 19:04-INFO-
20-03-21 19:06-INFO-Epoch 6, Batch 1596, Global step 14100:
20-03-21 19:06-INFO-training batch loss: 0.0436; avg_loss: 0.0437
20-03-21 19:06-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9823
20-03-21 19:06-INFO-
20-03-21 19:08-INFO-Epoch 6, Batch 1696, Global step 14200:
20-03-21 19:08-INFO-training batch loss: 0.0855; avg_loss: 0.0436
20-03-21 19:08-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9824
20-03-21 19:08-INFO-
20-03-21 19:10-INFO-Epoch 6, Batch 1796, Global step 14300:
20-03-21 19:10-INFO-training batch loss: 0.0549; avg_loss: 0.0436
20-03-21 19:10-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9824
20-03-21 19:10-INFO-
20-03-21 19:12-INFO-Epoch 6, Batch 1896, Global step 14400:
20-03-21 19:12-INFO-training batch loss: 0.0235; avg_loss: 0.0432
20-03-21 19:12-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9826
20-03-21 19:12-INFO-
20-03-21 19:14-INFO-Epoch 6, Batch 1996, Global step 14500:
20-03-21 19:14-INFO-training batch loss: 0.0195; avg_loss: 0.0431
20-03-21 19:14-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9827
20-03-21 19:14-INFO-
20-03-21 19:16-INFO-training batch accuracy: 0.9524; avg_accuracy: 0.9831
20-03-21 19:16-INFO-
20-03-21 19:18-INFO-Epoch 6, evaluating batch loss: 0.0705; avg_loss: 0.0874
20-03-21 19:18-INFO-evaluating batch accuracy: 0.9712; avg_accuracy: 0.9651

20-03-21 19:18-INFO-
20-03-21 19:18-INFO-Epoch 7, Batch 12, Global step 14600:
20-03-21 19:18-INFO-training batch loss: 0.0978; avg_loss: 0.0567
20-03-21 19:18-INFO-training batch accuracy: 0.9609; avg_accuracy: 0.8997
20-03-21 19:18-INFO-
20-03-21 19:20-INFO-Epoch 7, Batch 112, Global step 14700:
20-03-21 19:20-INFO-training batch loss: 0.0288; avg_loss: 0.0391
20-03-21 19:20-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9766
20-03-21 19:20-INFO-
20-03-21 19:22-INFO-Epoch 7, Batch 212, Global step 14800:
20-03-21 19:22-INFO-training batch loss: 0.0732; avg_loss: 0.0410
20-03-21 19:22-INFO-training batch accuracy: 0.9531; avg_accuracy: 0.9800
20-03-21 19:22-INFO-
20-03-21 19:24-INFO-Epoch 7, Batch 312, Global step 14900:
20-03-21 19:24-INFO-training batch loss: 0.1115; avg_loss: 0.0411
20-03-21 19:24-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9816
20-03-21 19:24-INFO-
20-03-21 19:26-INFO-Epoch 7, Batch 412, Global step 15000:
20-03-21 19:26-INFO-training batch loss: 0.0303; avg_loss: 0.0407
20-03-21 19:26-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9823
20-03-21 19:26-INFO-
20-03-21 19:28-INFO-Epoch 7, Batch 512, Global step 15100:
20-03-21 19:28-INFO-training batch loss: 0.0347; avg_loss: 0.0409
20-03-21 19:28-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9827
20-03-21 19:28-INFO-
20-03-21 19:30-INFO-Epoch 7, Batch 612, Global step 15200:
20-03-21 19:30-INFO-training batch loss: 0.0393; avg_loss: 0.0407
20-03-21 19:30-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9828
20-03-21 19:30-INFO-
20-03-21 19:32-INFO-Epoch 7, Batch 712, Global step 15300:
20-03-21 19:32-INFO-training batch loss: 0.0309; avg_loss: 0.0404
20-03-21 19:32-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9831
20-03-21 19:32-INFO-
20-03-21 19:34-INFO-Epoch 7, Batch 812, Global step 15400:
20-03-21 19:34-INFO-training batch loss: 0.0191; avg_loss: 0.0408
20-03-21 19:34-INFO-training batch accuracy: 1.0000; avg_accuracy: 0.9830
20-03-21 19:34-INFO-
20-03-21 19:36-INFO-Epoch 7, Batch 912, Global step 15500:
20-03-21 19:36-INFO-training batch loss: 0.0078; avg_loss: 0.0410
20-03-21 19:36-INFO-training batch accuracy: 1.0000; avg_accuracy: 0.9830
20-03-21 19:36-INFO-
20-03-21 19:38-INFO-Epoch 7, Batch 1012, Global step 15600:
20-03-21 19:38-INFO-training batch loss: 0.0228; avg_loss: 0.0407
20-03-21 19:38-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9832
20-03-21 19:38-INFO-
20-03-21 19:40-INFO-Epoch 7, Batch 1112, Global step 15700:
20-03-21 19:40-INFO-training batch loss: 0.0496; avg_loss: 0.0407
20-03-21 19:40-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9833
20-03-21 19:40-INFO-
20-03-21 19:42-INFO-Epoch 7, Batch 1212, Global step 15800:
20-03-21 19:42-INFO-training batch loss: 0.0178; avg_loss: 0.0405
20-03-21 19:42-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9835
20-03-21 19:42-INFO-
20-03-21 19:44-INFO-Epoch 7, Batch 1312, Global step 15900:
20-03-21 19:44-INFO-training batch loss: 0.0591; avg_loss: 0.0406
20-03-21 19:44-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9836
20-03-21 19:44-INFO-
20-03-21 19:46-INFO-Epoch 7, Batch 1412, Global step 16000:
20-03-21 19:46-INFO-training batch loss: 0.0161; avg_loss: 0.0405
20-03-21 19:46-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9837
20-03-21 19:46-INFO-
20-03-21 19:48-INFO-Epoch 7, Batch 1512, Global step 16100:
20-03-21 19:48-INFO-training batch loss: 0.0779; avg_loss: 0.0405
20-03-21 19:48-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9838
20-03-21 19:48-INFO-
20-03-21 19:50-INFO-Epoch 7, Batch 1612, Global step 16200:
20-03-21 19:50-INFO-training batch loss: 0.0525; avg_loss: 0.0404
20-03-21 19:50-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9839
20-03-21 19:50-INFO-
20-03-21 19:52-INFO-Epoch 7, Batch 1712, Global step 16300:
20-03-21 19:52-INFO-training batch loss: 0.0136; avg_loss: 0.0402
20-03-21 19:52-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9840
20-03-21 19:52-INFO-
20-03-21 19:54-INFO-Epoch 7, Batch 1812, Global step 16400:
20-03-21 19:54-INFO-training batch loss: 0.0217; avg_loss: 0.0401
20-03-21 19:54-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9841
20-03-21 19:54-INFO-
20-03-21 19:56-INFO-Epoch 7, Batch 1912, Global step 16500:
20-03-21 19:56-INFO-training batch loss: 0.0321; avg_loss: 0.0400
20-03-21 19:56-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9842
20-03-21 19:56-INFO-
20-03-21 19:58-INFO-Epoch 7, Batch 2012, Global step 16600:
20-03-21 19:58-INFO-training batch loss: 0.0454; avg_loss: 0.0398
20-03-21 19:58-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9843
20-03-21 19:58-INFO-
20-03-21 20:00-INFO-training batch accuracy: 0.9524; avg_accuracy: 0.9847
20-03-21 20:00-INFO-
20-03-21 20:02-INFO-Epoch 7, evaluating batch loss: 0.0560; avg_loss: 0.0814
20-03-21 20:02-INFO-evaluating batch accuracy: 0.9712; avg_accuracy: 0.9696

20-03-21 20:02-INFO-
20-03-21 20:02-INFO-Epoch 8, Batch 28, Global step 16700:
20-03-21 20:02-INFO-training batch loss: 0.1171; avg_loss: 0.0382
20-03-21 20:02-INFO-training batch accuracy: 0.9531; avg_accuracy: 0.9506
20-03-21 20:02-INFO-
20-03-21 20:04-INFO-Epoch 8, Batch 128, Global step 16800:
20-03-21 20:04-INFO-training batch loss: 0.0714; avg_loss: 0.0350
20-03-21 20:04-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9787
20-03-21 20:04-INFO-
20-03-21 20:06-INFO-Epoch 8, Batch 228, Global step 16900:
20-03-21 20:06-INFO-training batch loss: 0.0292; avg_loss: 0.0369
20-03-21 20:06-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9813
20-03-21 20:06-INFO-
20-03-21 20:08-INFO-Epoch 8, Batch 328, Global step 17000:
20-03-21 20:08-INFO-training batch loss: 0.0328; avg_loss: 0.0365
20-03-21 20:08-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9831
20-03-21 20:08-INFO-
20-03-21 20:10-INFO-Epoch 8, Batch 428, Global step 17100:
20-03-21 20:10-INFO-training batch loss: 0.0448; avg_loss: 0.0376
20-03-21 20:10-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9833
20-03-21 20:10-INFO-
20-03-21 20:12-INFO-Epoch 8, Batch 528, Global step 17200:
20-03-21 20:12-INFO-training batch loss: 0.0230; avg_loss: 0.0376
20-03-21 20:12-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9836
20-03-21 20:12-INFO-
20-03-21 20:14-INFO-Epoch 8, Batch 628, Global step 17300:
20-03-21 20:14-INFO-training batch loss: 0.0471; avg_loss: 0.0374
20-03-21 20:14-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9841
20-03-21 20:14-INFO-
20-03-21 20:16-INFO-Epoch 8, Batch 728, Global step 17400:
20-03-21 20:16-INFO-training batch loss: 0.0460; avg_loss: 0.0378
20-03-21 20:16-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9841
20-03-21 20:16-INFO-
20-03-21 20:18-INFO-Epoch 8, Batch 828, Global step 17500:
20-03-21 20:18-INFO-training batch loss: 0.0265; avg_loss: 0.0379
20-03-21 20:18-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9841
20-03-21 20:18-INFO-
20-03-21 20:20-INFO-Epoch 8, Batch 928, Global step 17600:
20-03-21 20:20-INFO-training batch loss: 0.0134; avg_loss: 0.0379
20-03-21 20:20-INFO-training batch accuracy: 1.0000; avg_accuracy: 0.9843
20-03-21 20:20-INFO-
20-03-21 20:22-INFO-Epoch 8, Batch 1028, Global step 17700:
20-03-21 20:22-INFO-training batch loss: 0.0226; avg_loss: 0.0381
20-03-21 20:22-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9844
20-03-21 20:22-INFO-
20-03-21 20:24-INFO-Epoch 8, Batch 1128, Global step 17800:
20-03-21 20:24-INFO-training batch loss: 0.0547; avg_loss: 0.0381
20-03-21 20:24-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9845
20-03-21 20:24-INFO-
20-03-21 20:26-INFO-Epoch 8, Batch 1228, Global step 17900:
20-03-21 20:26-INFO-training batch loss: 0.0439; avg_loss: 0.0379
20-03-21 20:26-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9846
20-03-21 20:26-INFO-
20-03-21 20:28-INFO-Epoch 8, Batch 1328, Global step 18000:
20-03-21 20:28-INFO-training batch loss: 0.0105; avg_loss: 0.0377
20-03-21 20:28-INFO-training batch accuracy: 1.0000; avg_accuracy: 0.9847
20-03-21 20:28-INFO-
20-03-21 20:30-INFO-Epoch 8, Batch 1428, Global step 18100:
20-03-21 20:30-INFO-training batch loss: 0.0231; avg_loss: 0.0377
20-03-21 20:30-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9848
20-03-21 20:30-INFO-
20-03-21 20:32-INFO-Epoch 8, Batch 1528, Global step 18200:
20-03-21 20:32-INFO-training batch loss: 0.0468; avg_loss: 0.0379
20-03-21 20:32-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9847
20-03-21 20:32-INFO-
20-03-21 20:34-INFO-Epoch 8, Batch 1628, Global step 18300:
20-03-21 20:34-INFO-training batch loss: 0.0323; avg_loss: 0.0377
20-03-21 20:34-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9849
20-03-21 20:34-INFO-
20-03-21 20:37-INFO-Epoch 8, Batch 1728, Global step 18400:
20-03-21 20:37-INFO-training batch loss: 0.0511; avg_loss: 0.0378
20-03-21 20:37-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9849
20-03-21 20:37-INFO-
20-03-21 20:39-INFO-Epoch 8, Batch 1828, Global step 18500:
20-03-21 20:39-INFO-training batch loss: 0.0469; avg_loss: 0.0376
20-03-21 20:39-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9850
20-03-21 20:39-INFO-
20-03-21 20:41-INFO-Epoch 8, Batch 1928, Global step 18600:
20-03-21 20:41-INFO-training batch loss: 0.1009; avg_loss: 0.0375
20-03-21 20:41-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9852
20-03-21 20:41-INFO-
20-03-21 20:43-INFO-Epoch 8, Batch 2028, Global step 18700:
20-03-21 20:43-INFO-training batch loss: 0.0327; avg_loss: 0.0372
20-03-21 20:43-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9853
20-03-21 20:43-INFO-
20-03-21 20:44-INFO-training batch accuracy: 0.9524; avg_accuracy: 0.9857
20-03-21 20:44-INFO-
20-03-21 20:46-INFO-Epoch 8, evaluating batch loss: 0.0658; avg_loss: 0.0851
20-03-21 20:46-INFO-evaluating batch accuracy: 0.9808; avg_accuracy: 0.9642

20-03-21 20:46-INFO-
20-03-21 20:47-INFO-Epoch 9, Batch 44, Global step 18800:
20-03-21 20:47-INFO-training batch loss: 0.0542; avg_loss: 0.0364
20-03-21 20:47-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9627
20-03-21 20:47-INFO-
20-03-21 20:49-INFO-Epoch 9, Batch 144, Global step 18900:
20-03-21 20:49-INFO-training batch loss: 0.0105; avg_loss: 0.0358
20-03-21 20:49-INFO-training batch accuracy: 1.0000; avg_accuracy: 0.9789
20-03-21 20:49-INFO-
20-03-21 20:51-INFO-Epoch 9, Batch 244, Global step 19000:
20-03-21 20:51-INFO-training batch loss: 0.0412; avg_loss: 0.0357
20-03-21 20:51-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9820
20-03-21 20:51-INFO-
20-03-21 20:53-INFO-Epoch 9, Batch 344, Global step 19100:
20-03-21 20:53-INFO-training batch loss: 0.0490; avg_loss: 0.0361
20-03-21 20:53-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9833
20-03-21 20:53-INFO-
20-03-21 20:55-INFO-Epoch 9, Batch 444, Global step 19200:
20-03-21 20:55-INFO-training batch loss: 0.0346; avg_loss: 0.0362
20-03-21 20:55-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9836
20-03-21 20:55-INFO-
20-03-21 20:57-INFO-Epoch 9, Batch 544, Global step 19300:
20-03-21 20:57-INFO-training batch loss: 0.0226; avg_loss: 0.0357
20-03-21 20:57-INFO-training batch accuracy: 1.0000; avg_accuracy: 0.9842
20-03-21 20:57-INFO-
20-03-21 20:59-INFO-Epoch 9, Batch 644, Global step 19400:
20-03-21 20:59-INFO-training batch loss: 0.0278; avg_loss: 0.0359
20-03-21 20:59-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9844
20-03-21 20:59-INFO-
20-03-21 21:01-INFO-Epoch 9, Batch 744, Global step 19500:
20-03-21 21:01-INFO-training batch loss: 0.0579; avg_loss: 0.0358
20-03-21 21:01-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9847
20-03-21 21:01-INFO-
20-03-21 21:03-INFO-Epoch 9, Batch 844, Global step 19600:
20-03-21 21:03-INFO-training batch loss: 0.0209; avg_loss: 0.0358
20-03-21 21:03-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9848
20-03-21 21:03-INFO-
20-03-21 21:05-INFO-Epoch 9, Batch 944, Global step 19700:
20-03-21 21:05-INFO-training batch loss: 0.0372; avg_loss: 0.0361
20-03-21 21:05-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9848
20-03-21 21:05-INFO-
20-03-21 21:07-INFO-Epoch 9, Batch 1044, Global step 19800:
20-03-21 21:07-INFO-training batch loss: 0.1010; avg_loss: 0.0374
20-03-21 21:07-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9846
20-03-21 21:07-INFO-
20-03-21 21:09-INFO-Epoch 9, Batch 1144, Global step 19900:
20-03-21 21:09-INFO-training batch loss: 0.1282; avg_loss: 0.0379
20-03-21 21:09-INFO-training batch accuracy: 0.9609; avg_accuracy: 0.9845
20-03-21 21:09-INFO-
20-03-21 21:11-INFO-Epoch 9, Batch 1244, Global step 20000:
20-03-21 21:11-INFO-training batch loss: 0.0379; avg_loss: 0.0379
20-03-21 21:11-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9845
20-03-21 21:11-INFO-
20-03-21 21:13-INFO-Epoch 9, Batch 1344, Global step 20100:
20-03-21 21:13-INFO-training batch loss: 0.0403; avg_loss: 0.0380
20-03-21 21:13-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9846
20-03-21 21:13-INFO-
20-03-21 21:15-INFO-Epoch 9, Batch 1444, Global step 20200:
20-03-21 21:15-INFO-training batch loss: 0.0311; avg_loss: 0.0379
20-03-21 21:15-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9847
20-03-21 21:15-INFO-
20-03-21 21:17-INFO-Epoch 9, Batch 1544, Global step 20300:
20-03-21 21:17-INFO-training batch loss: 0.0109; avg_loss: 0.0380
20-03-21 21:17-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9847
20-03-21 21:17-INFO-
20-03-21 21:19-INFO-Epoch 9, Batch 1644, Global step 20400:
20-03-21 21:19-INFO-training batch loss: 0.0239; avg_loss: 0.0381
20-03-21 21:19-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9847
20-03-21 21:19-INFO-
20-03-21 21:21-INFO-Epoch 9, Batch 1744, Global step 20500:
20-03-21 21:21-INFO-training batch loss: 0.0889; avg_loss: 0.0383
20-03-21 21:21-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9847
20-03-21 21:21-INFO-
20-03-21 21:23-INFO-Epoch 9, Batch 1844, Global step 20600:
20-03-21 21:23-INFO-training batch loss: 0.0441; avg_loss: 0.0383
20-03-21 21:23-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9847
20-03-21 21:23-INFO-
20-03-21 21:25-INFO-Epoch 9, Batch 1944, Global step 20700:
20-03-21 21:25-INFO-training batch loss: 0.0357; avg_loss: 0.0379
20-03-21 21:25-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9849
20-03-21 21:25-INFO-
20-03-21 21:27-INFO-Epoch 9, Batch 2044, Global step 20800:
20-03-21 21:27-INFO-training batch loss: 0.0649; avg_loss: 0.0377
20-03-21 21:27-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9851
20-03-21 21:27-INFO-
20-03-21 21:28-INFO-training batch accuracy: 0.9762; avg_accuracy: 0.9855
20-03-21 21:28-INFO-
20-03-21 21:30-INFO-Epoch 9, evaluating batch loss: 0.0488; avg_loss: 0.0799
20-03-21 21:30-INFO-evaluating batch accuracy: 0.9808; avg_accuracy: 0.9685

20-03-21 21:30-INFO-
20-03-21 21:31-INFO-Epoch 10, Batch 60, Global step 20900:
20-03-21 21:31-INFO-training batch loss: 0.0395; avg_loss: 0.0337
20-03-21 21:31-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9708
20-03-21 21:31-INFO-
20-03-21 21:33-INFO-Epoch 10, Batch 160, Global step 21000:
20-03-21 21:33-INFO-training batch loss: 0.0186; avg_loss: 0.0354
20-03-21 21:33-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9803
20-03-21 21:33-INFO-
20-03-21 21:35-INFO-Epoch 10, Batch 260, Global step 21100:
20-03-21 21:35-INFO-training batch loss: 0.0334; avg_loss: 0.0367
20-03-21 21:35-INFO-training batch accuracy: 1.0000; avg_accuracy: 0.9823
20-03-21 21:35-INFO-
20-03-21 21:37-INFO-Epoch 10, Batch 360, Global step 21200:
20-03-21 21:37-INFO-training batch loss: 0.0546; avg_loss: 0.0353
20-03-21 21:37-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9840
20-03-21 21:37-INFO-
20-03-21 21:39-INFO-Epoch 10, Batch 460, Global step 21300:
20-03-21 21:39-INFO-training batch loss: 0.0041; avg_loss: 0.0355
20-03-21 21:39-INFO-training batch accuracy: 1.0000; avg_accuracy: 0.9844
20-03-21 21:39-INFO-
20-03-21 21:41-INFO-Epoch 10, Batch 560, Global step 21400:
20-03-21 21:41-INFO-training batch loss: 0.0205; avg_loss: 0.0356
20-03-21 21:41-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9848
20-03-21 21:41-INFO-
20-03-21 21:43-INFO-Epoch 10, Batch 660, Global step 21500:
20-03-21 21:43-INFO-training batch loss: 0.0051; avg_loss: 0.0360
20-03-21 21:43-INFO-training batch accuracy: 1.0000; avg_accuracy: 0.9847
20-03-21 21:43-INFO-
20-03-21 21:45-INFO-Epoch 10, Batch 760, Global step 21600:
20-03-21 21:45-INFO-training batch loss: 0.0139; avg_loss: 0.0359
20-03-21 21:45-INFO-training batch accuracy: 1.0000; avg_accuracy: 0.9850
20-03-21 21:45-INFO-
20-03-21 21:47-INFO-Epoch 10, Batch 860, Global step 21700:
20-03-21 21:47-INFO-training batch loss: 0.0324; avg_loss: 0.0360
20-03-21 21:47-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9850
20-03-21 21:47-INFO-
20-03-21 21:49-INFO-Epoch 10, Batch 960, Global step 21800:
20-03-21 21:49-INFO-training batch loss: 0.0185; avg_loss: 0.0358
20-03-21 21:49-INFO-training batch accuracy: 1.0000; avg_accuracy: 0.9853
20-03-21 21:49-INFO-
20-03-21 21:51-INFO-Epoch 10, Batch 1060, Global step 21900:
20-03-21 21:51-INFO-training batch loss: 0.0179; avg_loss: 0.0360
20-03-21 21:51-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9854
20-03-21 21:51-INFO-
20-03-21 21:53-INFO-Epoch 10, Batch 1160, Global step 22000:
20-03-21 21:53-INFO-training batch loss: 0.0216; avg_loss: 0.0357
20-03-21 21:53-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9855
20-03-21 21:53-INFO-
20-03-21 21:55-INFO-Epoch 10, Batch 1260, Global step 22100:
20-03-21 21:55-INFO-training batch loss: 0.0311; avg_loss: 0.0356
20-03-21 21:55-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9856
20-03-21 21:55-INFO-
20-03-21 21:57-INFO-Epoch 10, Batch 1360, Global step 22200:
20-03-21 21:57-INFO-training batch loss: 0.0234; avg_loss: 0.0357
20-03-21 21:57-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9857
20-03-21 21:57-INFO-
20-03-21 21:59-INFO-Epoch 10, Batch 1460, Global step 22300:
20-03-21 21:59-INFO-training batch loss: 0.0161; avg_loss: 0.0357
20-03-21 21:59-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9857
20-03-21 21:59-INFO-
20-03-21 22:01-INFO-Epoch 10, Batch 1560, Global step 22400:
20-03-21 22:01-INFO-training batch loss: 0.0218; avg_loss: 0.0356
20-03-21 22:01-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9858
20-03-21 22:01-INFO-
20-03-21 22:03-INFO-Epoch 10, Batch 1660, Global step 22500:
20-03-21 22:03-INFO-training batch loss: 0.0314; avg_loss: 0.0355
20-03-21 22:03-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9859
20-03-21 22:03-INFO-
20-03-21 22:05-INFO-Epoch 10, Batch 1760, Global step 22600:
20-03-21 22:05-INFO-training batch loss: 0.0300; avg_loss: 0.0355
20-03-21 22:05-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9859
20-03-21 22:05-INFO-
20-03-21 22:07-INFO-Epoch 10, Batch 1860, Global step 22700:
20-03-21 22:07-INFO-training batch loss: 0.0112; avg_loss: 0.0353
20-03-21 22:07-INFO-training batch accuracy: 1.0000; avg_accuracy: 0.9860
20-03-21 22:07-INFO-
20-03-21 22:09-INFO-Epoch 10, Batch 1960, Global step 22800:
20-03-21 22:09-INFO-training batch loss: 0.0541; avg_loss: 0.0353
20-03-21 22:09-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9860
20-03-21 22:09-INFO-
20-03-21 22:11-INFO-Epoch 10, Batch 2060, Global step 22900:
20-03-21 22:11-INFO-training batch loss: 0.0133; avg_loss: 0.0354
20-03-21 22:11-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9861
20-03-21 22:11-INFO-
20-03-21 22:12-INFO-training batch accuracy: 0.9524; avg_accuracy: 0.9865
20-03-21 22:12-INFO-
20-03-21 22:14-INFO-Epoch 10, evaluating batch loss: 0.0531; avg_loss: 0.0788
20-03-21 22:14-INFO-evaluating batch accuracy: 0.9808; avg_accuracy: 0.9690

20-03-21 22:14-INFO-
20-03-21 22:15-INFO-Epoch 11, Batch 76, Global step 23000:
20-03-21 22:15-INFO-training batch loss: 0.0299; avg_loss: 0.0339
20-03-21 22:15-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9742
20-03-21 22:15-INFO-
20-03-21 22:17-INFO-Epoch 11, Batch 176, Global step 23100:
20-03-21 22:17-INFO-training batch loss: 0.0437; avg_loss: 0.0358
20-03-21 22:17-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9808
20-03-21 22:17-INFO-
20-03-21 22:19-INFO-Epoch 11, Batch 276, Global step 23200:
20-03-21 22:19-INFO-training batch loss: 0.0320; avg_loss: 0.0355
20-03-21 22:19-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9830
20-03-21 22:19-INFO-
20-03-21 22:21-INFO-Epoch 11, Batch 376, Global step 23300:
20-03-21 22:21-INFO-training batch loss: 0.0069; avg_loss: 0.0343
20-03-21 22:21-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9845
20-03-21 22:21-INFO-
20-03-21 22:23-INFO-Epoch 11, Batch 476, Global step 23400:
20-03-21 22:23-INFO-training batch loss: 0.0246; avg_loss: 0.0339
20-03-21 22:23-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9851
20-03-21 22:23-INFO-
20-03-21 22:25-INFO-Epoch 11, Batch 576, Global step 23500:
20-03-21 22:25-INFO-training batch loss: 0.0243; avg_loss: 0.0340
20-03-21 22:25-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9852
20-03-21 22:25-INFO-
20-03-21 22:27-INFO-Epoch 11, Batch 676, Global step 23600:
20-03-21 22:27-INFO-training batch loss: 0.0339; avg_loss: 0.0340
20-03-21 22:27-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9854
20-03-21 22:27-INFO-
20-03-21 22:29-INFO-Epoch 11, Batch 776, Global step 23700:
20-03-21 22:29-INFO-training batch loss: 0.0195; avg_loss: 0.0342
20-03-21 22:29-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9855
20-03-21 22:29-INFO-
20-03-21 22:31-INFO-Epoch 11, Batch 876, Global step 23800:
20-03-21 22:31-INFO-training batch loss: 0.0480; avg_loss: 0.0344
20-03-21 22:31-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9856
20-03-21 22:31-INFO-
20-03-21 22:33-INFO-Epoch 11, Batch 976, Global step 23900:
20-03-21 22:33-INFO-training batch loss: 0.0616; avg_loss: 0.0343
20-03-21 22:33-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9859
20-03-21 22:33-INFO-
20-03-21 22:35-INFO-Epoch 11, Batch 1076, Global step 24000:
20-03-21 22:35-INFO-training batch loss: 0.0638; avg_loss: 0.0345
20-03-21 22:35-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9859
20-03-21 22:35-INFO-
20-03-21 22:37-INFO-Epoch 11, Batch 1176, Global step 24100:
20-03-21 22:37-INFO-training batch loss: 0.0148; avg_loss: 0.0343
20-03-21 22:37-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9860
20-03-21 22:37-INFO-
20-03-21 22:39-INFO-Epoch 11, Batch 1276, Global step 24200:
20-03-21 22:39-INFO-training batch loss: 0.0309; avg_loss: 0.0342
20-03-21 22:39-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9861
20-03-21 22:39-INFO-
20-03-21 22:41-INFO-Epoch 11, Batch 1376, Global step 24300:
20-03-21 22:41-INFO-training batch loss: 0.0314; avg_loss: 0.0343
20-03-21 22:41-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9862
20-03-21 22:41-INFO-
20-03-21 22:43-INFO-Epoch 11, Batch 1476, Global step 24400:
20-03-21 22:43-INFO-training batch loss: 0.0089; avg_loss: 0.0343
20-03-21 22:43-INFO-training batch accuracy: 1.0000; avg_accuracy: 0.9862
20-03-21 22:43-INFO-
20-03-21 22:45-INFO-Epoch 11, Batch 1576, Global step 24500:
20-03-21 22:45-INFO-training batch loss: 0.0389; avg_loss: 0.0345
20-03-21 22:45-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9862
20-03-21 22:45-INFO-
20-03-21 22:47-INFO-Epoch 11, Batch 1676, Global step 24600:
20-03-21 22:47-INFO-training batch loss: 0.0595; avg_loss: 0.0349
20-03-21 22:47-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9861
20-03-21 22:47-INFO-
20-03-21 22:49-INFO-Epoch 11, Batch 1776, Global step 24700:
20-03-21 22:49-INFO-training batch loss: 0.0052; avg_loss: 0.0349
20-03-21 22:49-INFO-training batch accuracy: 1.0000; avg_accuracy: 0.9861
20-03-21 22:49-INFO-
20-03-21 22:51-INFO-Epoch 11, Batch 1876, Global step 24800:
20-03-21 22:51-INFO-training batch loss: 0.0108; avg_loss: 0.0347
20-03-21 22:51-INFO-training batch accuracy: 1.0000; avg_accuracy: 0.9862
20-03-21 22:51-INFO-
20-03-21 22:53-INFO-Epoch 11, Batch 1976, Global step 24900:
20-03-21 22:53-INFO-training batch loss: 0.0055; avg_loss: 0.0345
20-03-21 22:53-INFO-training batch accuracy: 1.0000; avg_accuracy: 0.9863
20-03-21 22:53-INFO-
20-03-21 22:55-INFO-Epoch 11, Batch 2076, Global step 25000:
20-03-21 22:55-INFO-training batch loss: 0.1028; avg_loss: 0.0346
20-03-21 22:55-INFO-training batch accuracy: 0.9531; avg_accuracy: 0.9863
20-03-21 22:55-INFO-
20-03-21 22:56-INFO-training batch accuracy: 0.9762; avg_accuracy: 0.9868
20-03-21 22:56-INFO-
20-03-21 22:57-INFO-Epoch 11, evaluating batch loss: 0.0522; avg_loss: 0.0743
20-03-21 22:57-INFO-evaluating batch accuracy: 0.9904; avg_accuracy: 0.9713

20-03-21 22:57-INFO-
20-03-21 22:59-INFO-Epoch 12, Batch 92, Global step 25100:
20-03-21 22:59-INFO-training batch loss: 0.0386; avg_loss: 0.0303
20-03-21 22:59-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9779
20-03-21 22:59-INFO-
20-03-21 23:01-INFO-Epoch 12, Batch 192, Global step 25200:
20-03-21 23:01-INFO-training batch loss: 0.0523; avg_loss: 0.0329
20-03-21 23:01-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9825
20-03-21 23:01-INFO-
20-03-21 23:03-INFO-Epoch 12, Batch 292, Global step 25300:
20-03-21 23:03-INFO-training batch loss: 0.0070; avg_loss: 0.0332
20-03-21 23:03-INFO-training batch accuracy: 1.0000; avg_accuracy: 0.9842
20-03-21 23:03-INFO-
20-03-21 23:05-INFO-Epoch 12, Batch 392, Global step 25400:
20-03-21 23:05-INFO-training batch loss: 0.0313; avg_loss: 0.0366
20-03-21 23:05-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9841
20-03-21 23:05-INFO-
20-03-21 23:07-INFO-Epoch 12, Batch 492, Global step 25500:
20-03-21 23:07-INFO-training batch loss: 0.0198; avg_loss: 0.0367
20-03-21 23:07-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9846
20-03-21 23:07-INFO-
20-03-21 23:09-INFO-Epoch 12, Batch 592, Global step 25600:
20-03-21 23:09-INFO-training batch loss: 0.0258; avg_loss: 0.0360
20-03-21 23:09-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9851
20-03-21 23:09-INFO-
20-03-21 23:11-INFO-Epoch 12, Batch 692, Global step 25700:
20-03-21 23:11-INFO-training batch loss: 0.0734; avg_loss: 0.0358
20-03-21 23:11-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9853
20-03-21 23:11-INFO-
20-03-21 23:13-INFO-Epoch 12, Batch 792, Global step 25800:
20-03-21 23:13-INFO-training batch loss: 0.0144; avg_loss: 0.0356
20-03-21 23:13-INFO-training batch accuracy: 1.0000; avg_accuracy: 0.9855
20-03-21 23:13-INFO-
20-03-21 23:15-INFO-Epoch 12, Batch 892, Global step 25900:
20-03-21 23:15-INFO-training batch loss: 0.0564; avg_loss: 0.0354
20-03-21 23:15-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9857
20-03-21 23:15-INFO-
20-03-21 23:17-INFO-Epoch 12, Batch 992, Global step 26000:
20-03-21 23:17-INFO-training batch loss: 0.0220; avg_loss: 0.0354
20-03-21 23:17-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9859
20-03-21 23:17-INFO-
20-03-21 23:19-INFO-Epoch 12, Batch 1092, Global step 26100:
20-03-21 23:19-INFO-training batch loss: 0.0304; avg_loss: 0.0357
20-03-21 23:19-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9860
20-03-21 23:19-INFO-
20-03-21 23:21-INFO-Epoch 12, Batch 1192, Global step 26200:
20-03-21 23:21-INFO-training batch loss: 0.0420; avg_loss: 0.0353
20-03-21 23:21-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9861
20-03-21 23:21-INFO-
20-03-21 23:23-INFO-Epoch 12, Batch 1292, Global step 26300:
20-03-21 23:23-INFO-training batch loss: 0.0264; avg_loss: 0.0352
20-03-21 23:23-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9862
20-03-21 23:23-INFO-
20-03-21 23:25-INFO-Epoch 12, Batch 1392, Global step 26400:
20-03-21 23:25-INFO-training batch loss: 0.0409; avg_loss: 0.0352
20-03-21 23:25-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9862
20-03-21 23:25-INFO-
20-03-21 23:27-INFO-Epoch 12, Batch 1492, Global step 26500:
20-03-21 23:27-INFO-training batch loss: 0.0376; avg_loss: 0.0351
20-03-21 23:27-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9863
20-03-21 23:27-INFO-
20-03-21 23:29-INFO-Epoch 12, Batch 1592, Global step 26600:
20-03-21 23:29-INFO-training batch loss: 0.0089; avg_loss: 0.0349
20-03-21 23:29-INFO-training batch accuracy: 1.0000; avg_accuracy: 0.9864
20-03-21 23:29-INFO-
20-03-21 23:31-INFO-Epoch 12, Batch 1692, Global step 26700:
20-03-21 23:31-INFO-training batch loss: 0.0163; avg_loss: 0.0348
20-03-21 23:31-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9864
20-03-21 23:31-INFO-
20-03-21 23:33-INFO-Epoch 12, Batch 1792, Global step 26800:
20-03-21 23:33-INFO-training batch loss: 0.0019; avg_loss: 0.0347
20-03-21 23:33-INFO-training batch accuracy: 1.0000; avg_accuracy: 0.9864
20-03-21 23:33-INFO-
20-03-21 23:36-INFO-Epoch 12, Batch 1892, Global step 26900:
20-03-21 23:36-INFO-training batch loss: 0.0211; avg_loss: 0.0344
20-03-21 23:36-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9866
20-03-21 23:36-INFO-
20-03-21 23:38-INFO-Epoch 12, Batch 1992, Global step 27000:
20-03-21 23:38-INFO-training batch loss: 0.0712; avg_loss: 0.0347
20-03-21 23:38-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9866
20-03-21 23:38-INFO-
20-03-21 23:39-INFO-training batch accuracy: 0.9286; avg_accuracy: 0.9864
20-03-21 23:39-INFO-
20-03-21 23:41-INFO-Epoch 12, evaluating batch loss: 0.0965; avg_loss: 0.1386
20-03-21 23:41-INFO-evaluating batch accuracy: 0.9712; avg_accuracy: 0.9471

20-03-21 23:41-INFO-
20-03-21 23:42-INFO-Epoch 13, Batch 8, Global step 27100:
20-03-21 23:42-INFO-training batch loss: 0.0626; avg_loss: 0.0648
20-03-21 23:42-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.8574
20-03-21 23:42-INFO-
20-03-21 23:43-INFO-Epoch 13, Batch 108, Global step 27200:
20-03-21 23:43-INFO-training batch loss: 0.0089; avg_loss: 0.0573
20-03-21 23:43-INFO-training batch accuracy: 1.0000; avg_accuracy: 0.9690
20-03-21 23:43-INFO-
20-03-21 23:45-INFO-Epoch 13, Batch 208, Global step 27300:
20-03-21 23:45-INFO-training batch loss: 0.0418; avg_loss: 0.0460
20-03-21 23:45-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9779
20-03-21 23:45-INFO-
20-03-21 23:47-INFO-Epoch 13, Batch 308, Global step 27400:
20-03-21 23:47-INFO-training batch loss: 0.0367; avg_loss: 0.0407
20-03-21 23:47-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9817
20-03-21 23:47-INFO-
20-03-21 23:49-INFO-Epoch 13, Batch 408, Global step 27500:
20-03-21 23:49-INFO-training batch loss: 0.0729; avg_loss: 0.0409
20-03-21 23:49-INFO-training batch accuracy: 0.9609; avg_accuracy: 0.9826
20-03-21 23:49-INFO-
20-03-21 23:51-INFO-Epoch 13, Batch 508, Global step 27600:
20-03-21 23:51-INFO-training batch loss: 0.0478; avg_loss: 0.0394
20-03-21 23:51-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9836
20-03-21 23:51-INFO-
20-03-21 23:53-INFO-Epoch 13, Batch 608, Global step 27700:
20-03-21 23:53-INFO-training batch loss: 0.0112; avg_loss: 0.0377
20-03-21 23:53-INFO-training batch accuracy: 1.0000; avg_accuracy: 0.9844
20-03-21 23:53-INFO-
20-03-21 23:55-INFO-Epoch 13, Batch 708, Global step 27800:
20-03-21 23:55-INFO-training batch loss: 0.0345; avg_loss: 0.0368
20-03-21 23:55-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9850
20-03-21 23:55-INFO-
20-03-21 23:58-INFO-Epoch 13, Batch 808, Global step 27900:
20-03-21 23:58-INFO-training batch loss: 0.0333; avg_loss: 0.0360
20-03-21 23:58-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9854
20-03-21 23:58-INFO-
20-03-22 00:00-INFO-Epoch 13, Batch 908, Global step 28000:
20-03-22 00:00-INFO-training batch loss: 0.0812; avg_loss: 0.0358
20-03-22 00:00-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9856
20-03-22 00:00-INFO-
20-03-22 00:02-INFO-Epoch 13, Batch 1008, Global step 28100:
20-03-22 00:02-INFO-training batch loss: 0.0098; avg_loss: 0.0355
20-03-22 00:02-INFO-training batch accuracy: 1.0000; avg_accuracy: 0.9858
20-03-22 00:02-INFO-
20-03-22 00:04-INFO-Epoch 13, Batch 1108, Global step 28200:
20-03-22 00:04-INFO-training batch loss: 0.0351; avg_loss: 0.0351
20-03-22 00:04-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9861
20-03-22 00:04-INFO-
20-03-22 00:06-INFO-Epoch 13, Batch 1208, Global step 28300:
20-03-22 00:06-INFO-training batch loss: 0.0349; avg_loss: 0.0346
20-03-22 00:06-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9863
20-03-22 00:06-INFO-
20-03-22 00:08-INFO-Epoch 13, Batch 1308, Global step 28400:
20-03-22 00:08-INFO-training batch loss: 0.0220; avg_loss: 0.0343
20-03-22 00:08-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9865
20-03-22 00:08-INFO-
20-03-22 00:10-INFO-Epoch 13, Batch 1408, Global step 28500:
20-03-22 00:10-INFO-training batch loss: 0.0416; avg_loss: 0.0340
20-03-22 00:10-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9866
20-03-22 00:10-INFO-
20-03-22 00:12-INFO-Epoch 13, Batch 1508, Global step 28600:
20-03-22 00:12-INFO-training batch loss: 0.0289; avg_loss: 0.0338
20-03-22 00:12-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9868
20-03-22 00:12-INFO-
