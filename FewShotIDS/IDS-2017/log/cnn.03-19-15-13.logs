20-03-19 15:13-INFO-{'session_length': 24, 'height': 32, 'width': 32, 'learning_rate': 5e-05, 'num_output': 128, 'num_labels': 13, 'is_train': True, 'early_stop': False, 'is_pretrain': False, 'is_time': False, 'is_size': False, 'uncertainty': False}
20-03-19 15:13-WARNING-From ../utils.py:123: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

20-03-19 15:13-WARNING-From ../model/train.py:79: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.

20-03-19 15:13-WARNING-From ../model/base_model.py:46: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

20-03-19 15:13-WARNING-From ../model/cnn_model.py:41: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

20-03-19 15:13-WARNING-From ../model/cnn_model.py:41: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.

20-03-19 15:13-WARNING-Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7fefe9ae5b50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7fefe9ae5b50>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-19 15:13-WARNING-Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7fefe9ae5810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7fefe9ae5810>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-19 15:13-WARNING-Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7fefe9ae5310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7fefe9ae5310>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-19 15:13-WARNING-Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7fefe9ae5210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7fefe9ae5210>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-19 15:13-WARNING-Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7fefe9ae5250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7fefe9ae5250>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-19 15:13-WARNING-Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7fefe8ec2950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7fefe8ec2950>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-19 15:13-WARNING-Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7fefe9ae5110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7fefe9ae5110>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-19 15:13-WARNING-Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7fefe8e87050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7fefe8e87050>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-19 15:13-WARNING-Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7fefe8e87050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7fefe8e87050>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-19 15:13-WARNING-Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7fefe8ec2950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7fefe8ec2950>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-19 15:13-WARNING-Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7fefe9ae5050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7fefe9ae5050>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-19 15:13-WARNING-Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7fefe9a54490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7fefe9a54490>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-19 15:13-WARNING-From /root/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/layers/python/layers/layers.py:1634: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.flatten instead.
20-03-19 15:13-WARNING-Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7fefe8e87050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7fefe8e87050>>: AttributeError: module 'gast' has no attribute 'Num'
20-03-19 15:13-WARNING-From ../model/utils/modules.py:235: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.dense instead.
20-03-19 15:13-WARNING-Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fefe8ee0f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fefe8ee0f10>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-19 15:13-WARNING-From ../model/utils/modules.py:237: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).
20-03-19 15:13-WARNING-Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7fefe8e70b50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7fefe8e70b50>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-19 15:13-WARNING-From ../model/utils/modules.py:240: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
20-03-19 15:13-WARNING-Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fefe8ecaf90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fefe8ecaf90>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-19 15:13-WARNING-Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fefe8e92450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fefe8e92450>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-19 15:13-WARNING-From ../model/base_model.py:132: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.

20-03-19 15:13-WARNING-From /root/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/clip_ops.py:286: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
20-03-19 15:13-WARNING-From ../model/train.py:90: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.

20-03-19 15:54-INFO-Epoch 0, Batch 100, Global step 100:
20-03-19 15:54-INFO-training batch loss: 0.4448; avg_loss: 0.9076
20-03-19 15:54-INFO-training batch accuracy: 0.8516; avg_accuracy: 0.7277
20-03-19 15:54-INFO-
20-03-19 15:55-INFO-Epoch 0, Batch 200, Global step 200:
20-03-19 15:55-INFO-training batch loss: 0.2934; avg_loss: 0.6194
20-03-19 15:55-INFO-training batch accuracy: 0.9141; avg_accuracy: 0.8139
20-03-19 15:55-INFO-
20-03-19 15:56-INFO-Epoch 0, Batch 300, Global step 300:
20-03-19 15:56-INFO-training batch loss: 0.2781; avg_loss: 0.5030
20-03-19 15:56-INFO-training batch accuracy: 0.9219; avg_accuracy: 0.8483
20-03-19 15:56-INFO-
20-03-19 15:57-INFO-Epoch 0, Batch 400, Global step 400:
20-03-19 15:57-INFO-training batch loss: 0.1913; avg_loss: 0.4348
20-03-19 15:57-INFO-training batch accuracy: 0.9453; avg_accuracy: 0.8687
20-03-19 15:57-INFO-
20-03-19 15:59-INFO-Epoch 0, Batch 500, Global step 500:
20-03-19 15:59-INFO-training batch loss: 0.1354; avg_loss: 0.3906
20-03-19 15:59-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.8818
20-03-19 15:59-INFO-
20-03-19 16:01-INFO-Epoch 0, Batch 600, Global step 600:
20-03-19 16:01-INFO-training batch loss: 0.1939; avg_loss: 0.3586
20-03-19 16:01-INFO-training batch accuracy: 0.9375; avg_accuracy: 0.8910
20-03-19 16:01-INFO-
20-03-19 16:03-INFO-Epoch 0, Batch 700, Global step 700:
20-03-19 16:03-INFO-training batch loss: 0.1917; avg_loss: 0.3340
20-03-19 16:03-INFO-training batch accuracy: 0.9375; avg_accuracy: 0.8979
20-03-19 16:03-INFO-
20-03-19 16:05-INFO-Epoch 0, Batch 800, Global step 800:
20-03-19 16:05-INFO-training batch loss: 0.2722; avg_loss: 0.3138
20-03-19 16:05-INFO-training batch accuracy: 0.9297; avg_accuracy: 0.9037
20-03-19 16:05-INFO-
20-03-19 16:07-INFO-Epoch 0, Batch 900, Global step 900:
20-03-19 16:07-INFO-training batch loss: 0.3073; avg_loss: 0.2983
20-03-19 16:07-INFO-training batch accuracy: 0.9219; avg_accuracy: 0.9081
20-03-19 16:07-INFO-
20-03-19 16:08-INFO-Epoch 0, Batch 1000, Global step 1000:
20-03-19 16:08-INFO-training batch loss: 0.0989; avg_loss: 0.2853
20-03-19 16:08-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9116
20-03-19 16:08-INFO-
20-03-19 16:11-INFO-Epoch 0, Batch 1100, Global step 1100:
20-03-19 16:11-INFO-training batch loss: 0.2034; avg_loss: 0.2741
20-03-19 16:11-INFO-training batch accuracy: 0.9453; avg_accuracy: 0.9149
20-03-19 16:11-INFO-
20-03-19 16:13-INFO-Epoch 0, Batch 1200, Global step 1200:
20-03-19 16:13-INFO-training batch loss: 0.1893; avg_loss: 0.2641
20-03-19 16:13-INFO-training batch accuracy: 0.9453; avg_accuracy: 0.9179
20-03-19 16:13-INFO-
20-03-19 16:15-INFO-Epoch 0, Batch 1300, Global step 1300:
20-03-19 16:15-INFO-training batch loss: 0.2335; avg_loss: 0.2561
20-03-19 16:15-INFO-training batch accuracy: 0.9141; avg_accuracy: 0.9201
20-03-19 16:15-INFO-
20-03-19 16:17-INFO-Epoch 0, Batch 1400, Global step 1400:
20-03-19 16:17-INFO-training batch loss: 0.1419; avg_loss: 0.2486
20-03-19 16:17-INFO-training batch accuracy: 0.9375; avg_accuracy: 0.9220
20-03-19 16:17-INFO-
20-03-19 16:19-INFO-Epoch 0, Batch 1500, Global step 1500:
20-03-19 16:19-INFO-training batch loss: 0.1104; avg_loss: 0.2417
20-03-19 16:19-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9239
20-03-19 16:19-INFO-
20-03-19 16:22-INFO-Epoch 0, Batch 1600, Global step 1600:
20-03-19 16:22-INFO-training batch loss: 0.1223; avg_loss: 0.2361
20-03-19 16:22-INFO-training batch accuracy: 0.9375; avg_accuracy: 0.9255
20-03-19 16:22-INFO-
20-03-19 16:24-INFO-Epoch 0, Batch 1700, Global step 1700:
20-03-19 16:24-INFO-training batch loss: 0.0750; avg_loss: 0.2308
20-03-19 16:24-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9268
20-03-19 16:24-INFO-
20-03-19 16:26-INFO-Epoch 0, Batch 1800, Global step 1800:
20-03-19 16:26-INFO-training batch loss: 0.1248; avg_loss: 0.2260
20-03-19 16:26-INFO-training batch accuracy: 0.9531; avg_accuracy: 0.9280
20-03-19 16:26-INFO-
20-03-19 16:27-INFO-Epoch 0, Batch 1900, Global step 1900:
20-03-19 16:27-INFO-training batch loss: 0.0804; avg_loss: 0.2220
20-03-19 16:27-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9291
20-03-19 16:27-INFO-
20-03-19 16:29-INFO-Epoch 0, Batch 2000, Global step 2000:
20-03-19 16:29-INFO-training batch loss: 0.2275; avg_loss: 0.2180
20-03-19 16:29-INFO-training batch accuracy: 0.9062; avg_accuracy: 0.9301
20-03-19 16:29-INFO-
20-03-19 16:30-INFO-training batch accuracy: 0.9048; avg_accuracy: 0.9313
20-03-19 16:30-INFO-
20-03-19 16:31-INFO-Epoch 0, evaluating batch loss: 0.1547; avg_loss: 0.2152
20-03-19 16:31-INFO-evaluating batch accuracy: 0.9615; avg_accuracy: 0.9225

20-03-19 16:31-INFO-
20-03-19 16:32-INFO-Epoch 1, Batch 16, Global step 2100:
20-03-19 16:32-INFO-training batch loss: 0.1417; avg_loss: 0.1201
20-03-19 16:32-INFO-training batch accuracy: 0.9609; avg_accuracy: 0.8945
20-03-19 16:32-INFO-
20-03-19 16:34-INFO-Epoch 1, Batch 116, Global step 2200:
20-03-19 16:34-INFO-training batch loss: 0.1248; avg_loss: 0.1388
20-03-19 16:34-INFO-training batch accuracy: 0.9453; avg_accuracy: 0.9426
20-03-19 16:34-INFO-
20-03-19 16:36-INFO-Epoch 1, Batch 216, Global step 2300:
20-03-19 16:36-INFO-training batch loss: 0.1117; avg_loss: 0.1421
20-03-19 16:36-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9450
20-03-19 16:36-INFO-
20-03-19 16:37-INFO-Epoch 1, Batch 316, Global step 2400:
20-03-19 16:37-INFO-training batch loss: 0.2112; avg_loss: 0.1423
20-03-19 16:37-INFO-training batch accuracy: 0.9219; avg_accuracy: 0.9464
20-03-19 16:37-INFO-
20-03-19 16:38-INFO-Epoch 1, Batch 416, Global step 2500:
20-03-19 16:38-INFO-training batch loss: 0.1673; avg_loss: 0.1405
20-03-19 16:38-INFO-training batch accuracy: 0.9375; avg_accuracy: 0.9476
20-03-19 16:38-INFO-
20-03-19 16:39-INFO-Epoch 1, Batch 516, Global step 2600:
20-03-19 16:39-INFO-training batch loss: 0.1261; avg_loss: 0.1406
20-03-19 16:39-INFO-training batch accuracy: 0.9375; avg_accuracy: 0.9475
20-03-19 16:39-INFO-
20-03-19 16:41-INFO-Epoch 1, Batch 616, Global step 2700:
20-03-19 16:41-INFO-training batch loss: 0.1223; avg_loss: 0.1407
20-03-19 16:41-INFO-training batch accuracy: 0.9375; avg_accuracy: 0.9478
20-03-19 16:41-INFO-
20-03-19 16:43-INFO-Epoch 1, Batch 716, Global step 2800:
20-03-19 16:43-INFO-training batch loss: 0.1877; avg_loss: 0.1395
20-03-19 16:43-INFO-training batch accuracy: 0.9609; avg_accuracy: 0.9484
20-03-19 16:43-INFO-
20-03-19 16:44-INFO-Epoch 1, Batch 816, Global step 2900:
20-03-19 16:44-INFO-training batch loss: 0.1274; avg_loss: 0.1388
20-03-19 16:44-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9487
20-03-19 16:44-INFO-
20-03-19 16:46-INFO-Epoch 1, Batch 916, Global step 3000:
20-03-19 16:46-INFO-training batch loss: 0.1049; avg_loss: 0.1386
20-03-19 16:46-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9487
20-03-19 16:46-INFO-
20-03-19 16:48-INFO-Epoch 1, Batch 1016, Global step 3100:
20-03-19 16:48-INFO-training batch loss: 0.1293; avg_loss: 0.1381
20-03-19 16:48-INFO-training batch accuracy: 0.9609; avg_accuracy: 0.9489
20-03-19 16:48-INFO-
20-03-19 16:49-INFO-Epoch 1, Batch 1116, Global step 3200:
20-03-19 16:49-INFO-training batch loss: 0.1605; avg_loss: 0.1376
20-03-19 16:49-INFO-training batch accuracy: 0.9375; avg_accuracy: 0.9493
20-03-19 16:49-INFO-
20-03-19 16:50-INFO-Epoch 1, Batch 1216, Global step 3300:
20-03-19 16:50-INFO-training batch loss: 0.0768; avg_loss: 0.1367
20-03-19 16:50-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9497
20-03-19 16:50-INFO-
20-03-19 16:51-INFO-Epoch 1, Batch 1316, Global step 3400:
20-03-19 16:51-INFO-training batch loss: 0.1079; avg_loss: 0.1367
20-03-19 16:51-INFO-training batch accuracy: 0.9609; avg_accuracy: 0.9496
20-03-19 16:51-INFO-
20-03-19 16:52-INFO-Epoch 1, Batch 1416, Global step 3500:
20-03-19 16:52-INFO-training batch loss: 0.1400; avg_loss: 0.1359
20-03-19 16:52-INFO-training batch accuracy: 0.9609; avg_accuracy: 0.9499
20-03-19 16:52-INFO-
20-03-19 16:53-INFO-Epoch 1, Batch 1516, Global step 3600:
20-03-19 16:53-INFO-training batch loss: 0.1901; avg_loss: 0.1357
20-03-19 16:53-INFO-training batch accuracy: 0.9219; avg_accuracy: 0.9502
20-03-19 16:53-INFO-
20-03-19 16:53-INFO-Epoch 1, Batch 1616, Global step 3700:
20-03-19 16:53-INFO-training batch loss: 0.1365; avg_loss: 0.1350
20-03-19 16:53-INFO-training batch accuracy: 0.9531; avg_accuracy: 0.9503
20-03-19 16:53-INFO-
20-03-19 16:54-INFO-Epoch 1, Batch 1716, Global step 3800:
20-03-19 16:54-INFO-training batch loss: 0.1054; avg_loss: 0.1345
20-03-19 16:54-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9504
20-03-19 16:54-INFO-
20-03-19 16:55-INFO-Epoch 1, Batch 1816, Global step 3900:
20-03-19 16:55-INFO-training batch loss: 0.0609; avg_loss: 0.1340
20-03-19 16:55-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9505
20-03-19 16:55-INFO-
20-03-19 16:57-INFO-Epoch 1, Batch 1916, Global step 4000:
20-03-19 16:57-INFO-training batch loss: 0.0550; avg_loss: 0.1337
20-03-19 16:57-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9505
20-03-19 16:57-INFO-
20-03-19 16:58-INFO-Epoch 1, Batch 2016, Global step 4100:
20-03-19 16:58-INFO-training batch loss: 0.1189; avg_loss: 0.1333
20-03-19 16:58-INFO-training batch accuracy: 0.9609; avg_accuracy: 0.9506
20-03-19 16:58-INFO-
20-03-19 17:00-INFO-training batch accuracy: 0.9048; avg_accuracy: 0.9510
20-03-19 17:00-INFO-
20-03-19 17:01-INFO-Epoch 1, evaluating batch loss: 0.1399; avg_loss: 0.1913
20-03-19 17:01-INFO-evaluating batch accuracy: 0.9519; avg_accuracy: 0.9241

20-03-19 17:01-INFO-
20-03-19 17:02-INFO-Epoch 2, Batch 32, Global step 4200:
20-03-19 17:02-INFO-training batch loss: 0.1205; avg_loss: 0.1171
20-03-19 17:02-INFO-training batch accuracy: 0.9609; avg_accuracy: 0.9243
20-03-19 17:02-INFO-
20-03-19 17:03-INFO-Epoch 2, Batch 132, Global step 4300:
20-03-19 17:03-INFO-training batch loss: 0.1476; avg_loss: 0.1226
20-03-19 17:03-INFO-training batch accuracy: 0.9375; avg_accuracy: 0.9445
20-03-19 17:03-INFO-
20-03-19 17:04-INFO-Epoch 2, Batch 232, Global step 4400:
20-03-19 17:04-INFO-training batch loss: 0.0616; avg_loss: 0.1259
20-03-19 17:04-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9469
20-03-19 17:04-INFO-
20-03-19 17:06-INFO-Epoch 2, Batch 332, Global step 4500:
20-03-19 17:06-INFO-training batch loss: 0.0721; avg_loss: 0.1243
20-03-19 17:06-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9489
20-03-19 17:06-INFO-
20-03-19 17:08-INFO-Epoch 2, Batch 432, Global step 4600:
20-03-19 17:08-INFO-training batch loss: 0.1532; avg_loss: 0.1241
20-03-19 17:08-INFO-training batch accuracy: 0.9297; avg_accuracy: 0.9496
20-03-19 17:08-INFO-
20-03-19 17:09-INFO-Epoch 2, Batch 532, Global step 4700:
20-03-19 17:09-INFO-training batch loss: 0.1975; avg_loss: 0.1239
20-03-19 17:09-INFO-training batch accuracy: 0.9141; avg_accuracy: 0.9496
20-03-19 17:09-INFO-
20-03-19 17:10-INFO-Epoch 2, Batch 632, Global step 4800:
20-03-19 17:10-INFO-training batch loss: 0.2004; avg_loss: 0.1242
20-03-19 17:10-INFO-training batch accuracy: 0.9297; avg_accuracy: 0.9499
20-03-19 17:10-INFO-
20-03-19 17:12-INFO-Epoch 2, Batch 732, Global step 4900:
20-03-19 17:12-INFO-training batch loss: 0.1207; avg_loss: 0.1239
20-03-19 17:12-INFO-training batch accuracy: 0.9531; avg_accuracy: 0.9502
20-03-19 17:12-INFO-
20-03-19 17:13-INFO-Epoch 2, Batch 832, Global step 5000:
20-03-19 17:13-INFO-training batch loss: 0.0769; avg_loss: 0.1236
20-03-19 17:13-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9504
20-03-19 17:13-INFO-
20-03-19 17:15-INFO-Epoch 2, Batch 932, Global step 5100:
20-03-19 17:15-INFO-training batch loss: 0.1173; avg_loss: 0.1233
20-03-19 17:15-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9507
20-03-19 17:15-INFO-
20-03-19 17:16-INFO-Epoch 2, Batch 1032, Global step 5200:
20-03-19 17:16-INFO-training batch loss: 0.0977; avg_loss: 0.1231
20-03-19 17:16-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9510
20-03-19 17:16-INFO-
20-03-19 17:18-INFO-Epoch 2, Batch 1132, Global step 5300:
20-03-19 17:18-INFO-training batch loss: 0.1578; avg_loss: 0.1230
20-03-19 17:18-INFO-training batch accuracy: 0.9297; avg_accuracy: 0.9513
20-03-19 17:18-INFO-
20-03-19 17:20-INFO-Epoch 2, Batch 1232, Global step 5400:
20-03-19 17:20-INFO-training batch loss: 0.1353; avg_loss: 0.1226
20-03-19 17:20-INFO-training batch accuracy: 0.9375; avg_accuracy: 0.9516
20-03-19 17:20-INFO-
20-03-19 17:22-INFO-Epoch 2, Batch 1332, Global step 5500:
20-03-19 17:22-INFO-training batch loss: 0.1008; avg_loss: 0.1224
20-03-19 17:22-INFO-training batch accuracy: 0.9609; avg_accuracy: 0.9517
20-03-19 17:22-INFO-
20-03-19 17:24-INFO-Epoch 2, Batch 1432, Global step 5600:
20-03-19 17:24-INFO-training batch loss: 0.1639; avg_loss: 0.1219
20-03-19 17:24-INFO-training batch accuracy: 0.9453; avg_accuracy: 0.9521
20-03-19 17:24-INFO-
20-03-19 17:27-INFO-Epoch 2, Batch 1532, Global step 5700:
20-03-19 17:27-INFO-training batch loss: 0.1121; avg_loss: 0.1218
20-03-19 17:27-INFO-training batch accuracy: 0.9609; avg_accuracy: 0.9521
20-03-19 17:27-INFO-
20-03-19 17:29-INFO-Epoch 2, Batch 1632, Global step 5800:
20-03-19 17:29-INFO-training batch loss: 0.1045; avg_loss: 0.1212
20-03-19 17:29-INFO-training batch accuracy: 0.9609; avg_accuracy: 0.9523
20-03-19 17:29-INFO-
20-03-19 17:32-INFO-Epoch 2, Batch 1732, Global step 5900:
20-03-19 17:32-INFO-training batch loss: 0.0757; avg_loss: 0.1209
20-03-19 17:32-INFO-training batch accuracy: 0.9609; avg_accuracy: 0.9523
20-03-19 17:32-INFO-
20-03-19 17:34-INFO-Epoch 2, Batch 1832, Global step 6000:
20-03-19 17:34-INFO-training batch loss: 0.0919; avg_loss: 0.1205
20-03-19 17:34-INFO-training batch accuracy: 0.9531; avg_accuracy: 0.9523
20-03-19 17:34-INFO-
20-03-19 17:35-INFO-Epoch 2, Batch 1932, Global step 6100:
20-03-19 17:35-INFO-training batch loss: 0.1226; avg_loss: 0.1205
20-03-19 17:35-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9523
20-03-19 17:35-INFO-
20-03-19 17:37-INFO-Epoch 2, Batch 2032, Global step 6200:
20-03-19 17:37-INFO-training batch loss: 0.1214; avg_loss: 0.1200
20-03-19 17:37-INFO-training batch accuracy: 0.9453; avg_accuracy: 0.9525
20-03-19 17:37-INFO-
20-03-19 17:38-INFO-training batch accuracy: 0.9048; avg_accuracy: 0.9529
20-03-19 17:38-INFO-
20-03-19 17:39-INFO-Epoch 2, evaluating batch loss: 0.1114; avg_loss: 0.1832
20-03-19 17:39-INFO-evaluating batch accuracy: 0.9712; avg_accuracy: 0.9245

20-03-19 17:39-INFO-
20-03-19 17:41-INFO-Epoch 3, Batch 48, Global step 6300:
20-03-19 17:41-INFO-training batch loss: 0.1204; avg_loss: 0.1100
20-03-19 17:41-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9338
20-03-19 17:41-INFO-
20-03-19 17:43-INFO-Epoch 3, Batch 148, Global step 6400:
20-03-19 17:43-INFO-training batch loss: 0.0799; avg_loss: 0.1132
20-03-19 17:43-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9472
20-03-19 17:43-INFO-
20-03-19 17:45-INFO-Epoch 3, Batch 248, Global step 6500:
20-03-19 17:45-INFO-training batch loss: 0.1730; avg_loss: 0.1136
20-03-19 17:45-INFO-training batch accuracy: 0.9062; avg_accuracy: 0.9505
20-03-19 17:45-INFO-
20-03-19 17:46-INFO-Epoch 3, Batch 348, Global step 6600:
20-03-19 17:46-INFO-training batch loss: 0.1641; avg_loss: 0.1127
20-03-19 17:46-INFO-training batch accuracy: 0.9453; avg_accuracy: 0.9523
20-03-19 17:46-INFO-
20-03-19 17:47-INFO-Epoch 3, Batch 448, Global step 6700:
20-03-19 17:47-INFO-training batch loss: 0.1226; avg_loss: 0.1131
20-03-19 17:47-INFO-training batch accuracy: 0.9453; avg_accuracy: 0.9527
20-03-19 17:47-INFO-
20-03-19 17:49-INFO-Epoch 3, Batch 548, Global step 6800:
20-03-19 17:49-INFO-training batch loss: 0.1516; avg_loss: 0.1129
20-03-19 17:49-INFO-training batch accuracy: 0.9375; avg_accuracy: 0.9531
20-03-19 17:49-INFO-
20-03-19 17:50-INFO-Epoch 3, Batch 648, Global step 6900:
20-03-19 17:50-INFO-training batch loss: 0.1197; avg_loss: 0.1138
20-03-19 17:50-INFO-training batch accuracy: 0.9609; avg_accuracy: 0.9533
20-03-19 17:50-INFO-
20-03-19 17:52-INFO-Epoch 3, Batch 748, Global step 7000:
20-03-19 17:52-INFO-training batch loss: 0.1164; avg_loss: 0.1139
20-03-19 17:52-INFO-training batch accuracy: 0.9453; avg_accuracy: 0.9533
20-03-19 17:52-INFO-
20-03-19 17:54-INFO-Epoch 3, Batch 848, Global step 7100:
20-03-19 17:54-INFO-training batch loss: 0.0918; avg_loss: 0.1135
20-03-19 17:54-INFO-training batch accuracy: 0.9609; avg_accuracy: 0.9534
20-03-19 17:54-INFO-
20-03-19 17:56-INFO-Epoch 3, Batch 948, Global step 7200:
20-03-19 17:56-INFO-training batch loss: 0.1728; avg_loss: 0.1134
20-03-19 17:56-INFO-training batch accuracy: 0.9062; avg_accuracy: 0.9536
20-03-19 17:56-INFO-
20-03-19 17:57-INFO-Epoch 3, Batch 1048, Global step 7300:
20-03-19 17:57-INFO-training batch loss: 0.0625; avg_loss: 0.1134
20-03-19 17:57-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9538
20-03-19 17:57-INFO-
20-03-19 17:59-INFO-Epoch 3, Batch 1148, Global step 7400:
20-03-19 17:59-INFO-training batch loss: 0.1012; avg_loss: 0.1132
20-03-19 17:59-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9540
20-03-19 17:59-INFO-
20-03-19 18:00-INFO-Epoch 3, Batch 1248, Global step 7500:
20-03-19 18:00-INFO-training batch loss: 0.1085; avg_loss: 0.1130
20-03-19 18:00-INFO-training batch accuracy: 0.9375; avg_accuracy: 0.9542
20-03-19 18:00-INFO-
20-03-19 18:02-INFO-Epoch 3, Batch 1348, Global step 7600:
20-03-19 18:02-INFO-training batch loss: 0.0791; avg_loss: 0.1129
20-03-19 18:02-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9543
20-03-19 18:02-INFO-
20-03-19 18:03-INFO-Epoch 3, Batch 1448, Global step 7700:
20-03-19 18:03-INFO-training batch loss: 0.0915; avg_loss: 0.1126
20-03-19 18:03-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9544
20-03-19 18:03-INFO-
20-03-19 18:04-INFO-Epoch 3, Batch 1548, Global step 7800:
20-03-19 18:04-INFO-training batch loss: 0.0932; avg_loss: 0.1125
20-03-19 18:04-INFO-training batch accuracy: 0.9531; avg_accuracy: 0.9545
20-03-19 18:04-INFO-
20-03-19 18:04-INFO-Epoch 3, Batch 1648, Global step 7900:
20-03-19 18:04-INFO-training batch loss: 0.0915; avg_loss: 0.1120
20-03-19 18:04-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9546
20-03-19 18:04-INFO-
20-03-19 18:05-INFO-Epoch 3, Batch 1748, Global step 8000:
20-03-19 18:05-INFO-training batch loss: 0.1090; avg_loss: 0.1118
20-03-19 18:05-INFO-training batch accuracy: 0.9375; avg_accuracy: 0.9547
20-03-19 18:05-INFO-
20-03-19 18:07-INFO-Epoch 3, Batch 1848, Global step 8100:
20-03-19 18:07-INFO-training batch loss: 0.1662; avg_loss: 0.1115
20-03-19 18:07-INFO-training batch accuracy: 0.9062; avg_accuracy: 0.9547
20-03-19 18:07-INFO-
20-03-19 18:08-INFO-Epoch 3, Batch 1948, Global step 8200:
20-03-19 18:08-INFO-training batch loss: 0.0626; avg_loss: 0.1113
20-03-19 18:08-INFO-training batch accuracy: 0.9609; avg_accuracy: 0.9548
20-03-19 18:08-INFO-
20-03-19 18:10-INFO-Epoch 3, Batch 2048, Global step 8300:
20-03-19 18:10-INFO-training batch loss: 0.0937; avg_loss: 0.1109
20-03-19 18:10-INFO-training batch accuracy: 0.9531; avg_accuracy: 0.9550
20-03-19 18:10-INFO-
20-03-19 18:11-INFO-training batch accuracy: 0.9048; avg_accuracy: 0.9554
20-03-19 18:11-INFO-
20-03-19 18:12-INFO-Epoch 3, evaluating batch loss: 0.1303; avg_loss: 0.1700
20-03-19 18:12-INFO-evaluating batch accuracy: 0.9712; avg_accuracy: 0.9255

20-03-19 18:12-INFO-
20-03-19 18:13-INFO-Epoch 4, Batch 64, Global step 8400:
20-03-19 18:13-INFO-training batch loss: 0.1207; avg_loss: 0.1022
20-03-19 18:13-INFO-training batch accuracy: 0.9531; avg_accuracy: 0.9438
20-03-19 18:13-INFO-
20-03-19 18:14-INFO-Epoch 4, Batch 164, Global step 8500:
20-03-19 18:14-INFO-training batch loss: 0.0549; avg_loss: 0.1051
20-03-19 18:14-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9512
20-03-19 18:14-INFO-
20-03-19 18:15-INFO-Epoch 4, Batch 264, Global step 8600:
20-03-19 18:15-INFO-training batch loss: 0.1743; avg_loss: 0.1068
20-03-19 18:15-INFO-training batch accuracy: 0.9219; avg_accuracy: 0.9539
20-03-19 18:15-INFO-
20-03-19 18:17-INFO-Epoch 4, Batch 364, Global step 8700:
20-03-19 18:17-INFO-training batch loss: 0.1255; avg_loss: 0.1057
20-03-19 18:17-INFO-training batch accuracy: 0.9375; avg_accuracy: 0.9552
20-03-19 18:17-INFO-
20-03-19 18:19-INFO-Epoch 4, Batch 464, Global step 8800:
20-03-19 18:19-INFO-training batch loss: 0.0568; avg_loss: 0.1061
20-03-19 18:19-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9553
20-03-19 18:19-INFO-
20-03-19 18:20-INFO-Epoch 4, Batch 564, Global step 8900:
20-03-19 18:20-INFO-training batch loss: 0.0661; avg_loss: 0.1058
20-03-19 18:20-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9556
20-03-19 18:20-INFO-
20-03-19 18:21-INFO-Epoch 4, Batch 664, Global step 9000:
20-03-19 18:21-INFO-training batch loss: 0.0710; avg_loss: 0.1058
20-03-19 18:21-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9560
20-03-19 18:21-INFO-
20-03-19 18:23-INFO-Epoch 4, Batch 764, Global step 9100:
20-03-19 18:23-INFO-training batch loss: 0.0995; avg_loss: 0.1059
20-03-19 18:23-INFO-training batch accuracy: 0.9531; avg_accuracy: 0.9561
20-03-19 18:23-INFO-
20-03-19 18:24-INFO-Epoch 4, Batch 864, Global step 9200:
20-03-19 18:24-INFO-training batch loss: 0.1595; avg_loss: 0.1061
20-03-19 18:24-INFO-training batch accuracy: 0.9531; avg_accuracy: 0.9561
20-03-19 18:24-INFO-
20-03-19 18:26-INFO-Epoch 4, Batch 964, Global step 9300:
20-03-19 18:26-INFO-training batch loss: 0.1180; avg_loss: 0.1060
20-03-19 18:26-INFO-training batch accuracy: 0.9531; avg_accuracy: 0.9563
20-03-19 18:26-INFO-
20-03-19 18:27-INFO-Epoch 4, Batch 1064, Global step 9400:
20-03-19 18:27-INFO-training batch loss: 0.1079; avg_loss: 0.1060
20-03-19 18:27-INFO-training batch accuracy: 0.9609; avg_accuracy: 0.9564
20-03-19 18:27-INFO-
20-03-19 18:29-INFO-Epoch 4, Batch 1164, Global step 9500:
20-03-19 18:29-INFO-training batch loss: 0.0933; avg_loss: 0.1057
20-03-19 18:29-INFO-training batch accuracy: 0.9609; avg_accuracy: 0.9566
20-03-19 18:29-INFO-
20-03-19 18:31-INFO-Epoch 4, Batch 1264, Global step 9600:
20-03-19 18:31-INFO-training batch loss: 0.0671; avg_loss: 0.1055
20-03-19 18:31-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9567
20-03-19 18:31-INFO-
20-03-19 18:32-INFO-Epoch 4, Batch 1364, Global step 9700:
20-03-19 18:32-INFO-training batch loss: 0.1176; avg_loss: 0.1054
20-03-19 18:32-INFO-training batch accuracy: 0.9609; avg_accuracy: 0.9568
20-03-19 18:32-INFO-
20-03-19 18:34-INFO-Epoch 4, Batch 1464, Global step 9800:
20-03-19 18:34-INFO-training batch loss: 0.0855; avg_loss: 0.1053
20-03-19 18:34-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9569
20-03-19 18:34-INFO-
20-03-19 18:36-INFO-Epoch 4, Batch 1564, Global step 9900:
20-03-19 18:36-INFO-training batch loss: 0.0931; avg_loss: 0.1050
20-03-19 18:36-INFO-training batch accuracy: 0.9609; avg_accuracy: 0.9571
20-03-19 18:36-INFO-
20-03-19 18:37-INFO-Epoch 4, Batch 1664, Global step 10000:
20-03-19 18:37-INFO-training batch loss: 0.0555; avg_loss: 0.1048
20-03-19 18:37-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9571
20-03-19 18:37-INFO-
20-03-19 18:38-INFO-Epoch 4, Batch 1764, Global step 10100:
20-03-19 18:38-INFO-training batch loss: 0.1124; avg_loss: 0.1047
20-03-19 18:38-INFO-training batch accuracy: 0.9453; avg_accuracy: 0.9571
20-03-19 18:38-INFO-
20-03-19 18:39-INFO-Epoch 4, Batch 1864, Global step 10200:
20-03-19 18:39-INFO-training batch loss: 0.1005; avg_loss: 0.1045
20-03-19 18:39-INFO-training batch accuracy: 0.9453; avg_accuracy: 0.9571
20-03-19 18:39-INFO-
20-03-19 18:40-INFO-Epoch 4, Batch 1964, Global step 10300:
20-03-19 18:40-INFO-training batch loss: 0.0912; avg_loss: 0.1043
20-03-19 18:40-INFO-training batch accuracy: 0.9609; avg_accuracy: 0.9571
20-03-19 18:40-INFO-
20-03-19 18:41-INFO-Epoch 4, Batch 2064, Global step 10400:
20-03-19 18:41-INFO-training batch loss: 0.1156; avg_loss: 0.1040
20-03-19 18:41-INFO-training batch accuracy: 0.9531; avg_accuracy: 0.9574
20-03-19 18:41-INFO-
20-03-19 18:41-INFO-training batch accuracy: 0.9048; avg_accuracy: 0.9578
20-03-19 18:41-INFO-
20-03-19 18:42-INFO-Epoch 4, evaluating batch loss: 0.1198; avg_loss: 0.1533
20-03-19 18:42-INFO-evaluating batch accuracy: 0.9615; avg_accuracy: 0.9306

20-03-19 18:42-INFO-
20-03-19 18:43-INFO-Epoch 5, Batch 80, Global step 10500:
20-03-19 18:43-INFO-training batch loss: 0.0337; avg_loss: 0.0974
20-03-19 18:43-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9477
20-03-19 18:43-INFO-
20-03-19 18:44-INFO-Epoch 5, Batch 180, Global step 10600:
20-03-19 18:44-INFO-training batch loss: 0.0758; avg_loss: 0.1013
20-03-19 18:44-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9532
20-03-19 18:44-INFO-
20-03-19 18:45-INFO-Epoch 5, Batch 280, Global step 10700:
20-03-19 18:45-INFO-training batch loss: 0.1037; avg_loss: 0.1012
20-03-19 18:45-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9555
20-03-19 18:45-INFO-
20-03-19 18:46-INFO-Epoch 5, Batch 380, Global step 10800:
20-03-19 18:46-INFO-training batch loss: 0.0785; avg_loss: 0.0996
20-03-19 18:46-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9574
20-03-19 18:46-INFO-
20-03-19 18:48-INFO-Epoch 5, Batch 480, Global step 10900:
20-03-19 18:48-INFO-training batch loss: 0.1366; avg_loss: 0.1001
20-03-19 18:48-INFO-training batch accuracy: 0.9375; avg_accuracy: 0.9573
20-03-19 18:48-INFO-
20-03-19 18:50-INFO-Epoch 5, Batch 580, Global step 11000:
20-03-19 18:50-INFO-training batch loss: 0.1019; avg_loss: 0.0999
20-03-19 18:50-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9578
20-03-19 18:50-INFO-
20-03-19 18:52-INFO-Epoch 5, Batch 680, Global step 11100:
20-03-19 18:52-INFO-training batch loss: 0.1254; avg_loss: 0.1000
20-03-19 18:52-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9584
20-03-19 18:52-INFO-
20-03-19 18:54-INFO-Epoch 5, Batch 780, Global step 11200:
20-03-19 18:54-INFO-training batch loss: 0.1064; avg_loss: 0.1003
20-03-19 18:54-INFO-training batch accuracy: 0.9453; avg_accuracy: 0.9584
20-03-19 18:54-INFO-
20-03-19 18:55-INFO-Epoch 5, Batch 880, Global step 11300:
20-03-19 18:55-INFO-training batch loss: 0.0794; avg_loss: 0.1004
20-03-19 18:55-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9584
20-03-19 18:55-INFO-
20-03-19 18:57-INFO-Epoch 5, Batch 980, Global step 11400:
20-03-19 18:57-INFO-training batch loss: 0.0734; avg_loss: 0.1005
20-03-19 18:57-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9585
20-03-19 18:57-INFO-
20-03-19 18:58-INFO-Epoch 5, Batch 1080, Global step 11500:
20-03-19 18:58-INFO-training batch loss: 0.1180; avg_loss: 0.1006
20-03-19 18:58-INFO-training batch accuracy: 0.9531; avg_accuracy: 0.9586
20-03-19 18:58-INFO-
20-03-19 19:00-INFO-Epoch 5, Batch 1180, Global step 11600:
20-03-19 19:00-INFO-training batch loss: 0.1073; avg_loss: 0.1003
20-03-19 19:00-INFO-training batch accuracy: 0.9453; avg_accuracy: 0.9588
20-03-19 19:00-INFO-
20-03-19 19:02-INFO-Epoch 5, Batch 1280, Global step 11700:
20-03-19 19:02-INFO-training batch loss: 0.0671; avg_loss: 0.1003
20-03-19 19:02-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9588
20-03-19 19:02-INFO-
20-03-19 19:03-INFO-Epoch 5, Batch 1380, Global step 11800:
20-03-19 19:03-INFO-training batch loss: 0.1209; avg_loss: 0.1004
20-03-19 19:03-INFO-training batch accuracy: 0.9453; avg_accuracy: 0.9589
20-03-19 19:03-INFO-
20-03-19 19:04-INFO-Epoch 5, Batch 1480, Global step 11900:
20-03-19 19:04-INFO-training batch loss: 0.0806; avg_loss: 0.1003
20-03-19 19:04-INFO-training batch accuracy: 0.9609; avg_accuracy: 0.9591
20-03-19 19:04-INFO-
20-03-19 19:04-INFO-Epoch 5, Batch 1580, Global step 12000:
20-03-19 19:04-INFO-training batch loss: 0.0554; avg_loss: 0.1001
20-03-19 19:04-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9591
20-03-19 19:04-INFO-
20-03-19 19:06-INFO-Epoch 5, Batch 1680, Global step 12100:
20-03-19 19:06-INFO-training batch loss: 0.0646; avg_loss: 0.1000
20-03-19 19:06-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9591
20-03-19 19:06-INFO-
20-03-19 19:08-INFO-Epoch 5, Batch 1780, Global step 12200:
20-03-19 19:08-INFO-training batch loss: 0.1321; avg_loss: 0.1000
20-03-19 19:08-INFO-training batch accuracy: 0.9453; avg_accuracy: 0.9591
20-03-19 19:08-INFO-
20-03-19 19:10-INFO-Epoch 5, Batch 1880, Global step 12300:
20-03-19 19:10-INFO-training batch loss: 0.1039; avg_loss: 0.0998
20-03-19 19:10-INFO-training batch accuracy: 0.9609; avg_accuracy: 0.9591
20-03-19 19:10-INFO-
20-03-19 19:12-INFO-Epoch 5, Batch 1980, Global step 12400:
20-03-19 19:12-INFO-training batch loss: 0.0521; avg_loss: 0.0996
20-03-19 19:12-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9591
20-03-19 19:12-INFO-
20-03-19 19:14-INFO-Epoch 5, Batch 2080, Global step 12500:
20-03-19 19:14-INFO-training batch loss: 0.1326; avg_loss: 0.0996
20-03-19 19:14-INFO-training batch accuracy: 0.9609; avg_accuracy: 0.9592
20-03-19 19:14-INFO-
20-03-19 19:14-INFO-training batch accuracy: 0.9048; avg_accuracy: 0.9597
20-03-19 19:14-INFO-
20-03-19 19:16-INFO-Epoch 5, evaluating batch loss: 0.1185; avg_loss: 0.1554
20-03-19 19:16-INFO-evaluating batch accuracy: 0.9712; avg_accuracy: 0.9319

20-03-19 19:16-INFO-
20-03-19 19:16-WARNING-From /root/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to delete files with this prefix.
20-03-19 19:19-INFO-Epoch 6, Batch 96, Global step 12600:
20-03-19 19:19-INFO-training batch loss: 0.0495; avg_loss: 0.0935
20-03-19 19:19-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9506
20-03-19 19:19-INFO-
20-03-19 19:21-INFO-Epoch 6, Batch 196, Global step 12700:
20-03-19 19:21-INFO-training batch loss: 0.0920; avg_loss: 0.0963
20-03-19 19:21-INFO-training batch accuracy: 0.9453; avg_accuracy: 0.9560
20-03-19 19:21-INFO-
20-03-19 19:21-INFO-Epoch 6, Batch 296, Global step 12800:
20-03-19 19:21-INFO-training batch loss: 0.0908; avg_loss: 0.0966
20-03-19 19:21-INFO-training batch accuracy: 0.9531; avg_accuracy: 0.9577
20-03-19 19:21-INFO-
20-03-19 19:23-INFO-Epoch 6, Batch 396, Global step 12900:
20-03-19 19:23-INFO-training batch loss: 0.1266; avg_loss: 0.0959
20-03-19 19:23-INFO-training batch accuracy: 0.9297; avg_accuracy: 0.9587
20-03-19 19:23-INFO-
20-03-19 19:24-INFO-Epoch 6, Batch 496, Global step 13000:
20-03-19 19:24-INFO-training batch loss: 0.0393; avg_loss: 0.0959
20-03-19 19:24-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9589
20-03-19 19:24-INFO-
20-03-19 19:26-INFO-Epoch 6, Batch 596, Global step 13100:
20-03-19 19:26-INFO-training batch loss: 0.0939; avg_loss: 0.0966
20-03-19 19:26-INFO-training batch accuracy: 0.9609; avg_accuracy: 0.9594
20-03-19 19:26-INFO-
20-03-19 19:27-INFO-Epoch 6, Batch 696, Global step 13200:
20-03-19 19:27-INFO-training batch loss: 0.0402; avg_loss: 0.0968
20-03-19 19:27-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9597
20-03-19 19:27-INFO-
20-03-19 19:29-INFO-Epoch 6, Batch 796, Global step 13300:
20-03-19 19:29-INFO-training batch loss: 0.1244; avg_loss: 0.0966
20-03-19 19:29-INFO-training batch accuracy: 0.9609; avg_accuracy: 0.9600
20-03-19 19:29-INFO-
20-03-19 19:31-INFO-Epoch 6, Batch 896, Global step 13400:
20-03-19 19:31-INFO-training batch loss: 0.0753; avg_loss: 0.0969
20-03-19 19:31-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9598
20-03-19 19:31-INFO-
20-03-19 19:32-INFO-Epoch 6, Batch 996, Global step 13500:
20-03-19 19:32-INFO-training batch loss: 0.0795; avg_loss: 0.0969
20-03-19 19:32-INFO-training batch accuracy: 0.9453; avg_accuracy: 0.9602
20-03-19 19:32-INFO-
20-03-19 19:34-INFO-Epoch 6, Batch 1096, Global step 13600:
20-03-19 19:34-INFO-training batch loss: 0.0857; avg_loss: 0.0971
20-03-19 19:34-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9602
20-03-19 19:34-INFO-
20-03-19 19:36-INFO-Epoch 6, Batch 1196, Global step 13700:
20-03-19 19:36-INFO-training batch loss: 0.1358; avg_loss: 0.0965
20-03-19 19:36-INFO-training batch accuracy: 0.9375; avg_accuracy: 0.9605
20-03-19 19:36-INFO-
20-03-19 19:38-INFO-Epoch 6, Batch 1296, Global step 13800:
20-03-19 19:38-INFO-training batch loss: 0.0559; avg_loss: 0.0969
20-03-19 19:38-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9604
20-03-19 19:38-INFO-
20-03-19 19:41-INFO-Epoch 6, Batch 1396, Global step 13900:
20-03-19 19:41-INFO-training batch loss: 0.1064; avg_loss: 0.0968
20-03-19 19:41-INFO-training batch accuracy: 0.9453; avg_accuracy: 0.9606
20-03-19 19:41-INFO-
20-03-19 19:43-INFO-Epoch 6, Batch 1496, Global step 14000:
20-03-19 19:43-INFO-training batch loss: 0.0845; avg_loss: 0.0968
20-03-19 19:43-INFO-training batch accuracy: 0.9531; avg_accuracy: 0.9606
20-03-19 19:43-INFO-
20-03-19 19:45-INFO-Epoch 6, Batch 1596, Global step 14100:
20-03-19 19:45-INFO-training batch loss: 0.1225; avg_loss: 0.0967
20-03-19 19:45-INFO-training batch accuracy: 0.9609; avg_accuracy: 0.9606
20-03-19 19:45-INFO-
20-03-19 19:46-INFO-Epoch 6, Batch 1696, Global step 14200:
20-03-19 19:46-INFO-training batch loss: 0.1484; avg_loss: 0.0967
20-03-19 19:46-INFO-training batch accuracy: 0.9375; avg_accuracy: 0.9606
20-03-19 19:46-INFO-
20-03-19 19:48-INFO-Epoch 6, Batch 1796, Global step 14300:
20-03-19 19:48-INFO-training batch loss: 0.1073; avg_loss: 0.0965
20-03-19 19:48-INFO-training batch accuracy: 0.9375; avg_accuracy: 0.9604
20-03-19 19:48-INFO-
20-03-19 19:50-INFO-Epoch 6, Batch 1896, Global step 14400:
20-03-19 19:50-INFO-training batch loss: 0.0584; avg_loss: 0.0965
20-03-19 19:50-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9604
20-03-19 19:50-INFO-
20-03-19 19:52-INFO-Epoch 6, Batch 1996, Global step 14500:
20-03-19 19:52-INFO-training batch loss: 0.1053; avg_loss: 0.0962
20-03-19 19:52-INFO-training batch accuracy: 0.9531; avg_accuracy: 0.9606
20-03-19 19:52-INFO-
20-03-19 19:54-INFO-training batch accuracy: 0.9048; avg_accuracy: 0.9611
20-03-19 19:54-INFO-
20-03-19 19:55-INFO-Epoch 6, evaluating batch loss: 0.1143; avg_loss: 0.1517
20-03-19 19:55-INFO-evaluating batch accuracy: 0.9712; avg_accuracy: 0.9317

20-03-19 19:55-INFO-
20-03-19 19:56-INFO-Epoch 7, Batch 12, Global step 14600:
20-03-19 19:56-INFO-training batch loss: 0.1043; avg_loss: 0.0771
20-03-19 19:56-INFO-training batch accuracy: 0.9609; avg_accuracy: 0.8913
20-03-19 19:56-INFO-
20-03-19 19:57-INFO-Epoch 7, Batch 112, Global step 14700:
20-03-19 19:57-INFO-training batch loss: 0.0605; avg_loss: 0.0904
20-03-19 19:57-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9534
20-03-19 19:57-INFO-
20-03-19 19:59-INFO-Epoch 7, Batch 212, Global step 14800:
20-03-19 19:59-INFO-training batch loss: 0.1566; avg_loss: 0.0929
20-03-19 19:59-INFO-training batch accuracy: 0.9375; avg_accuracy: 0.9577
20-03-19 19:59-INFO-
20-03-19 20:01-INFO-Epoch 7, Batch 312, Global step 14900:
20-03-19 20:01-INFO-training batch loss: 0.2456; avg_loss: 0.0929
20-03-19 20:01-INFO-training batch accuracy: 0.9297; avg_accuracy: 0.9590
20-03-19 20:01-INFO-
20-03-19 20:03-INFO-Epoch 7, Batch 412, Global step 15000:
20-03-19 20:03-INFO-training batch loss: 0.0514; avg_loss: 0.0923
20-03-19 20:03-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9598
20-03-19 20:03-INFO-
20-03-19 20:06-INFO-Epoch 7, Batch 512, Global step 15100:
20-03-19 20:06-INFO-training batch loss: 0.0990; avg_loss: 0.0927
20-03-19 20:06-INFO-training batch accuracy: 0.9375; avg_accuracy: 0.9600
20-03-19 20:06-INFO-
20-03-19 20:09-INFO-Epoch 7, Batch 612, Global step 15200:
20-03-19 20:09-INFO-training batch loss: 0.0949; avg_loss: 0.0931
20-03-19 20:09-INFO-training batch accuracy: 0.9609; avg_accuracy: 0.9604
20-03-19 20:09-INFO-
20-03-19 20:11-INFO-Epoch 7, Batch 712, Global step 15300:
20-03-19 20:11-INFO-training batch loss: 0.1102; avg_loss: 0.0934
20-03-19 20:11-INFO-training batch accuracy: 0.9531; avg_accuracy: 0.9606
20-03-19 20:11-INFO-
20-03-19 20:14-INFO-Epoch 7, Batch 812, Global step 15400:
20-03-19 20:14-INFO-training batch loss: 0.0877; avg_loss: 0.0935
20-03-19 20:14-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9608
20-03-19 20:14-INFO-
20-03-19 20:17-INFO-Epoch 7, Batch 912, Global step 15500:
20-03-19 20:17-INFO-training batch loss: 0.0795; avg_loss: 0.0940
20-03-19 20:17-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9606
20-03-19 20:17-INFO-
20-03-19 20:20-INFO-Epoch 7, Batch 1012, Global step 15600:
20-03-19 20:20-INFO-training batch loss: 0.1177; avg_loss: 0.0940
20-03-19 20:20-INFO-training batch accuracy: 0.9609; avg_accuracy: 0.9607
20-03-19 20:20-INFO-
20-03-19 20:23-INFO-Epoch 7, Batch 1112, Global step 15700:
20-03-19 20:23-INFO-training batch loss: 0.1709; avg_loss: 0.0944
20-03-19 20:23-INFO-training batch accuracy: 0.9297; avg_accuracy: 0.9607
20-03-19 20:23-INFO-
20-03-19 20:26-INFO-Epoch 7, Batch 1212, Global step 15800:
20-03-19 20:26-INFO-training batch loss: 0.0369; avg_loss: 0.0938
20-03-19 20:26-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9610
20-03-19 20:26-INFO-
20-03-19 20:29-INFO-Epoch 7, Batch 1312, Global step 15900:
20-03-19 20:29-INFO-training batch loss: 0.1477; avg_loss: 0.0942
20-03-19 20:29-INFO-training batch accuracy: 0.9453; avg_accuracy: 0.9611
20-03-19 20:29-INFO-
20-03-19 20:33-INFO-Epoch 7, Batch 1412, Global step 16000:
20-03-19 20:33-INFO-training batch loss: 0.0930; avg_loss: 0.0943
20-03-19 20:33-INFO-training batch accuracy: 0.9531; avg_accuracy: 0.9612
20-03-19 20:33-INFO-
20-03-19 20:36-INFO-Epoch 7, Batch 1512, Global step 16100:
20-03-19 20:36-INFO-training batch loss: 0.1197; avg_loss: 0.0944
20-03-19 20:36-INFO-training batch accuracy: 0.9531; avg_accuracy: 0.9612
20-03-19 20:36-INFO-
20-03-19 20:39-INFO-Epoch 7, Batch 1612, Global step 16200:
20-03-19 20:39-INFO-training batch loss: 0.0968; avg_loss: 0.0940
20-03-19 20:39-INFO-training batch accuracy: 0.9375; avg_accuracy: 0.9614
20-03-19 20:39-INFO-
20-03-19 20:42-INFO-Epoch 7, Batch 1712, Global step 16300:
20-03-19 20:42-INFO-training batch loss: 0.0762; avg_loss: 0.0941
20-03-19 20:42-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9613
20-03-19 20:42-INFO-
20-03-19 20:45-INFO-Epoch 7, Batch 1812, Global step 16400:
20-03-19 20:45-INFO-training batch loss: 0.0878; avg_loss: 0.0940
20-03-19 20:45-INFO-training batch accuracy: 0.9609; avg_accuracy: 0.9612
20-03-19 20:45-INFO-
20-03-19 20:47-INFO-Epoch 7, Batch 1912, Global step 16500:
20-03-19 20:47-INFO-training batch loss: 0.1275; avg_loss: 0.0939
20-03-19 20:47-INFO-training batch accuracy: 0.9609; avg_accuracy: 0.9612
20-03-19 20:47-INFO-
20-03-19 20:50-INFO-Epoch 7, Batch 2012, Global step 16600:
20-03-19 20:50-INFO-training batch loss: 0.0992; avg_loss: 0.0936
20-03-19 20:50-INFO-training batch accuracy: 0.9609; avg_accuracy: 0.9614
20-03-19 20:50-INFO-
20-03-19 20:52-INFO-training batch accuracy: 0.9048; avg_accuracy: 0.9619
20-03-19 20:52-INFO-
20-03-19 20:55-INFO-Epoch 7, evaluating batch loss: 0.1082; avg_loss: 0.1448
20-03-19 20:55-INFO-evaluating batch accuracy: 0.9615; avg_accuracy: 0.9345

20-03-19 20:55-INFO-
20-03-19 20:56-INFO-Epoch 8, Batch 28, Global step 16700:
20-03-19 20:56-INFO-training batch loss: 0.1375; avg_loss: 0.0832
20-03-19 20:56-INFO-training batch accuracy: 0.9219; avg_accuracy: 0.9339
20-03-19 20:56-INFO-
20-03-19 20:59-INFO-Epoch 8, Batch 128, Global step 16800:
20-03-19 20:59-INFO-training batch loss: 0.1105; avg_loss: 0.0894
20-03-19 20:59-INFO-training batch accuracy: 0.9609; avg_accuracy: 0.9552
20-03-19 20:59-INFO-
20-03-19 21:02-INFO-Epoch 8, Batch 228, Global step 16900:
20-03-19 21:02-INFO-training batch loss: 0.0813; avg_loss: 0.0908
20-03-19 21:02-INFO-training batch accuracy: 0.9531; avg_accuracy: 0.9587
20-03-19 21:02-INFO-
20-03-19 21:05-INFO-Epoch 8, Batch 328, Global step 17000:
20-03-19 21:05-INFO-training batch loss: 0.0743; avg_loss: 0.0910
20-03-19 21:05-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9602
20-03-19 21:05-INFO-
20-03-19 21:08-INFO-Epoch 8, Batch 428, Global step 17100:
20-03-19 21:08-INFO-training batch loss: 0.0988; avg_loss: 0.0906
20-03-19 21:08-INFO-training batch accuracy: 0.9609; avg_accuracy: 0.9606
20-03-19 21:08-INFO-
20-03-19 21:11-INFO-Epoch 8, Batch 528, Global step 17200:
20-03-19 21:11-INFO-training batch loss: 0.0611; avg_loss: 0.0906
20-03-19 21:11-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9611
20-03-19 21:11-INFO-
20-03-19 21:14-INFO-Epoch 8, Batch 628, Global step 17300:
20-03-19 21:14-INFO-training batch loss: 0.0613; avg_loss: 0.0913
20-03-19 21:14-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9614
20-03-19 21:14-INFO-
20-03-19 21:17-INFO-Epoch 8, Batch 728, Global step 17400:
20-03-19 21:17-INFO-training batch loss: 0.0991; avg_loss: 0.0917
20-03-19 21:17-INFO-training batch accuracy: 0.9375; avg_accuracy: 0.9614
20-03-19 21:17-INFO-
20-03-19 21:20-INFO-Epoch 8, Batch 828, Global step 17500:
20-03-19 21:20-INFO-training batch loss: 0.0342; avg_loss: 0.0918
20-03-19 21:20-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9617
20-03-19 21:20-INFO-
20-03-19 21:23-INFO-Epoch 8, Batch 928, Global step 17600:
20-03-19 21:23-INFO-training batch loss: 0.1213; avg_loss: 0.0921
20-03-19 21:23-INFO-training batch accuracy: 0.9531; avg_accuracy: 0.9615
20-03-19 21:23-INFO-
20-03-19 21:26-INFO-Epoch 8, Batch 1028, Global step 17700:
20-03-19 21:26-INFO-training batch loss: 0.0752; avg_loss: 0.0921
20-03-19 21:26-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9616
20-03-19 21:26-INFO-
20-03-19 21:29-INFO-Epoch 8, Batch 1128, Global step 17800:
20-03-19 21:29-INFO-training batch loss: 0.0834; avg_loss: 0.0924
20-03-19 21:29-INFO-training batch accuracy: 0.9766; avg_accuracy: 0.9616
20-03-19 21:29-INFO-
20-03-19 21:32-INFO-Epoch 8, Batch 1228, Global step 17900:
20-03-19 21:32-INFO-training batch loss: 0.1313; avg_loss: 0.0922
20-03-19 21:32-INFO-training batch accuracy: 0.9609; avg_accuracy: 0.9619
20-03-19 21:32-INFO-
20-03-19 21:35-INFO-Epoch 8, Batch 1328, Global step 18000:
20-03-19 21:35-INFO-training batch loss: 0.0605; avg_loss: 0.0923
20-03-19 21:35-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9619
20-03-19 21:35-INFO-
20-03-19 21:38-INFO-Epoch 8, Batch 1428, Global step 18100:
20-03-19 21:38-INFO-training batch loss: 0.0823; avg_loss: 0.0923
20-03-19 21:38-INFO-training batch accuracy: 0.9609; avg_accuracy: 0.9620
20-03-19 21:38-INFO-
20-03-19 21:41-INFO-Epoch 8, Batch 1528, Global step 18200:
20-03-19 21:41-INFO-training batch loss: 0.1150; avg_loss: 0.0924
20-03-19 21:41-INFO-training batch accuracy: 0.9531; avg_accuracy: 0.9619
20-03-19 21:41-INFO-
20-03-19 21:44-INFO-Epoch 8, Batch 1628, Global step 18300:
20-03-19 21:44-INFO-training batch loss: 0.1267; avg_loss: 0.0920
20-03-19 21:44-INFO-training batch accuracy: 0.9297; avg_accuracy: 0.9621
20-03-19 21:44-INFO-
20-03-19 21:47-INFO-Epoch 8, Batch 1728, Global step 18400:
20-03-19 21:47-INFO-training batch loss: 0.1216; avg_loss: 0.0921
20-03-19 21:47-INFO-training batch accuracy: 0.9531; avg_accuracy: 0.9620
20-03-19 21:47-INFO-
20-03-19 21:50-INFO-Epoch 8, Batch 1828, Global step 18500:
20-03-19 21:50-INFO-training batch loss: 0.0973; avg_loss: 0.0921
20-03-19 21:50-INFO-training batch accuracy: 0.9453; avg_accuracy: 0.9619
20-03-19 21:50-INFO-
20-03-19 21:53-INFO-Epoch 8, Batch 1928, Global step 18600:
20-03-19 21:53-INFO-training batch loss: 0.1145; avg_loss: 0.0920
20-03-19 21:53-INFO-training batch accuracy: 0.9531; avg_accuracy: 0.9619
20-03-19 21:53-INFO-
20-03-19 21:56-INFO-Epoch 8, Batch 2028, Global step 18700:
20-03-19 21:56-INFO-training batch loss: 0.0726; avg_loss: 0.0916
20-03-19 21:56-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9621
20-03-19 21:56-INFO-
20-03-19 21:58-INFO-training batch accuracy: 0.9048; avg_accuracy: 0.9625
20-03-19 21:58-INFO-
20-03-19 22:01-INFO-Epoch 8, evaluating batch loss: 0.1214; avg_loss: 0.1449
20-03-19 22:01-INFO-evaluating batch accuracy: 0.9712; avg_accuracy: 0.9352

20-03-19 22:01-INFO-
20-03-19 22:03-INFO-Epoch 9, Batch 44, Global step 18800:
20-03-19 22:03-INFO-training batch loss: 0.1074; avg_loss: 0.0832
20-03-19 22:03-INFO-training batch accuracy: 0.9453; avg_accuracy: 0.9446
20-03-19 22:03-INFO-
20-03-19 22:06-INFO-Epoch 9, Batch 144, Global step 18900:
20-03-19 22:06-INFO-training batch loss: 0.0607; avg_loss: 0.0887
20-03-19 22:06-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9558
20-03-19 22:06-INFO-
20-03-19 22:09-INFO-Epoch 9, Batch 244, Global step 19000:
20-03-19 22:09-INFO-training batch loss: 0.1266; avg_loss: 0.0886
20-03-19 22:09-INFO-training batch accuracy: 0.9375; avg_accuracy: 0.9598
20-03-19 22:09-INFO-
20-03-19 22:12-INFO-Epoch 9, Batch 344, Global step 19100:
20-03-19 22:12-INFO-training batch loss: 0.0878; avg_loss: 0.0886
20-03-19 22:12-INFO-training batch accuracy: 0.9609; avg_accuracy: 0.9611
20-03-19 22:12-INFO-
20-03-19 22:15-INFO-Epoch 9, Batch 444, Global step 19200:
20-03-19 22:15-INFO-training batch loss: 0.1182; avg_loss: 0.0888
20-03-19 22:15-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9613
20-03-19 22:15-INFO-
20-03-19 22:19-INFO-Epoch 9, Batch 544, Global step 19300:
20-03-19 22:19-INFO-training batch loss: 0.0866; avg_loss: 0.0887
20-03-19 22:19-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9618
20-03-19 22:19-INFO-
20-03-19 22:22-INFO-Epoch 9, Batch 644, Global step 19400:
20-03-19 22:22-INFO-training batch loss: 0.0476; avg_loss: 0.0896
20-03-19 22:22-INFO-training batch accuracy: 0.9922; avg_accuracy: 0.9619
20-03-19 22:22-INFO-
20-03-19 22:25-INFO-Epoch 9, Batch 744, Global step 19500:
20-03-19 22:25-INFO-training batch loss: 0.1250; avg_loss: 0.0896
20-03-19 22:25-INFO-training batch accuracy: 0.9219; avg_accuracy: 0.9621
20-03-19 22:25-INFO-
20-03-19 22:28-INFO-Epoch 9, Batch 844, Global step 19600:
20-03-19 22:28-INFO-training batch loss: 0.1022; avg_loss: 0.0898
20-03-19 22:28-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9622
20-03-19 22:28-INFO-
20-03-19 22:31-INFO-Epoch 9, Batch 944, Global step 19700:
20-03-19 22:31-INFO-training batch loss: 0.1108; avg_loss: 0.0900
20-03-19 22:31-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9621
20-03-19 22:31-INFO-
20-03-19 22:34-INFO-Epoch 9, Batch 1044, Global step 19800:
20-03-19 22:34-INFO-training batch loss: 0.1569; avg_loss: 0.0905
20-03-19 22:34-INFO-training batch accuracy: 0.9609; avg_accuracy: 0.9621
20-03-19 22:34-INFO-
20-03-19 22:37-INFO-Epoch 9, Batch 1144, Global step 19900:
20-03-19 22:37-INFO-training batch loss: 0.1402; avg_loss: 0.0905
20-03-19 22:37-INFO-training batch accuracy: 0.9297; avg_accuracy: 0.9623
20-03-19 22:37-INFO-
20-03-19 22:40-INFO-Epoch 9, Batch 1244, Global step 20000:
20-03-19 22:40-INFO-training batch loss: 0.0661; avg_loss: 0.0902
20-03-19 22:40-INFO-training batch accuracy: 0.9688; avg_accuracy: 0.9625
20-03-19 22:40-INFO-
20-03-19 22:44-INFO-Epoch 9, Batch 1344, Global step 20100:
20-03-19 22:44-INFO-training batch loss: 0.0951; avg_loss: 0.0904
20-03-19 22:44-INFO-training batch accuracy: 0.9609; avg_accuracy: 0.9625
20-03-19 22:44-INFO-
20-03-19 22:47-INFO-Epoch 9, Batch 1444, Global step 20200:
20-03-19 22:47-INFO-training batch loss: 0.0886; avg_loss: 0.0904
20-03-19 22:47-INFO-training batch accuracy: 0.9609; avg_accuracy: 0.9626
20-03-19 22:47-INFO-
20-03-19 22:50-INFO-Epoch 9, Batch 1544, Global step 20300:
20-03-19 22:50-INFO-training batch loss: 0.0372; avg_loss: 0.0906
20-03-19 22:50-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9625
20-03-19 22:50-INFO-
20-03-19 22:53-INFO-Epoch 9, Batch 1644, Global step 20400:
20-03-19 22:53-INFO-training batch loss: 0.0862; avg_loss: 0.0902
20-03-19 22:53-INFO-training batch accuracy: 0.9844; avg_accuracy: 0.9626
20-03-19 22:53-INFO-
20-03-19 22:56-INFO-Epoch 9, Batch 1744, Global step 20500:
20-03-19 22:56-INFO-training batch loss: 0.1785; avg_loss: 0.0903
20-03-19 22:56-INFO-training batch accuracy: 0.9375; avg_accuracy: 0.9625
20-03-19 22:56-INFO-
20-03-19 22:59-INFO-Epoch 9, Batch 1844, Global step 20600:
20-03-19 22:59-INFO-training batch loss: 0.1042; avg_loss: 0.0901
20-03-19 22:59-INFO-training batch accuracy: 0.9531; avg_accuracy: 0.9625
20-03-19 22:59-INFO-
20-03-19 23:02-INFO-Epoch 9, Batch 1944, Global step 20700:
20-03-19 23:02-INFO-training batch loss: 0.0813; avg_loss: 0.0900
20-03-19 23:02-INFO-training batch accuracy: 0.9531; avg_accuracy: 0.9626
20-03-19 23:02-INFO-
20-03-19 23:06-INFO-Epoch 9, Batch 2044, Global step 20800:
20-03-19 23:06-INFO-training batch loss: 0.1170; avg_loss: 0.0897
20-03-19 23:06-INFO-training batch accuracy: 0.9375; avg_accuracy: 0.9628
20-03-19 23:06-INFO-
20-03-19 23:07-INFO-training batch accuracy: 0.9048; avg_accuracy: 0.9632
20-03-19 23:07-INFO-
20-03-19 23:10-INFO-Epoch 9, evaluating batch loss: 0.1031; avg_loss: 0.1450
20-03-19 23:10-INFO-evaluating batch accuracy: 0.9615; avg_accuracy: 0.9349

20-03-19 23:10-INFO-
