20-03-23 16:48-INFO-{'session_length': 24, 'height': 32, 'width': 32, 'num_labels': 8, 'learning_rate': 0.005, 'filter_sizes': [3, 4, 5, 6], 'num_filters': 64, 'filter_sizes_hierarchical': [3, 4, 5], 'num_fitlers_hierarchical': 64, 'is_train': True, 'early_stop': False, 'is_tuning': False}
20-03-23 16:48-WARNING-From ../utils.py:129: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

20-03-23 16:48-WARNING-From ../model/train.py:105: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.

20-03-23 16:48-WARNING-From ../model/siamese_network.py:30: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

20-03-23 16:48-WARNING-From ../model/siamese_network.py:69: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

20-03-23 16:48-WARNING-From ../model/siamese_network.py:69: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.

20-03-23 16:48-WARNING-From ../model/utils/utils.py:26: conv1d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv1D` instead.
20-03-23 16:48-WARNING-Entity <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7f07db45dcd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7f07db45dcd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-23 16:48-WARNING-From ../model/utils/utils.py:45: max_pooling1d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.MaxPooling1D instead.
20-03-23 16:48-WARNING-Entity <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7f07db45dc90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7f07db45dc90>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-23 16:48-WARNING-Entity <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7f07db45d790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7f07db45d790>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-23 16:48-WARNING-Entity <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7f07dabb5910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7f07dabb5910>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-23 16:48-WARNING-Entity <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7f07dabb5fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7f07dabb5fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-23 16:48-WARNING-Entity <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7f07dabb52d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7f07dabb52d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-23 16:48-WARNING-Entity <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7f07dab8add0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7f07dab8add0>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-23 16:48-WARNING-Entity <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7f07dabb50d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7f07dabb50d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-23 16:48-WARNING-From ../model/utils/modules.py:205: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.dropout instead.
20-03-23 16:48-WARNING-Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7f07dbe3f550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7f07dbe3f550>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-23 16:48-WARNING-Variable *= will be deprecated. Use `var.assign(var * other)` if you want assignment to the variable value or `x = x * y` if you want a new python Tensor object.
20-03-23 16:48-WARNING-Entity <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7f07dab25610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7f07dab25610>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-23 16:48-WARNING-Entity <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7f07dab25610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7f07dab25610>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-23 16:48-WARNING-Entity <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7f07dab8a610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7f07dab8a610>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-23 16:48-WARNING-Entity <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7f07daae8fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7f07daae8fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-23 16:48-WARNING-Entity <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7f07daba31d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7f07daba31d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-23 16:48-WARNING-Entity <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7f07daba3f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7f07daba3f10>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-23 16:48-WARNING-From ../model/utils/modules.py:240: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.dense instead.
20-03-23 16:48-WARNING-Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f07dab4e350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f07dab4e350>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-23 16:48-WARNING-From ../model/utils/modules.py:242: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).
20-03-23 16:48-WARNING-Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f07daae8b90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f07daae8b90>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-23 16:48-WARNING-From ../model/utils/modules.py:245: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
20-03-23 16:48-WARNING-Entity <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7f07daa78dd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7f07daa78dd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-23 16:48-WARNING-Entity <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7f07dab2af50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7f07dab2af50>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-23 16:48-WARNING-Entity <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7f07daa78c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7f07daa78c50>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-23 16:48-WARNING-Entity <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7f07daacc190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7f07daacc190>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-23 16:48-WARNING-Entity <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7f07da9ba510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7f07da9ba510>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-23 16:48-WARNING-Entity <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7f07db47d590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7f07db47d590>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-23 16:48-WARNING-Entity <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7f07da9ba910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7f07da9ba910>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-23 16:48-WARNING-Entity <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7f07da9baa50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7f07da9baa50>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-23 16:48-WARNING-Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7f07dac25f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7f07dac25f90>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-23 16:48-WARNING-Entity <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7f07da9ff190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7f07da9ff190>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-23 16:48-WARNING-Entity <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7f07da9f3c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7f07da9f3c90>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-23 16:48-WARNING-Entity <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7f07dabb0790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7f07dabb0790>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-23 16:48-WARNING-Entity <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7f07da9b4cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7f07da9b4cd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-23 16:48-WARNING-Entity <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7f07daa41c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7f07daa41c50>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-23 16:48-WARNING-Entity <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7f07daa64d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7f07daa64d90>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-23 16:48-WARNING-Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f07dac25f50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f07dac25f50>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-23 16:48-WARNING-Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f07dac25e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f07dac25e90>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-23 16:48-WARNING-Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f07da8d0910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f07da8d0910>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-23 16:48-WARNING-Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f07da9bb350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f07da9bb350>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-23 16:48-WARNING-From ../model/base_model.py:132: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.

20-03-23 16:48-WARNING-From /root/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/clip_ops.py:286: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
20-03-23 16:48-WARNING-From ../model/train.py:113: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.

20-03-23 16:48-INFO-Epoch 0, Batch 1, Global step 1:
20-03-23 16:48-INFO-training batch loss: 1.5559; avg_loss: 1.5559
20-03-23 16:48-INFO-training batch acc: 0.5938; avg_acc: 0.5938
20-03-23 16:48-INFO-
20-03-23 16:48-INFO-Epoch 0, Batch 2, Global step 2:
20-03-23 16:48-INFO-training batch loss: 40.7901; avg_loss: 21.1730
20-03-23 16:48-INFO-training batch acc: 0.4062; avg_acc: 0.5000
20-03-23 16:48-INFO-
20-03-23 16:48-INFO-Epoch 0, Batch 3, Global step 3:
20-03-23 16:48-INFO-training batch loss: 21.2524; avg_loss: 21.1995
20-03-23 16:48-INFO-training batch acc: 0.4453; avg_acc: 0.4818
20-03-23 16:48-INFO-
20-03-23 16:48-INFO-Epoch 0, Batch 4, Global step 4:
20-03-23 16:48-INFO-training batch loss: 2.8148; avg_loss: 16.6033
20-03-23 16:48-INFO-training batch acc: 0.3672; avg_acc: 0.4531
20-03-23 16:48-INFO-
20-03-23 16:48-INFO-Epoch 0, Batch 5, Global step 5:
20-03-23 16:48-INFO-training batch loss: 6.8524; avg_loss: 14.6531
20-03-23 16:48-INFO-training batch acc: 0.5781; avg_acc: 0.4781
20-03-23 16:48-INFO-
20-03-23 16:48-INFO-Epoch 0, Batch 6, Global step 6:
20-03-23 16:48-INFO-training batch loss: 3.7485; avg_loss: 12.8357
20-03-23 16:48-INFO-training batch acc: 0.6641; avg_acc: 0.5091
20-03-23 16:48-INFO-
20-03-23 16:48-INFO-Epoch 0, Batch 7, Global step 7:
20-03-23 16:48-INFO-training batch loss: 1.2362; avg_loss: 11.1786
20-03-23 16:48-INFO-training batch acc: 0.6406; avg_acc: 0.5279
20-03-23 16:48-INFO-
20-03-23 16:48-INFO-Epoch 0, Batch 8, Global step 8:
20-03-23 16:48-INFO-training batch loss: 2.5757; avg_loss: 10.1033
20-03-23 16:48-INFO-training batch acc: 0.4531; avg_acc: 0.5186
20-03-23 16:48-INFO-
20-03-23 16:48-INFO-Epoch 0, Batch 9, Global step 9:
20-03-23 16:48-INFO-training batch loss: 3.1861; avg_loss: 9.3347
20-03-23 16:48-INFO-training batch acc: 0.3359; avg_acc: 0.4983
20-03-23 16:48-INFO-
20-03-23 16:48-INFO-Epoch 0, Batch 10, Global step 10:
20-03-23 16:48-INFO-training batch loss: 0.9582; avg_loss: 8.4970
20-03-23 16:48-INFO-training batch acc: 0.5156; avg_acc: 0.5000
20-03-23 16:48-INFO-
20-03-23 16:48-INFO-Epoch 0, Batch 11, Global step 11:
20-03-23 16:48-INFO-training batch loss: 1.0119; avg_loss: 7.8166
20-03-23 16:48-INFO-training batch acc: 0.6641; avg_acc: 0.5149
20-03-23 16:48-INFO-
20-03-23 16:49-INFO-Epoch 0, Batch 12, Global step 12:
20-03-23 16:49-INFO-training batch loss: 1.1985; avg_loss: 7.2651
20-03-23 16:49-INFO-training batch acc: 0.6094; avg_acc: 0.5228
20-03-23 16:49-INFO-
20-03-23 16:49-INFO-Epoch 0, Batch 13, Global step 13:
20-03-23 16:49-INFO-training batch loss: 0.8023; avg_loss: 6.7679
20-03-23 16:49-INFO-training batch acc: 0.6094; avg_acc: 0.5294
20-03-23 16:49-INFO-
20-03-23 16:49-INFO-Epoch 0, Batch 14, Global step 14:
20-03-23 16:49-INFO-training batch loss: 0.9131; avg_loss: 6.3497
20-03-23 16:49-INFO-training batch acc: 0.5156; avg_acc: 0.5285
20-03-23 16:49-INFO-
20-03-23 16:49-INFO-Epoch 0, Batch 15, Global step 15:
20-03-23 16:49-INFO-training batch loss: 0.9712; avg_loss: 5.9912
20-03-23 16:49-INFO-training batch acc: 0.4453; avg_acc: 0.5229
20-03-23 16:49-INFO-
20-03-23 16:49-INFO-Epoch 0, Batch 16, Global step 16:
20-03-23 16:49-INFO-training batch loss: 0.7893; avg_loss: 5.6660
20-03-23 16:49-INFO-training batch acc: 0.4922; avg_acc: 0.5210
20-03-23 16:49-INFO-
20-03-23 16:49-INFO-Epoch 0, Batch 17, Global step 17:
20-03-23 16:49-INFO-training batch loss: 0.7196; avg_loss: 5.3751
20-03-23 16:49-INFO-training batch acc: 0.5469; avg_acc: 0.5225
20-03-23 16:49-INFO-
20-03-23 16:49-INFO-Epoch 0, Batch 18, Global step 18:
20-03-23 16:49-INFO-training batch loss: 0.7731; avg_loss: 5.1194
20-03-23 16:49-INFO-training batch acc: 0.5938; avg_acc: 0.5265
20-03-23 16:49-INFO-
20-03-23 16:49-INFO-Epoch 0, Batch 19, Global step 19:
20-03-23 16:49-INFO-training batch loss: 0.7930; avg_loss: 4.8917
20-03-23 16:49-INFO-training batch acc: 0.5391; avg_acc: 0.5271
20-03-23 16:49-INFO-
20-03-23 16:49-INFO-Epoch 0, Batch 20, Global step 20:
20-03-23 16:49-INFO-training batch loss: 0.6284; avg_loss: 4.6785
20-03-23 16:49-INFO-training batch acc: 0.6406; avg_acc: 0.5328
20-03-23 16:49-INFO-
20-03-23 16:49-INFO-Epoch 0, Batch 21, Global step 21:
20-03-23 16:49-INFO-training batch loss: 0.6812; avg_loss: 4.4882
20-03-23 16:49-INFO-training batch acc: 0.6094; avg_acc: 0.5365
20-03-23 16:49-INFO-
20-03-23 16:49-INFO-Epoch 0, Batch 22, Global step 22:
20-03-23 16:49-INFO-training batch loss: 0.6951; avg_loss: 4.3158
20-03-23 16:49-INFO-training batch acc: 0.5391; avg_acc: 0.5366
20-03-23 16:49-INFO-
20-03-23 16:49-INFO-Epoch 0, Batch 23, Global step 23:
20-03-23 16:49-INFO-training batch loss: 0.6934; avg_loss: 4.1583
20-03-23 16:49-INFO-training batch acc: 0.5859; avg_acc: 0.5387
20-03-23 16:49-INFO-
20-03-23 16:49-INFO-Epoch 0, Batch 24, Global step 24:
20-03-23 16:49-INFO-training batch loss: 0.6677; avg_loss: 4.0128
20-03-23 16:49-INFO-training batch acc: 0.6250; avg_acc: 0.5423
20-03-23 16:49-INFO-
20-03-23 16:49-INFO-Epoch 0, Batch 25, Global step 25:
20-03-23 16:49-INFO-training batch loss: 0.6607; avg_loss: 3.8788
20-03-23 16:49-INFO-training batch acc: 0.6094; avg_acc: 0.5450
20-03-23 16:49-INFO-
20-03-23 16:49-INFO-Epoch 0, Batch 26, Global step 26:
20-03-23 16:49-INFO-training batch loss: 0.6567; avg_loss: 3.7548
20-03-23 16:49-INFO-training batch acc: 0.6484; avg_acc: 0.5490
20-03-23 16:49-INFO-
20-03-23 16:49-INFO-Epoch 0, Batch 27, Global step 27:
20-03-23 16:49-INFO-training batch loss: 0.7134; avg_loss: 3.6422
20-03-23 16:49-INFO-training batch acc: 0.5625; avg_acc: 0.5495
20-03-23 16:49-INFO-
20-03-23 16:49-INFO-Epoch 0, Batch 28, Global step 28:
20-03-23 16:49-INFO-training batch loss: 0.6784; avg_loss: 3.5363
20-03-23 16:49-INFO-training batch acc: 0.5625; avg_acc: 0.5499
20-03-23 16:49-INFO-
20-03-23 16:49-INFO-Epoch 0, Batch 29, Global step 29:
20-03-23 16:49-INFO-training batch loss: 0.7318; avg_loss: 3.4396
20-03-23 16:49-INFO-training batch acc: 0.5547; avg_acc: 0.5501
20-03-23 16:49-INFO-
20-03-23 16:49-INFO-Epoch 0, Batch 30, Global step 30:
20-03-23 16:49-INFO-training batch loss: 0.6984; avg_loss: 3.3483
20-03-23 16:49-INFO-training batch acc: 0.5312; avg_acc: 0.5495
20-03-23 16:49-INFO-
20-03-23 16:49-INFO-Epoch 0, Batch 31, Global step 31:
20-03-23 16:49-INFO-training batch loss: 0.6620; avg_loss: 3.2616
20-03-23 16:49-INFO-training batch acc: 0.6250; avg_acc: 0.5519
20-03-23 16:49-INFO-
20-03-23 16:49-INFO-Epoch 0, Batch 32, Global step 32:
20-03-23 16:49-INFO-training batch loss: 0.6872; avg_loss: 3.1812
20-03-23 16:49-INFO-training batch acc: 0.5703; avg_acc: 0.5525
20-03-23 16:49-INFO-
20-03-23 16:49-INFO-Epoch 0, Batch 33, Global step 33:
20-03-23 16:49-INFO-training batch loss: 0.6780; avg_loss: 3.1053
20-03-23 16:49-INFO-training batch acc: 0.6016; avg_acc: 0.5540
20-03-23 16:49-INFO-
20-03-23 16:49-INFO-Epoch 0, Batch 34, Global step 34:
20-03-23 16:49-INFO-training batch loss: 0.6937; avg_loss: 3.0344
20-03-23 16:49-INFO-training batch acc: 0.5625; avg_acc: 0.5542
20-03-23 16:49-INFO-
20-03-23 16:49-INFO-Epoch 0, Batch 35, Global step 35:
20-03-23 16:49-INFO-training batch loss: 0.6676; avg_loss: 2.9667
20-03-23 16:49-INFO-training batch acc: 0.6094; avg_acc: 0.5558
20-03-23 16:49-INFO-
20-03-23 16:49-INFO-Epoch 0, Batch 36, Global step 36:
20-03-23 16:49-INFO-training batch loss: 0.6865; avg_loss: 2.9034
20-03-23 16:49-INFO-training batch acc: 0.5781; avg_acc: 0.5564
20-03-23 16:49-INFO-
20-03-23 16:49-INFO-Epoch 0, Batch 37, Global step 37:
20-03-23 16:49-INFO-training batch loss: 0.6603; avg_loss: 2.8428
20-03-23 16:49-INFO-training batch acc: 0.5938; avg_acc: 0.5574
20-03-23 16:49-INFO-
20-03-23 16:50-INFO-Epoch 0, Batch 38, Global step 38:
20-03-23 16:50-INFO-training batch loss: 0.6482; avg_loss: 2.7850
20-03-23 16:50-INFO-training batch acc: 0.6641; avg_acc: 0.5602
20-03-23 16:50-INFO-
20-03-23 16:50-INFO-Epoch 0, Batch 39, Global step 39:
20-03-23 16:50-INFO-training batch loss: 0.6364; avg_loss: 2.7299
20-03-23 16:50-INFO-training batch acc: 0.5781; avg_acc: 0.5607
20-03-23 16:50-INFO-
20-03-23 16:50-INFO-Epoch 0, Batch 40, Global step 40:
20-03-23 16:50-INFO-training batch loss: 0.7041; avg_loss: 2.6793
20-03-23 16:50-INFO-training batch acc: 0.5547; avg_acc: 0.5605
20-03-23 16:50-INFO-
20-03-23 16:50-INFO-Epoch 0, Batch 41, Global step 41:
20-03-23 16:50-INFO-training batch loss: 0.6669; avg_loss: 2.6302
20-03-23 16:50-INFO-training batch acc: 0.5703; avg_acc: 0.5608
20-03-23 16:50-INFO-
20-03-23 16:50-INFO-Epoch 0, Batch 42, Global step 42:
20-03-23 16:50-INFO-training batch loss: 0.6618; avg_loss: 2.5833
20-03-23 16:50-INFO-training batch acc: 0.6016; avg_acc: 0.5618
20-03-23 16:50-INFO-
20-03-23 16:50-INFO-Epoch 0, Batch 43, Global step 43:
20-03-23 16:50-INFO-training batch loss: 0.6741; avg_loss: 2.5389
20-03-23 16:50-INFO-training batch acc: 0.6016; avg_acc: 0.5627
20-03-23 16:50-INFO-
20-03-23 16:50-INFO-Epoch 0, Batch 44, Global step 44:
20-03-23 16:50-INFO-training batch loss: 0.6564; avg_loss: 2.4962
20-03-23 16:50-INFO-training batch acc: 0.6016; avg_acc: 0.5636
20-03-23 16:50-INFO-
20-03-23 16:50-INFO-Epoch 0, Batch 45, Global step 45:
20-03-23 16:50-INFO-training batch loss: 0.6532; avg_loss: 2.4552
20-03-23 16:50-INFO-training batch acc: 0.6562; avg_acc: 0.5656
20-03-23 16:50-INFO-
20-03-23 16:50-INFO-Epoch 0, Batch 46, Global step 46:
20-03-23 16:50-INFO-training batch loss: 0.6544; avg_loss: 2.4160
20-03-23 16:50-INFO-training batch acc: 0.5703; avg_acc: 0.5657
20-03-23 16:50-INFO-
20-03-23 16:50-INFO-Epoch 0, Batch 47, Global step 47:
20-03-23 16:50-INFO-training batch loss: 0.6485; avg_loss: 2.3784
20-03-23 16:50-INFO-training batch acc: 0.6094; avg_acc: 0.5667
20-03-23 16:50-INFO-
20-03-23 16:50-INFO-Epoch 0, Batch 48, Global step 48:
20-03-23 16:50-INFO-training batch loss: 0.6231; avg_loss: 2.3419
20-03-23 16:50-INFO-training batch acc: 0.6953; avg_acc: 0.5693
20-03-23 16:50-INFO-
20-03-23 16:50-INFO-Epoch 0, Batch 49, Global step 49:
20-03-23 16:50-INFO-training batch loss: 0.6233; avg_loss: 2.3068
20-03-23 16:50-INFO-training batch acc: 0.6406; avg_acc: 0.5708
20-03-23 16:50-INFO-
20-03-23 16:50-INFO-Epoch 0, Batch 50, Global step 50:
20-03-23 16:50-INFO-training batch loss: 0.6626; avg_loss: 2.2739
20-03-23 16:50-INFO-training batch acc: 0.5625; avg_acc: 0.5706
20-03-23 16:50-INFO-
20-03-23 16:50-INFO-Epoch 0, Batch 51, Global step 51:
20-03-23 16:50-INFO-training batch loss: 0.6931; avg_loss: 2.2429
20-03-23 16:50-INFO-training batch acc: 0.5781; avg_acc: 0.5708
20-03-23 16:50-INFO-
20-03-23 16:50-INFO-Epoch 0, Batch 52, Global step 52:
20-03-23 16:50-INFO-training batch loss: 0.6578; avg_loss: 2.2124
20-03-23 16:50-INFO-training batch acc: 0.5938; avg_acc: 0.5712
20-03-23 16:50-INFO-
20-03-23 16:50-INFO-Epoch 0, Batch 53, Global step 53:
20-03-23 16:50-INFO-training batch loss: 0.6762; avg_loss: 2.1835
20-03-23 16:50-INFO-training batch acc: 0.5703; avg_acc: 0.5712
20-03-23 16:50-INFO-
20-03-23 16:50-INFO-Epoch 0, Batch 54, Global step 54:
20-03-23 16:50-INFO-training batch loss: 0.6455; avg_loss: 2.1550
20-03-23 16:50-INFO-training batch acc: 0.6641; avg_acc: 0.5729
20-03-23 16:50-INFO-
20-03-23 16:50-INFO-Epoch 0, Batch 55, Global step 55:
20-03-23 16:50-INFO-training batch loss: 0.6334; avg_loss: 2.1273
20-03-23 16:50-INFO-training batch acc: 0.6953; avg_acc: 0.5751
20-03-23 16:50-INFO-
20-03-23 16:50-INFO-Epoch 0, Batch 56, Global step 56:
20-03-23 16:50-INFO-training batch loss: 0.6245; avg_loss: 2.1005
20-03-23 16:50-INFO-training batch acc: 0.6641; avg_acc: 0.5767
20-03-23 16:50-INFO-
20-03-23 16:50-INFO-Epoch 0, Batch 57, Global step 57:
20-03-23 16:50-INFO-training batch loss: 0.6483; avg_loss: 2.0750
20-03-23 16:50-INFO-training batch acc: 0.6484; avg_acc: 0.5780
20-03-23 16:50-INFO-
20-03-23 16:50-INFO-Epoch 0, Batch 58, Global step 58:
20-03-23 16:50-INFO-training batch loss: 0.6225; avg_loss: 2.0499
20-03-23 16:50-INFO-training batch acc: 0.6250; avg_acc: 0.5788
20-03-23 16:50-INFO-
20-03-23 16:50-INFO-Epoch 0, Batch 59, Global step 59:
20-03-23 16:50-INFO-training batch loss: 0.6353; avg_loss: 2.0260
20-03-23 16:50-INFO-training batch acc: 0.6484; avg_acc: 0.5800
20-03-23 16:50-INFO-
20-03-23 16:50-INFO-Epoch 0, Batch 60, Global step 60:
20-03-23 16:50-INFO-training batch loss: 0.6365; avg_loss: 2.0028
20-03-23 16:50-INFO-training batch acc: 0.5547; avg_acc: 0.5796
20-03-23 16:50-INFO-
20-03-23 16:50-INFO-Epoch 0, Batch 61, Global step 61:
20-03-23 16:50-INFO-training batch loss: 0.6381; avg_loss: 1.9804
20-03-23 16:50-INFO-training batch acc: 0.7031; avg_acc: 0.5816
20-03-23 16:50-INFO-
20-03-23 16:50-INFO-Epoch 0, Batch 62, Global step 62:
20-03-23 16:50-INFO-training batch loss: 0.7298; avg_loss: 1.9603
20-03-23 16:50-INFO-training batch acc: 0.5781; avg_acc: 0.5815
20-03-23 16:50-INFO-
20-03-23 16:51-INFO-Epoch 0, Batch 63, Global step 63:
20-03-23 16:51-INFO-training batch loss: 0.6318; avg_loss: 1.9392
20-03-23 16:51-INFO-training batch acc: 0.6484; avg_acc: 0.5826
20-03-23 16:51-INFO-
20-03-23 16:51-INFO-Epoch 0, Batch 64, Global step 64:
20-03-23 16:51-INFO-training batch loss: 0.6593; avg_loss: 1.9192
20-03-23 16:51-INFO-training batch acc: 0.6484; avg_acc: 0.5836
20-03-23 16:51-INFO-
20-03-23 16:51-INFO-Epoch 0, Batch 65, Global step 65:
20-03-23 16:51-INFO-training batch loss: 0.6517; avg_loss: 1.8997
20-03-23 16:51-INFO-training batch acc: 0.6797; avg_acc: 0.5851
20-03-23 16:51-INFO-
20-03-23 16:51-INFO-Epoch 0, Batch 66, Global step 66:
20-03-23 16:51-INFO-training batch loss: 0.6351; avg_loss: 1.8805
20-03-23 16:51-INFO-training batch acc: 0.7344; avg_acc: 0.5874
20-03-23 16:51-INFO-
20-03-23 16:51-INFO-Epoch 0, Batch 67, Global step 67:
20-03-23 16:51-INFO-training batch loss: 0.6451; avg_loss: 1.8621
20-03-23 16:51-INFO-training batch acc: 0.7109; avg_acc: 0.5892
20-03-23 16:51-INFO-
20-03-23 16:51-INFO-Epoch 0, Batch 68, Global step 68:
20-03-23 16:51-INFO-training batch loss: 0.6286; avg_loss: 1.8439
20-03-23 16:51-INFO-training batch acc: 0.7109; avg_acc: 0.5910
20-03-23 16:51-INFO-
20-03-23 16:51-INFO-Epoch 0, Batch 69, Global step 69:
20-03-23 16:51-INFO-training batch loss: 0.6021; avg_loss: 1.8259
20-03-23 16:51-INFO-training batch acc: 0.7188; avg_acc: 0.5928
20-03-23 16:51-INFO-
20-03-23 16:51-INFO-Epoch 0, Batch 70, Global step 70:
20-03-23 16:51-INFO-training batch loss: 0.6107; avg_loss: 1.8086
20-03-23 16:51-INFO-training batch acc: 0.7266; avg_acc: 0.5948
20-03-23 16:51-INFO-
20-03-23 16:51-INFO-Epoch 0, Batch 71, Global step 71:
20-03-23 16:51-INFO-training batch loss: 0.7014; avg_loss: 1.7930
20-03-23 16:51-INFO-training batch acc: 0.6719; avg_acc: 0.5958
20-03-23 16:51-INFO-
20-03-23 16:51-INFO-Epoch 0, Batch 72, Global step 72:
20-03-23 16:51-INFO-training batch loss: 0.6158; avg_loss: 1.7766
20-03-23 16:51-INFO-training batch acc: 0.6797; avg_acc: 0.5970
20-03-23 16:51-INFO-
20-03-23 16:51-INFO-Epoch 0, Batch 73, Global step 73:
20-03-23 16:51-INFO-training batch loss: 0.6356; avg_loss: 1.7610
20-03-23 16:51-INFO-training batch acc: 0.6719; avg_acc: 0.5980
20-03-23 16:51-INFO-
20-03-23 16:51-INFO-Epoch 0, Batch 74, Global step 74:
20-03-23 16:51-INFO-training batch loss: 0.6380; avg_loss: 1.7458
20-03-23 16:51-INFO-training batch acc: 0.6641; avg_acc: 0.5989
20-03-23 16:51-INFO-
20-03-23 16:51-INFO-Epoch 0, Batch 75, Global step 75:
20-03-23 16:51-INFO-training batch loss: 0.6193; avg_loss: 1.7308
20-03-23 16:51-INFO-training batch acc: 0.6641; avg_acc: 0.5998
20-03-23 16:51-INFO-
20-03-23 16:51-INFO-Epoch 0, Batch 76, Global step 76:
20-03-23 16:51-INFO-training batch loss: 0.5696; avg_loss: 1.7155
20-03-23 16:51-INFO-training batch acc: 0.7734; avg_acc: 0.6021
20-03-23 16:51-INFO-
20-03-23 16:51-INFO-Epoch 0, Batch 77, Global step 77:
20-03-23 16:51-INFO-training batch loss: 0.6059; avg_loss: 1.7011
20-03-23 16:51-INFO-training batch acc: 0.6484; avg_acc: 0.6027
20-03-23 16:51-INFO-
20-03-23 16:51-INFO-Epoch 0, Batch 78, Global step 78:
20-03-23 16:51-INFO-training batch loss: 0.5698; avg_loss: 1.6866
20-03-23 16:51-INFO-training batch acc: 0.6719; avg_acc: 0.6036
20-03-23 16:51-INFO-
20-03-23 16:51-INFO-Epoch 0, Batch 79, Global step 79:
20-03-23 16:51-INFO-training batch loss: 0.5788; avg_loss: 1.6726
20-03-23 16:51-INFO-training batch acc: 0.7109; avg_acc: 0.6049
20-03-23 16:51-INFO-
20-03-23 16:51-INFO-Epoch 0, Batch 80, Global step 80:
20-03-23 16:51-INFO-training batch loss: 0.5435; avg_loss: 1.6585
20-03-23 16:51-INFO-training batch acc: 0.7578; avg_acc: 0.6068
20-03-23 16:51-INFO-
20-03-23 16:51-INFO-Epoch 0, Batch 81, Global step 81:
20-03-23 16:51-INFO-training batch loss: 0.4856; avg_loss: 1.6440
20-03-23 16:51-INFO-training batch acc: 0.7734; avg_acc: 0.6089
20-03-23 16:51-INFO-
20-03-23 16:51-INFO-Epoch 0, Batch 82, Global step 82:
20-03-23 16:51-INFO-training batch loss: 0.5472; avg_loss: 1.6306
20-03-23 16:51-INFO-training batch acc: 0.7344; avg_acc: 0.6104
20-03-23 16:51-INFO-
20-03-23 16:51-INFO-Epoch 0, Batch 83, Global step 83:
20-03-23 16:51-INFO-training batch loss: 0.5654; avg_loss: 1.6178
20-03-23 16:51-INFO-training batch acc: 0.7109; avg_acc: 0.6116
20-03-23 16:51-INFO-
20-03-23 16:51-INFO-Epoch 0, Batch 84, Global step 84:
20-03-23 16:51-INFO-training batch loss: 0.4671; avg_loss: 1.6041
20-03-23 16:51-INFO-training batch acc: 0.8281; avg_acc: 0.6142
20-03-23 16:51-INFO-
20-03-23 16:51-INFO-Epoch 0, Batch 85, Global step 85:
20-03-23 16:51-INFO-training batch loss: 0.4902; avg_loss: 1.5910
20-03-23 16:51-INFO-training batch acc: 0.7656; avg_acc: 0.6160
20-03-23 16:51-INFO-
20-03-23 16:51-INFO-Epoch 0, Batch 86, Global step 86:
20-03-23 16:51-INFO-training batch loss: 0.4692; avg_loss: 1.5779
20-03-23 16:51-INFO-training batch acc: 0.7656; avg_acc: 0.6177
20-03-23 16:51-INFO-
20-03-23 16:51-INFO-Epoch 0, Batch 87, Global step 87:
20-03-23 16:51-INFO-training batch loss: 0.4758; avg_loss: 1.5653
20-03-23 16:51-INFO-training batch acc: 0.7891; avg_acc: 0.6197
20-03-23 16:51-INFO-
20-03-23 16:51-INFO-Epoch 0, Batch 88, Global step 88:
20-03-23 16:51-INFO-training batch loss: 0.4358; avg_loss: 1.5524
20-03-23 16:51-INFO-training batch acc: 0.7891; avg_acc: 0.6216
20-03-23 16:51-INFO-
20-03-23 16:52-INFO-Epoch 0, Batch 89, Global step 89:
20-03-23 16:52-INFO-training batch loss: 0.4020; avg_loss: 1.5395
20-03-23 16:52-INFO-training batch acc: 0.8359; avg_acc: 0.6240
20-03-23 16:52-INFO-
20-03-23 16:52-INFO-Epoch 0, Batch 90, Global step 90:
20-03-23 16:52-INFO-training batch loss: 0.3894; avg_loss: 1.5267
20-03-23 16:52-INFO-training batch acc: 0.8281; avg_acc: 0.6263
20-03-23 16:52-INFO-
20-03-23 16:52-INFO-Epoch 0, Batch 91, Global step 91:
20-03-23 16:52-INFO-training batch loss: 0.4569; avg_loss: 1.5150
20-03-23 16:52-INFO-training batch acc: 0.7656; avg_acc: 0.6278
20-03-23 16:52-INFO-
20-03-23 16:52-INFO-Epoch 0, Batch 92, Global step 92:
20-03-23 16:52-INFO-training batch loss: 0.3013; avg_loss: 1.5018
20-03-23 16:52-INFO-training batch acc: 0.8906; avg_acc: 0.6307
20-03-23 16:52-INFO-
20-03-23 16:52-INFO-Epoch 0, Batch 93, Global step 93:
20-03-23 16:52-INFO-training batch loss: 0.4625; avg_loss: 1.4906
20-03-23 16:52-INFO-training batch acc: 0.7109; avg_acc: 0.6316
20-03-23 16:52-INFO-
20-03-23 16:52-INFO-Epoch 0, Batch 94, Global step 94:
20-03-23 16:52-INFO-training batch loss: 0.3587; avg_loss: 1.4786
20-03-23 16:52-INFO-training batch acc: 0.8281; avg_acc: 0.6336
20-03-23 16:52-INFO-
20-03-23 16:52-INFO-Epoch 0, Batch 95, Global step 95:
20-03-23 16:52-INFO-training batch loss: 0.3564; avg_loss: 1.4668
20-03-23 16:52-INFO-training batch acc: 0.8516; avg_acc: 0.6359
20-03-23 16:52-INFO-
20-03-23 16:52-INFO-Epoch 0, Batch 96, Global step 96:
20-03-23 16:52-INFO-training batch loss: 0.3947; avg_loss: 1.4556
20-03-23 16:52-INFO-training batch acc: 0.8672; avg_acc: 0.6383
20-03-23 16:52-INFO-
20-03-23 16:52-INFO-Epoch 0, Batch 97, Global step 97:
20-03-23 16:52-INFO-training batch loss: 0.3793; avg_loss: 1.4445
20-03-23 16:52-INFO-training batch acc: 0.8359; avg_acc: 0.6404
20-03-23 16:52-INFO-
20-03-23 16:52-INFO-Epoch 0, Batch 98, Global step 98:
20-03-23 16:52-INFO-training batch loss: 0.3166; avg_loss: 1.4330
20-03-23 16:52-INFO-training batch acc: 0.8750; avg_acc: 0.6428
20-03-23 16:52-INFO-
20-03-23 16:52-INFO-Epoch 0, Batch 99, Global step 99:
20-03-23 16:52-INFO-training batch loss: 0.3472; avg_loss: 1.4220
20-03-23 16:52-INFO-training batch acc: 0.8359; avg_acc: 0.6447
20-03-23 16:52-INFO-
20-03-23 16:52-INFO-Epoch 0, Batch 100, Global step 100:
20-03-23 16:52-INFO-training batch loss: 0.2433; avg_loss: 1.4102
20-03-23 16:52-INFO-training batch acc: 0.8906; avg_acc: 0.6472
20-03-23 16:52-INFO-
20-03-23 16:52-INFO-Epoch 0, Batch 101, Global step 101:
20-03-23 16:52-INFO-training batch loss: 0.2437; avg_loss: 1.3987
20-03-23 16:52-INFO-training batch acc: 0.9297; avg_acc: 0.6500
20-03-23 16:52-INFO-
20-03-23 16:52-INFO-Epoch 0, Batch 102, Global step 102:
20-03-23 16:52-INFO-training batch loss: 0.2450; avg_loss: 1.3874
20-03-23 16:52-INFO-training batch acc: 0.9141; avg_acc: 0.6526
20-03-23 16:52-INFO-
20-03-23 16:52-INFO-Epoch 0, Batch 103, Global step 103:
20-03-23 16:52-INFO-training batch loss: 0.2729; avg_loss: 1.3766
20-03-23 16:52-INFO-training batch acc: 0.8750; avg_acc: 0.6547
20-03-23 16:52-INFO-
20-03-23 16:52-INFO-Epoch 0, Batch 104, Global step 104:
20-03-23 16:52-INFO-training batch loss: 0.2570; avg_loss: 1.3658
20-03-23 16:52-INFO-training batch acc: 0.9062; avg_acc: 0.6572
20-03-23 16:52-INFO-
20-03-23 16:52-INFO-Epoch 0, Batch 105, Global step 105:
20-03-23 16:52-INFO-training batch loss: 0.2776; avg_loss: 1.3554
20-03-23 16:52-INFO-training batch acc: 0.8672; avg_acc: 0.6592
20-03-23 16:52-INFO-
20-03-23 16:52-INFO-Epoch 0, Batch 106, Global step 106:
20-03-23 16:52-INFO-training batch loss: 0.3005; avg_loss: 1.3455
20-03-23 16:52-INFO-training batch acc: 0.8750; avg_acc: 0.6612
20-03-23 16:52-INFO-
20-03-23 16:52-INFO-Epoch 0, Batch 107, Global step 107:
20-03-23 16:52-INFO-training batch loss: 0.2658; avg_loss: 1.3354
20-03-23 16:52-INFO-training batch acc: 0.8594; avg_acc: 0.6630
20-03-23 16:52-INFO-
20-03-23 16:52-INFO-Epoch 0, Batch 108, Global step 108:
20-03-23 16:52-INFO-training batch loss: 0.2879; avg_loss: 1.3257
20-03-23 16:52-INFO-training batch acc: 0.8750; avg_acc: 0.6650
20-03-23 16:52-INFO-
20-03-23 16:52-INFO-Epoch 0, Batch 109, Global step 109:
20-03-23 16:52-INFO-training batch loss: 0.2450; avg_loss: 1.3158
20-03-23 16:52-INFO-training batch acc: 0.8984; avg_acc: 0.6671
20-03-23 16:52-INFO-
20-03-23 16:52-INFO-Epoch 0, Batch 110, Global step 110:
20-03-23 16:52-INFO-training batch loss: 0.1856; avg_loss: 1.3055
20-03-23 16:52-INFO-training batch acc: 0.9375; avg_acc: 0.6696
20-03-23 16:52-INFO-
20-03-23 16:52-INFO-Epoch 0, Batch 111, Global step 111:
20-03-23 16:52-INFO-training batch loss: 0.1541; avg_loss: 1.2951
20-03-23 16:52-INFO-training batch acc: 0.9766; avg_acc: 0.6724
20-03-23 16:52-INFO-
20-03-23 16:52-INFO-Epoch 0, Batch 112, Global step 112:
20-03-23 16:52-INFO-training batch loss: 0.1494; avg_loss: 1.2849
20-03-23 16:52-INFO-training batch acc: 0.9688; avg_acc: 0.6750
20-03-23 16:52-INFO-
20-03-23 16:52-INFO-Epoch 0, Batch 113, Global step 113:
20-03-23 16:52-INFO-training batch loss: 0.1211; avg_loss: 1.2746
20-03-23 16:52-INFO-training batch acc: 0.9844; avg_acc: 0.6778
20-03-23 16:52-INFO-
20-03-23 16:52-INFO-Epoch 0, Batch 114, Global step 114:
20-03-23 16:52-INFO-training batch loss: 0.2271; avg_loss: 1.2654
20-03-23 16:52-INFO-training batch acc: 0.9219; avg_acc: 0.6799
20-03-23 16:52-INFO-
20-03-23 16:53-INFO-Epoch 0, Batch 115, Global step 115:
20-03-23 16:53-INFO-training batch loss: 0.2705; avg_loss: 1.2568
20-03-23 16:53-INFO-training batch acc: 0.8984; avg_acc: 0.6818
20-03-23 16:53-INFO-
20-03-23 16:53-INFO-Epoch 0, Batch 116, Global step 116:
20-03-23 16:53-INFO-training batch loss: 0.2323; avg_loss: 1.2479
20-03-23 16:53-INFO-training batch acc: 0.9062; avg_acc: 0.6837
20-03-23 16:53-INFO-
20-03-23 16:53-INFO-Epoch 0, Batch 117, Global step 117:
20-03-23 16:53-INFO-training batch loss: 0.1135; avg_loss: 1.2382
20-03-23 16:53-INFO-training batch acc: 0.9609; avg_acc: 0.6861
20-03-23 16:53-INFO-
20-03-23 16:53-INFO-Epoch 0, Batch 118, Global step 118:
20-03-23 16:53-INFO-training batch loss: 0.1061; avg_loss: 1.2286
20-03-23 16:53-INFO-training batch acc: 0.9766; avg_acc: 0.6886
20-03-23 16:53-INFO-
20-03-23 16:53-INFO-Epoch 0, Batch 119, Global step 119:
20-03-23 16:53-INFO-training batch loss: 0.1455; avg_loss: 1.2195
20-03-23 16:53-INFO-training batch acc: 0.9766; avg_acc: 0.6910
20-03-23 16:53-INFO-
20-03-23 16:53-INFO-Epoch 0, Batch 120, Global step 120:
20-03-23 16:53-INFO-training batch loss: 0.2279; avg_loss: 1.2113
20-03-23 16:53-INFO-training batch acc: 0.9062; avg_acc: 0.6928
20-03-23 16:53-INFO-
20-03-23 16:53-INFO-Epoch 0, Batch 121, Global step 121:
20-03-23 16:53-INFO-training batch loss: 0.1622; avg_loss: 1.2026
20-03-23 16:53-INFO-training batch acc: 0.9453; avg_acc: 0.6949
20-03-23 16:53-INFO-
20-03-23 16:53-INFO-Epoch 0, Batch 122, Global step 122:
20-03-23 16:53-INFO-training batch loss: 0.2278; avg_loss: 1.1946
20-03-23 16:53-INFO-training batch acc: 0.9297; avg_acc: 0.6968
20-03-23 16:53-INFO-
20-03-23 16:53-INFO-Epoch 0, Batch 123, Global step 123:
20-03-23 16:53-INFO-training batch loss: 0.1784; avg_loss: 1.1863
20-03-23 16:53-INFO-training batch acc: 0.9453; avg_acc: 0.6988
20-03-23 16:53-INFO-
20-03-23 16:53-INFO-Epoch 0, Batch 124, Global step 124:
20-03-23 16:53-INFO-training batch loss: 0.1365; avg_loss: 1.1779
20-03-23 16:53-INFO-training batch acc: 0.9531; avg_acc: 0.7009
20-03-23 16:53-INFO-
20-03-23 16:53-INFO-Epoch 0, Batch 125, Global step 125:
20-03-23 16:53-INFO-training batch loss: 0.1719; avg_loss: 1.1698
20-03-23 16:53-INFO-training batch acc: 0.9531; avg_acc: 0.7029
20-03-23 16:53-INFO-
20-03-23 16:53-INFO-Epoch 0, Batch 126, Global step 126:
20-03-23 16:53-INFO-training batch loss: 0.1289; avg_loss: 1.1616
20-03-23 16:53-INFO-training batch acc: 0.9688; avg_acc: 0.7050
20-03-23 16:53-INFO-
20-03-23 16:53-INFO-Epoch 0, Batch 127, Global step 127:
20-03-23 16:53-INFO-training batch loss: 0.0829; avg_loss: 1.1531
20-03-23 16:53-INFO-training batch acc: 0.9766; avg_acc: 0.7071
20-03-23 16:53-INFO-
20-03-23 16:53-INFO-Epoch 0, Batch 128, Global step 128:
20-03-23 16:53-INFO-training batch loss: 0.0885; avg_loss: 1.1448
20-03-23 16:53-INFO-training batch acc: 0.9609; avg_acc: 0.7091
20-03-23 16:53-INFO-
20-03-23 16:53-INFO-Epoch 0, Batch 129, Global step 129:
20-03-23 16:53-INFO-training batch loss: 0.2143; avg_loss: 1.1375
20-03-23 16:53-INFO-training batch acc: 0.9609; avg_acc: 0.7111
20-03-23 16:53-INFO-
20-03-23 16:53-INFO-Epoch 0, Batch 130, Global step 130:
20-03-23 16:53-INFO-training batch loss: 0.0954; avg_loss: 1.1295
20-03-23 16:53-INFO-training batch acc: 0.9766; avg_acc: 0.7131
20-03-23 16:53-INFO-
20-03-23 16:53-INFO-Epoch 0, Batch 131, Global step 131:
20-03-23 16:53-INFO-training batch loss: 0.1876; avg_loss: 1.1223
20-03-23 16:53-INFO-training batch acc: 0.9531; avg_acc: 0.7149
20-03-23 16:53-INFO-
20-03-23 16:53-INFO-Epoch 0, Batch 132, Global step 132:
20-03-23 16:53-INFO-training batch loss: 0.1577; avg_loss: 1.1150
20-03-23 16:53-INFO-training batch acc: 0.9609; avg_acc: 0.7168
20-03-23 16:53-INFO-
20-03-23 16:53-INFO-Epoch 0, Batch 133, Global step 133:
20-03-23 16:53-INFO-training batch loss: 0.0844; avg_loss: 1.1073
20-03-23 16:53-INFO-training batch acc: 0.9609; avg_acc: 0.7186
20-03-23 16:53-INFO-
20-03-23 16:53-INFO-Epoch 0, Batch 134, Global step 134:
20-03-23 16:53-INFO-training batch loss: 0.0807; avg_loss: 1.0996
20-03-23 16:53-INFO-training batch acc: 0.9609; avg_acc: 0.7204
20-03-23 16:53-INFO-
20-03-23 16:53-INFO-Epoch 0, Batch 135, Global step 135:
20-03-23 16:53-INFO-training batch loss: 0.1057; avg_loss: 1.0923
20-03-23 16:53-INFO-training batch acc: 0.9688; avg_acc: 0.7223
20-03-23 16:53-INFO-
20-03-23 16:53-INFO-Epoch 0, Batch 136, Global step 136:
20-03-23 16:53-INFO-training batch loss: 0.1551; avg_loss: 1.0854
20-03-23 16:53-INFO-training batch acc: 0.9766; avg_acc: 0.7241
20-03-23 16:53-INFO-
20-03-23 16:53-INFO-Epoch 0, Batch 137, Global step 137:
20-03-23 16:53-INFO-training batch loss: 0.1521; avg_loss: 1.0786
20-03-23 16:53-INFO-training batch acc: 0.9609; avg_acc: 0.7259
20-03-23 16:53-INFO-
20-03-23 16:53-INFO-Epoch 0, Batch 138, Global step 138:
20-03-23 16:53-INFO-training batch loss: 0.1808; avg_loss: 1.0721
20-03-23 16:53-INFO-training batch acc: 0.9453; avg_acc: 0.7275
20-03-23 16:53-INFO-
20-03-23 16:53-INFO-Epoch 0, Batch 139, Global step 139:
20-03-23 16:53-INFO-training batch loss: 0.1323; avg_loss: 1.0653
20-03-23 16:53-INFO-training batch acc: 0.9453; avg_acc: 0.7290
20-03-23 16:53-INFO-
20-03-23 16:53-INFO-Epoch 0, Batch 140, Global step 140:
20-03-23 16:53-INFO-training batch loss: 0.0906; avg_loss: 1.0583
20-03-23 16:53-INFO-training batch acc: 0.9922; avg_acc: 0.7309
20-03-23 16:53-INFO-
20-03-23 16:53-INFO-Epoch 0, Batch 141, Global step 141:
20-03-23 16:53-INFO-training batch loss: 0.2085; avg_loss: 1.0523
20-03-23 16:53-INFO-training batch acc: 0.9531; avg_acc: 0.7325
20-03-23 16:53-INFO-
20-03-23 16:54-INFO-Epoch 0, Batch 142, Global step 142:
20-03-23 16:54-INFO-training batch loss: 0.1256; avg_loss: 1.0458
20-03-23 16:54-INFO-training batch acc: 0.9609; avg_acc: 0.7341
20-03-23 16:54-INFO-
20-03-23 16:54-INFO-Epoch 0, Batch 143, Global step 143:
20-03-23 16:54-INFO-training batch loss: 0.0722; avg_loss: 1.0390
20-03-23 16:54-INFO-training batch acc: 0.9766; avg_acc: 0.7358
20-03-23 16:54-INFO-
20-03-23 16:54-INFO-Epoch 0, Batch 144, Global step 144:
20-03-23 16:54-INFO-training batch loss: 0.1723; avg_loss: 1.0329
20-03-23 16:54-INFO-training batch acc: 0.9453; avg_acc: 0.7373
20-03-23 16:54-INFO-
20-03-23 16:54-INFO-Epoch 0, Batch 145, Global step 145:
20-03-23 16:54-INFO-training batch loss: 0.1242; avg_loss: 1.0267
20-03-23 16:54-INFO-training batch acc: 0.9609; avg_acc: 0.7388
20-03-23 16:54-INFO-
20-03-23 16:54-INFO-Epoch 0, Batch 146, Global step 146:
20-03-23 16:54-INFO-training batch loss: 0.0876; avg_loss: 1.0202
20-03-23 16:54-INFO-training batch acc: 0.9766; avg_acc: 0.7404
20-03-23 16:54-INFO-
20-03-23 16:54-INFO-Epoch 0, Batch 147, Global step 147:
20-03-23 16:54-INFO-training batch loss: 0.0533; avg_loss: 1.0137
20-03-23 16:54-INFO-training batch acc: 0.9922; avg_acc: 0.7421
20-03-23 16:54-INFO-
20-03-23 16:54-INFO-Epoch 0, Batch 148, Global step 148:
20-03-23 16:54-INFO-training batch loss: 0.0738; avg_loss: 1.0073
20-03-23 16:54-INFO-training batch acc: 0.9844; avg_acc: 0.7438
20-03-23 16:54-INFO-
20-03-23 16:54-INFO-Epoch 0, Batch 149, Global step 149:
20-03-23 16:54-INFO-training batch loss: 0.1208; avg_loss: 1.0014
20-03-23 16:54-INFO-training batch acc: 0.9531; avg_acc: 0.7452
20-03-23 16:54-INFO-
20-03-23 16:54-INFO-Epoch 0, Batch 150, Global step 150:
20-03-23 16:54-INFO-training batch loss: 0.0973; avg_loss: 0.9953
20-03-23 16:54-INFO-training batch acc: 0.9453; avg_acc: 0.7465
20-03-23 16:54-INFO-
20-03-23 16:54-INFO-Epoch 0, Batch 151, Global step 151:
20-03-23 16:54-INFO-training batch loss: 0.1305; avg_loss: 0.9896
20-03-23 16:54-INFO-training batch acc: 0.9609; avg_acc: 0.7479
20-03-23 16:54-INFO-
20-03-23 16:54-INFO-Epoch 0, Batch 152, Global step 152:
20-03-23 16:54-INFO-training batch loss: 0.0906; avg_loss: 0.9837
20-03-23 16:54-INFO-training batch acc: 0.9688; avg_acc: 0.7494
20-03-23 16:54-INFO-
20-03-23 16:54-INFO-Epoch 0, Batch 153, Global step 153:
20-03-23 16:54-INFO-training batch loss: 0.1462; avg_loss: 0.9782
20-03-23 16:54-INFO-training batch acc: 0.9688; avg_acc: 0.7508
20-03-23 16:54-INFO-
20-03-23 16:54-INFO-Epoch 0, Batch 154, Global step 154:
20-03-23 16:54-INFO-training batch loss: 0.2064; avg_loss: 0.9732
20-03-23 16:54-INFO-training batch acc: 0.9531; avg_acc: 0.7521
20-03-23 16:54-INFO-
20-03-23 16:54-INFO-Epoch 0, Batch 155, Global step 155:
20-03-23 16:54-INFO-training batch loss: 0.0987; avg_loss: 0.9676
20-03-23 16:54-INFO-training batch acc: 0.9688; avg_acc: 0.7535
20-03-23 16:54-INFO-
20-03-23 16:54-INFO-Epoch 0, Batch 156, Global step 156:
20-03-23 16:54-INFO-training batch loss: 0.0867; avg_loss: 0.9619
20-03-23 16:54-INFO-training batch acc: 0.9688; avg_acc: 0.7549
20-03-23 16:54-INFO-
20-03-23 16:54-INFO-Epoch 0, Batch 157, Global step 157:
20-03-23 16:54-INFO-training batch loss: 0.1525; avg_loss: 0.9568
20-03-23 16:54-INFO-training batch acc: 0.9609; avg_acc: 0.7562
20-03-23 16:54-INFO-
20-03-23 16:54-INFO-Epoch 0, Batch 158, Global step 158:
20-03-23 16:54-INFO-training batch loss: 0.0569; avg_loss: 0.9511
20-03-23 16:54-INFO-training batch acc: 0.9688; avg_acc: 0.7576
20-03-23 16:54-INFO-
20-03-23 16:54-INFO-Epoch 0, Batch 159, Global step 159:
20-03-23 16:54-INFO-training batch loss: 0.0275; avg_loss: 0.9453
20-03-23 16:54-INFO-training batch acc: 1.0000; avg_acc: 0.7591
20-03-23 16:54-INFO-
20-03-23 16:54-INFO-Epoch 0, Batch 160, Global step 160:
20-03-23 16:54-INFO-training batch loss: 0.0805; avg_loss: 0.9399
20-03-23 16:54-INFO-training batch acc: 0.9766; avg_acc: 0.7604
20-03-23 16:54-INFO-
20-03-23 16:54-INFO-Epoch 0, Batch 161, Global step 161:
20-03-23 16:54-INFO-training batch loss: 0.0452; avg_loss: 0.9343
20-03-23 16:54-INFO-training batch acc: 0.9922; avg_acc: 0.7619
20-03-23 16:54-INFO-
20-03-23 16:54-INFO-Epoch 0, Batch 162, Global step 162:
20-03-23 16:54-INFO-training batch loss: 0.0789; avg_loss: 0.9290
20-03-23 16:54-INFO-training batch acc: 0.9766; avg_acc: 0.7632
20-03-23 16:54-INFO-
20-03-23 16:54-INFO-Epoch 0, Batch 163, Global step 163:
20-03-23 16:54-INFO-training batch loss: 0.0950; avg_loss: 0.9239
20-03-23 16:54-INFO-training batch acc: 0.9766; avg_acc: 0.7645
20-03-23 16:54-INFO-
20-03-23 16:54-INFO-Epoch 0, Batch 164, Global step 164:
20-03-23 16:54-INFO-training batch loss: 0.0689; avg_loss: 0.9187
20-03-23 16:54-INFO-training batch acc: 0.9844; avg_acc: 0.7659
20-03-23 16:54-INFO-
20-03-23 16:54-INFO-Epoch 0, Batch 165, Global step 165:
20-03-23 16:54-INFO-training batch loss: 0.0486; avg_loss: 0.9134
20-03-23 16:54-INFO-training batch acc: 0.9844; avg_acc: 0.7672
20-03-23 16:54-INFO-
20-03-23 16:54-INFO-Epoch 0, Batch 166, Global step 166:
20-03-23 16:54-INFO-training batch loss: 0.0774; avg_loss: 0.9084
20-03-23 16:54-INFO-training batch acc: 0.9844; avg_acc: 0.7685
20-03-23 16:54-INFO-
20-03-23 16:54-INFO-Epoch 0, Batch 167, Global step 167:
20-03-23 16:54-INFO-training batch loss: 0.0557; avg_loss: 0.9033
20-03-23 16:54-INFO-training batch acc: 0.9766; avg_acc: 0.7697
20-03-23 16:54-INFO-
20-03-23 16:55-INFO-Epoch 0, Batch 168, Global step 168:
20-03-23 16:55-INFO-training batch loss: 0.0538; avg_loss: 0.8982
20-03-23 16:55-INFO-training batch acc: 0.9766; avg_acc: 0.7710
20-03-23 16:55-INFO-
20-03-23 16:55-INFO-Epoch 0, Batch 169, Global step 169:
20-03-23 16:55-INFO-training batch loss: 0.0685; avg_loss: 0.8933
20-03-23 16:55-INFO-training batch acc: 0.9766; avg_acc: 0.7722
20-03-23 16:55-INFO-
20-03-23 16:55-INFO-Epoch 0, Batch 170, Global step 170:
20-03-23 16:55-INFO-training batch loss: 0.1580; avg_loss: 0.8890
20-03-23 16:55-INFO-training batch acc: 0.9766; avg_acc: 0.7734
20-03-23 16:55-INFO-
20-03-23 16:55-INFO-Epoch 0, Batch 171, Global step 171:
20-03-23 16:55-INFO-training batch loss: 0.0925; avg_loss: 0.8843
20-03-23 16:55-INFO-training batch acc: 0.9766; avg_acc: 0.7746
20-03-23 16:55-INFO-
20-03-23 16:55-INFO-Epoch 0, Batch 172, Global step 172:
20-03-23 16:55-INFO-training batch loss: 0.1822; avg_loss: 0.8802
20-03-23 16:55-INFO-training batch acc: 0.9609; avg_acc: 0.7757
20-03-23 16:55-INFO-
20-03-23 16:55-INFO-Epoch 0, Batch 173, Global step 173:
20-03-23 16:55-INFO-training batch loss: 0.1451; avg_loss: 0.8760
20-03-23 16:55-INFO-training batch acc: 0.9297; avg_acc: 0.7766
20-03-23 16:55-INFO-
20-03-23 16:55-INFO-Epoch 0, Batch 174, Global step 174:
20-03-23 16:55-INFO-training batch loss: 0.0226; avg_loss: 0.8711
20-03-23 16:55-INFO-training batch acc: 0.9922; avg_acc: 0.7778
20-03-23 16:55-INFO-
20-03-23 16:55-INFO-Epoch 0, Batch 175, Global step 175:
20-03-23 16:55-INFO-training batch loss: 0.0618; avg_loss: 0.8665
20-03-23 16:55-INFO-training batch acc: 0.9766; avg_acc: 0.7789
20-03-23 16:55-INFO-
20-03-23 16:55-INFO-Epoch 0, Batch 176, Global step 176:
20-03-23 16:55-INFO-training batch loss: 0.0786; avg_loss: 0.8620
20-03-23 16:55-INFO-training batch acc: 0.9688; avg_acc: 0.7800
20-03-23 16:55-INFO-
20-03-23 16:55-INFO-Epoch 0, Batch 177, Global step 177:
20-03-23 16:55-INFO-training batch loss: 0.0247; avg_loss: 0.8573
20-03-23 16:55-INFO-training batch acc: 1.0000; avg_acc: 0.7812
20-03-23 16:55-INFO-
20-03-23 16:55-INFO-Epoch 0, Batch 178, Global step 178:
20-03-23 16:55-INFO-training batch loss: 0.1421; avg_loss: 0.8532
20-03-23 16:55-INFO-training batch acc: 0.9609; avg_acc: 0.7823
20-03-23 16:55-INFO-
20-03-23 16:55-INFO-Epoch 0, Batch 179, Global step 179:
20-03-23 16:55-INFO-training batch loss: 0.0689; avg_loss: 0.8489
20-03-23 16:55-INFO-training batch acc: 0.9766; avg_acc: 0.7833
20-03-23 16:55-INFO-
20-03-23 16:55-INFO-Epoch 0, Batch 180, Global step 180:
20-03-23 16:55-INFO-training batch loss: 0.0756; avg_loss: 0.8446
20-03-23 16:55-INFO-training batch acc: 0.9688; avg_acc: 0.7844
20-03-23 16:55-INFO-
20-03-23 16:55-INFO-Epoch 0, Batch 181, Global step 181:
20-03-23 16:55-INFO-training batch loss: 0.0360; avg_loss: 0.8401
20-03-23 16:55-INFO-training batch acc: 0.9922; avg_acc: 0.7855
20-03-23 16:55-INFO-
20-03-23 16:55-INFO-Epoch 0, Batch 182, Global step 182:
20-03-23 16:55-INFO-training batch loss: 0.0444; avg_loss: 0.8357
20-03-23 16:55-INFO-training batch acc: 0.9844; avg_acc: 0.7866
20-03-23 16:55-INFO-
20-03-23 16:55-INFO-Epoch 0, Batch 183, Global step 183:
20-03-23 16:55-INFO-training batch loss: 0.0298; avg_loss: 0.8313
20-03-23 16:55-INFO-training batch acc: 0.9922; avg_acc: 0.7877
20-03-23 16:55-INFO-
20-03-23 16:55-INFO-Epoch 0, Batch 184, Global step 184:
20-03-23 16:55-INFO-training batch loss: 0.1584; avg_loss: 0.8277
20-03-23 16:55-INFO-training batch acc: 0.9531; avg_acc: 0.7886
20-03-23 16:55-INFO-
20-03-23 16:55-INFO-Epoch 0, Batch 185, Global step 185:
20-03-23 16:55-INFO-training batch loss: 0.0160; avg_loss: 0.8233
20-03-23 16:55-INFO-training batch acc: 1.0000; avg_acc: 0.7898
20-03-23 16:55-INFO-
20-03-23 16:55-INFO-Epoch 0, Batch 186, Global step 186:
20-03-23 16:55-INFO-training batch loss: 0.0299; avg_loss: 0.8190
20-03-23 16:55-INFO-training batch acc: 1.0000; avg_acc: 0.7909
20-03-23 16:55-INFO-
20-03-23 16:55-INFO-Epoch 0, Batch 187, Global step 187:
20-03-23 16:55-INFO-training batch loss: 0.0801; avg_loss: 0.8151
20-03-23 16:55-INFO-training batch acc: 0.9922; avg_acc: 0.7920
20-03-23 16:55-INFO-
20-03-23 16:55-INFO-Epoch 0, Batch 188, Global step 188:
20-03-23 16:55-INFO-training batch loss: 0.0243; avg_loss: 0.8109
20-03-23 16:55-INFO-training batch acc: 1.0000; avg_acc: 0.7931
20-03-23 16:55-INFO-
20-03-23 16:55-INFO-Epoch 0, Batch 189, Global step 189:
20-03-23 16:55-INFO-training batch loss: 0.0116; avg_loss: 0.8066
20-03-23 16:55-INFO-training batch acc: 1.0000; avg_acc: 0.7942
20-03-23 16:55-INFO-
20-03-23 16:55-INFO-Epoch 0, Batch 190, Global step 190:
20-03-23 16:55-INFO-training batch loss: 0.0295; avg_loss: 0.8025
20-03-23 16:55-INFO-training batch acc: 0.9922; avg_acc: 0.7952
20-03-23 16:55-INFO-
20-03-23 16:55-INFO-Epoch 0, Batch 191, Global step 191:
20-03-23 16:55-INFO-training batch loss: 0.0363; avg_loss: 0.7985
20-03-23 16:55-INFO-training batch acc: 0.9922; avg_acc: 0.7963
20-03-23 16:55-INFO-
20-03-23 16:55-INFO-Epoch 0, Batch 192, Global step 192:
20-03-23 16:55-INFO-training batch loss: 0.0934; avg_loss: 0.7949
20-03-23 16:55-INFO-training batch acc: 0.9766; avg_acc: 0.7972
20-03-23 16:55-INFO-
20-03-23 16:55-INFO-Epoch 0, Batch 193, Global step 193:
20-03-23 16:55-INFO-training batch loss: 0.0272; avg_loss: 0.7909
20-03-23 16:55-INFO-training batch acc: 0.9922; avg_acc: 0.7982
20-03-23 16:55-INFO-
20-03-23 16:56-INFO-Epoch 0, Batch 194, Global step 194:
20-03-23 16:56-INFO-training batch loss: 0.0153; avg_loss: 0.7869
20-03-23 16:56-INFO-training batch acc: 1.0000; avg_acc: 0.7993
20-03-23 16:56-INFO-
20-03-23 16:56-INFO-Epoch 0, Batch 195, Global step 195:
20-03-23 16:56-INFO-training batch loss: 0.0115; avg_loss: 0.7829
20-03-23 16:56-INFO-training batch acc: 1.0000; avg_acc: 0.8003
20-03-23 16:56-INFO-
20-03-23 16:56-INFO-Epoch 0, Batch 196, Global step 196:
20-03-23 16:56-INFO-training batch loss: 0.0850; avg_loss: 0.7793
20-03-23 16:56-INFO-training batch acc: 0.9844; avg_acc: 0.8012
20-03-23 16:56-INFO-
20-03-23 16:56-INFO-Epoch 0, Batch 197, Global step 197:
20-03-23 16:56-INFO-training batch loss: 0.0340; avg_loss: 0.7756
20-03-23 16:56-INFO-training batch acc: 0.9922; avg_acc: 0.8022
20-03-23 16:56-INFO-
20-03-23 16:56-INFO-Epoch 0, Batch 198, Global step 198:
20-03-23 16:56-INFO-training batch loss: 0.0373; avg_loss: 0.7718
20-03-23 16:56-INFO-training batch acc: 0.9922; avg_acc: 0.8031
20-03-23 16:56-INFO-
20-03-23 16:56-INFO-Epoch 0, Batch 199, Global step 199:
20-03-23 16:56-INFO-training batch loss: 0.0083; avg_loss: 0.7680
20-03-23 16:56-INFO-training batch acc: 1.0000; avg_acc: 0.8041
20-03-23 16:56-INFO-
20-03-23 16:56-INFO-Epoch 0, Batch 200, Global step 200:
20-03-23 16:56-INFO-training batch loss: 0.0919; avg_loss: 0.7646
20-03-23 16:56-INFO-training batch acc: 0.9844; avg_acc: 0.8050
20-03-23 16:56-INFO-
20-03-23 16:56-INFO-Epoch 0, Batch 201, Global step 201:
20-03-23 16:56-INFO-training batch loss: 0.0232; avg_loss: 0.7609
20-03-23 16:56-INFO-training batch acc: 0.9922; avg_acc: 0.8060
20-03-23 16:56-INFO-
20-03-23 16:56-INFO-Epoch 0, Batch 202, Global step 202:
20-03-23 16:56-INFO-training batch loss: 0.0152; avg_loss: 0.7572
20-03-23 16:56-INFO-training batch acc: 0.9922; avg_acc: 0.8069
20-03-23 16:56-INFO-
20-03-23 16:56-INFO-Epoch 0, Batch 203, Global step 203:
20-03-23 16:56-INFO-training batch loss: 0.0384; avg_loss: 0.7537
20-03-23 16:56-INFO-training batch acc: 0.9922; avg_acc: 0.8078
20-03-23 16:56-INFO-
20-03-23 16:56-INFO-Epoch 0, Batch 204, Global step 204:
20-03-23 16:56-INFO-training batch loss: 0.0715; avg_loss: 0.7503
20-03-23 16:56-INFO-training batch acc: 0.9844; avg_acc: 0.8087
20-03-23 16:56-INFO-
20-03-23 16:56-INFO-Epoch 0, Batch 205, Global step 205:
20-03-23 16:56-INFO-training batch loss: 0.0267; avg_loss: 0.7468
20-03-23 16:56-INFO-training batch acc: 1.0000; avg_acc: 0.8096
20-03-23 16:56-INFO-
20-03-23 16:56-INFO-Epoch 0, Batch 206, Global step 206:
20-03-23 16:56-INFO-training batch loss: 0.0166; avg_loss: 0.7433
20-03-23 16:56-INFO-training batch acc: 0.9922; avg_acc: 0.8105
20-03-23 16:56-INFO-
20-03-23 16:56-INFO-Epoch 0, Batch 207, Global step 207:
20-03-23 16:56-INFO-training batch loss: 0.0255; avg_loss: 0.7398
20-03-23 16:56-INFO-training batch acc: 0.9922; avg_acc: 0.8114
20-03-23 16:56-INFO-
20-03-23 16:56-INFO-Epoch 0, Batch 208, Global step 208:
20-03-23 16:56-INFO-training batch loss: 0.0326; avg_loss: 0.7364
20-03-23 16:56-INFO-training batch acc: 0.9922; avg_acc: 0.8122
20-03-23 16:56-INFO-
20-03-23 16:56-INFO-Epoch 0, Batch 209, Global step 209:
20-03-23 16:56-INFO-training batch loss: 0.0672; avg_loss: 0.7332
20-03-23 16:56-INFO-training batch acc: 0.9922; avg_acc: 0.8131
20-03-23 16:56-INFO-
20-03-23 16:56-INFO-Epoch 0, Batch 210, Global step 210:
20-03-23 16:56-INFO-training batch loss: 0.0159; avg_loss: 0.7298
20-03-23 16:56-INFO-training batch acc: 0.9922; avg_acc: 0.8140
20-03-23 16:56-INFO-
20-03-23 16:56-INFO-Epoch 0, Batch 211, Global step 211:
20-03-23 16:56-INFO-training batch loss: 0.0078; avg_loss: 0.7264
20-03-23 16:56-INFO-training batch acc: 1.0000; avg_acc: 0.8148
20-03-23 16:56-INFO-
20-03-23 16:56-INFO-Epoch 0, Batch 212, Global step 212:
20-03-23 16:56-INFO-training batch loss: 0.0158; avg_loss: 0.7230
20-03-23 16:56-INFO-training batch acc: 0.9922; avg_acc: 0.8157
20-03-23 16:56-INFO-
20-03-23 16:56-INFO-Epoch 0, Batch 213, Global step 213:
20-03-23 16:56-INFO-training batch loss: 0.0926; avg_loss: 0.7201
20-03-23 16:56-INFO-training batch acc: 0.9844; avg_acc: 0.8165
20-03-23 16:56-INFO-
20-03-23 16:56-INFO-Epoch 0, Batch 214, Global step 214:
20-03-23 16:56-INFO-training batch loss: 0.0088; avg_loss: 0.7167
20-03-23 16:56-INFO-training batch acc: 1.0000; avg_acc: 0.8173
20-03-23 16:56-INFO-
20-03-23 16:56-INFO-Epoch 0, Batch 215, Global step 215:
20-03-23 16:56-INFO-training batch loss: 0.0129; avg_loss: 0.7135
20-03-23 16:56-INFO-training batch acc: 1.0000; avg_acc: 0.8182
20-03-23 16:56-INFO-
20-03-23 16:56-INFO-Epoch 0, Batch 216, Global step 216:
20-03-23 16:56-INFO-training batch loss: 0.0947; avg_loss: 0.7106
20-03-23 16:56-INFO-training batch acc: 0.9922; avg_acc: 0.8190
20-03-23 16:56-INFO-
20-03-23 16:56-INFO-Epoch 0, Batch 217, Global step 217:
20-03-23 16:56-INFO-training batch loss: 0.0187; avg_loss: 0.7074
20-03-23 16:56-INFO-training batch acc: 1.0000; avg_acc: 0.8198
20-03-23 16:56-INFO-
20-03-23 16:56-INFO-Epoch 0, Batch 218, Global step 218:
20-03-23 16:56-INFO-training batch loss: 0.1362; avg_loss: 0.7048
20-03-23 16:56-INFO-training batch acc: 0.9766; avg_acc: 0.8205
20-03-23 16:56-INFO-
20-03-23 16:56-INFO-Epoch 0, Batch 219, Global step 219:
20-03-23 16:56-INFO-training batch loss: 0.0171; avg_loss: 0.7016
20-03-23 16:56-INFO-training batch acc: 1.0000; avg_acc: 0.8213
20-03-23 16:56-INFO-
20-03-23 16:57-INFO-Epoch 0, Batch 220, Global step 220:
20-03-23 16:57-INFO-training batch loss: 0.0100; avg_loss: 0.6985
20-03-23 16:57-INFO-training batch acc: 1.0000; avg_acc: 0.8222
20-03-23 16:57-INFO-
20-03-23 16:57-INFO-Epoch 0, Batch 221, Global step 221:
20-03-23 16:57-INFO-training batch loss: 0.0318; avg_loss: 0.6955
20-03-23 16:57-INFO-training batch acc: 0.9766; avg_acc: 0.8229
20-03-23 16:57-INFO-
20-03-23 16:57-INFO-Epoch 0, Batch 222, Global step 222:
20-03-23 16:57-INFO-training batch loss: 0.0871; avg_loss: 0.6927
20-03-23 16:57-INFO-training batch acc: 0.9688; avg_acc: 0.8235
20-03-23 16:57-INFO-
20-03-23 16:57-INFO-Epoch 0, Batch 223, Global step 223:
20-03-23 16:57-INFO-training batch loss: 0.0510; avg_loss: 0.6899
20-03-23 16:57-INFO-training batch acc: 0.9922; avg_acc: 0.8243
20-03-23 16:57-INFO-
20-03-23 16:57-INFO-Epoch 0, Batch 224, Global step 224:
20-03-23 16:57-INFO-training batch loss: 0.0205; avg_loss: 0.6869
20-03-23 16:57-INFO-training batch acc: 1.0000; avg_acc: 0.8251
20-03-23 16:57-INFO-
20-03-23 16:57-INFO-Epoch 0, Batch 225, Global step 225:
20-03-23 16:57-INFO-training batch loss: 0.0258; avg_loss: 0.6839
20-03-23 16:57-INFO-training batch acc: 1.0000; avg_acc: 0.8258
20-03-23 16:57-INFO-
20-03-23 16:57-INFO-Epoch 0, Batch 226, Global step 226:
20-03-23 16:57-INFO-training batch loss: 0.0604; avg_loss: 0.6812
20-03-23 16:57-INFO-training batch acc: 0.9844; avg_acc: 0.8265
20-03-23 16:57-INFO-
20-03-23 16:57-INFO-Epoch 0, Batch 227, Global step 227:
20-03-23 16:57-INFO-training batch loss: 0.0503; avg_loss: 0.6784
20-03-23 16:57-INFO-training batch acc: 0.9844; avg_acc: 0.8272
20-03-23 16:57-INFO-
20-03-23 16:57-INFO-Epoch 0, Batch 228, Global step 228:
20-03-23 16:57-INFO-training batch loss: 0.0147; avg_loss: 0.6755
20-03-23 16:57-INFO-training batch acc: 1.0000; avg_acc: 0.8280
20-03-23 16:57-INFO-
20-03-23 16:57-INFO-Epoch 0, Batch 229, Global step 229:
20-03-23 16:57-INFO-training batch loss: 0.0177; avg_loss: 0.6726
20-03-23 16:57-INFO-training batch acc: 1.0000; avg_acc: 0.8287
20-03-23 16:57-INFO-
20-03-23 16:57-INFO-Epoch 0, Batch 230, Global step 230:
20-03-23 16:57-INFO-training batch loss: 0.0134; avg_loss: 0.6697
20-03-23 16:57-INFO-training batch acc: 1.0000; avg_acc: 0.8295
20-03-23 16:57-INFO-
20-03-23 16:57-INFO-Epoch 0, Batch 231, Global step 231:
20-03-23 16:57-INFO-training batch loss: 0.1138; avg_loss: 0.6673
20-03-23 16:57-INFO-training batch acc: 0.9688; avg_acc: 0.8301
20-03-23 16:57-INFO-
20-03-23 16:57-INFO-Epoch 0, Batch 232, Global step 232:
20-03-23 16:57-INFO-training batch loss: 0.0142; avg_loss: 0.6645
20-03-23 16:57-INFO-training batch acc: 1.0000; avg_acc: 0.8308
20-03-23 16:57-INFO-
20-03-23 16:57-INFO-Epoch 0, Batch 233, Global step 233:
20-03-23 16:57-INFO-training batch loss: 0.0257; avg_loss: 0.6618
20-03-23 16:57-INFO-training batch acc: 0.9844; avg_acc: 0.8315
20-03-23 16:57-INFO-
20-03-23 16:57-INFO-Epoch 0, Batch 234, Global step 234:
20-03-23 16:57-INFO-training batch loss: 0.0533; avg_loss: 0.6592
20-03-23 16:57-INFO-training batch acc: 0.9922; avg_acc: 0.8322
20-03-23 16:57-INFO-
20-03-23 16:57-INFO-Epoch 0, Batch 235, Global step 235:
20-03-23 16:57-INFO-training batch loss: 0.0211; avg_loss: 0.6565
20-03-23 16:57-INFO-training batch acc: 0.9922; avg_acc: 0.8328
20-03-23 16:57-INFO-
20-03-23 16:57-INFO-Epoch 0, Batch 236, Global step 236:
20-03-23 16:57-INFO-training batch loss: 0.0697; avg_loss: 0.6540
20-03-23 16:57-INFO-training batch acc: 0.9844; avg_acc: 0.8335
20-03-23 16:57-INFO-
20-03-23 16:57-INFO-Epoch 0, Batch 237, Global step 237:
20-03-23 16:57-INFO-training batch loss: 0.0210; avg_loss: 0.6513
20-03-23 16:57-INFO-training batch acc: 0.9844; avg_acc: 0.8341
20-03-23 16:57-INFO-
20-03-23 16:57-INFO-Epoch 0, Batch 238, Global step 238:
20-03-23 16:57-INFO-training batch loss: 0.0653; avg_loss: 0.6488
20-03-23 16:57-INFO-training batch acc: 0.9844; avg_acc: 0.8348
20-03-23 16:57-INFO-
20-03-23 16:57-INFO-Epoch 0, Batch 239, Global step 239:
20-03-23 16:57-INFO-training batch loss: 0.0220; avg_loss: 0.6462
20-03-23 16:57-INFO-training batch acc: 1.0000; avg_acc: 0.8354
20-03-23 16:57-INFO-
20-03-23 16:57-INFO-Epoch 0, Batch 240, Global step 240:
20-03-23 16:57-INFO-training batch loss: 0.0459; avg_loss: 0.6437
20-03-23 16:57-INFO-training batch acc: 0.9922; avg_acc: 0.8361
20-03-23 16:57-INFO-
20-03-23 16:57-INFO-Epoch 0, Batch 241, Global step 241:
20-03-23 16:57-INFO-training batch loss: 0.1263; avg_loss: 0.6416
20-03-23 16:57-INFO-training batch acc: 0.9688; avg_acc: 0.8367
20-03-23 16:57-INFO-
20-03-23 16:57-INFO-Epoch 0, Batch 242, Global step 242:
20-03-23 16:57-INFO-training batch loss: 0.0307; avg_loss: 0.6391
20-03-23 16:57-INFO-training batch acc: 0.9922; avg_acc: 0.8373
20-03-23 16:57-INFO-
20-03-23 16:57-INFO-Epoch 0, Batch 243, Global step 243:
20-03-23 16:57-INFO-training batch loss: 0.0215; avg_loss: 0.6365
20-03-23 16:57-INFO-training batch acc: 0.9922; avg_acc: 0.8379
20-03-23 16:57-INFO-
20-03-23 16:57-INFO-Epoch 0, Batch 244, Global step 244:
20-03-23 16:57-INFO-training batch loss: 0.0647; avg_loss: 0.6342
20-03-23 16:57-INFO-training batch acc: 0.9844; avg_acc: 0.8385
20-03-23 16:57-INFO-
20-03-23 16:58-INFO-Epoch 0, Batch 245, Global step 245:
20-03-23 16:58-INFO-training batch loss: 0.0129; avg_loss: 0.6316
20-03-23 16:58-INFO-training batch acc: 1.0000; avg_acc: 0.8392
20-03-23 16:58-INFO-
20-03-23 16:58-INFO-Epoch 0, Batch 246, Global step 246:
20-03-23 16:58-INFO-training batch loss: 0.0412; avg_loss: 0.6292
20-03-23 16:58-INFO-training batch acc: 0.9922; avg_acc: 0.8398
20-03-23 16:58-INFO-
20-03-23 16:58-INFO-Epoch 0, Batch 247, Global step 247:
20-03-23 16:58-INFO-training batch loss: 0.0820; avg_loss: 0.6270
20-03-23 16:58-INFO-training batch acc: 0.9688; avg_acc: 0.8403
20-03-23 16:58-INFO-
20-03-23 16:58-INFO-Epoch 0, Batch 248, Global step 248:
20-03-23 16:58-INFO-training batch loss: 0.0154; avg_loss: 0.6246
20-03-23 16:58-INFO-training batch acc: 1.0000; avg_acc: 0.8410
20-03-23 16:58-INFO-
20-03-23 16:58-INFO-Epoch 0, Batch 249, Global step 249:
20-03-23 16:58-INFO-training batch loss: 0.0254; avg_loss: 0.6221
20-03-23 16:58-INFO-training batch acc: 0.9922; avg_acc: 0.8416
20-03-23 16:58-INFO-
20-03-23 16:58-INFO-Epoch 0, Batch 250, Global step 250:
20-03-23 16:58-INFO-training batch loss: 0.0392; avg_loss: 0.6198
20-03-23 16:58-INFO-training batch acc: 0.9766; avg_acc: 0.8421
20-03-23 16:58-INFO-
20-03-23 16:58-INFO-Epoch 0, Batch 251, Global step 251:
20-03-23 16:58-INFO-training batch loss: 0.0307; avg_loss: 0.6175
20-03-23 16:58-INFO-training batch acc: 0.9922; avg_acc: 0.8427
20-03-23 16:58-INFO-
20-03-23 16:58-INFO-Epoch 0, Batch 252, Global step 252:
20-03-23 16:58-INFO-training batch loss: 0.0322; avg_loss: 0.6151
20-03-23 16:58-INFO-training batch acc: 0.9844; avg_acc: 0.8433
20-03-23 16:58-INFO-
20-03-23 16:58-INFO-Epoch 0, Batch 253, Global step 253:
20-03-23 16:58-INFO-training batch loss: 0.0190; avg_loss: 0.6128
20-03-23 16:58-INFO-training batch acc: 0.9922; avg_acc: 0.8439
20-03-23 16:58-INFO-
20-03-23 16:58-INFO-Epoch 0, Batch 254, Global step 254:
20-03-23 16:58-INFO-training batch loss: 0.0382; avg_loss: 0.6105
20-03-23 16:58-INFO-training batch acc: 0.9844; avg_acc: 0.8444
20-03-23 16:58-INFO-
20-03-23 16:58-INFO-Epoch 0, Batch 255, Global step 255:
20-03-23 16:58-INFO-training batch loss: 0.0670; avg_loss: 0.6084
20-03-23 16:58-INFO-training batch acc: 0.9922; avg_acc: 0.8450
20-03-23 16:58-INFO-
20-03-23 16:58-INFO-Epoch 0, Batch 256, Global step 256:
20-03-23 16:58-INFO-training batch loss: 0.0621; avg_loss: 0.6063
20-03-23 16:58-INFO-training batch acc: 0.9844; avg_acc: 0.8456
20-03-23 16:58-INFO-
20-03-23 16:58-INFO-Epoch 0, Batch 257, Global step 257:
20-03-23 16:58-INFO-training batch loss: 0.0122; avg_loss: 0.6039
20-03-23 16:58-INFO-training batch acc: 1.0000; avg_acc: 0.8462
20-03-23 16:58-INFO-
20-03-23 16:58-INFO-Epoch 0, Batch 258, Global step 258:
20-03-23 16:58-INFO-training batch loss: 0.0101; avg_loss: 0.6016
20-03-23 16:58-INFO-training batch acc: 1.0000; avg_acc: 0.8467
20-03-23 16:58-INFO-
20-03-23 16:58-INFO-Epoch 0, Batch 259, Global step 259:
20-03-23 16:58-INFO-training batch loss: 0.0255; avg_loss: 0.5994
20-03-23 16:58-INFO-training batch acc: 0.9922; avg_acc: 0.8473
20-03-23 16:58-INFO-
20-03-23 16:58-INFO-Epoch 0, Batch 260, Global step 260:
20-03-23 16:58-INFO-training batch loss: 0.0252; avg_loss: 0.5972
20-03-23 16:58-INFO-training batch acc: 0.9922; avg_acc: 0.8479
20-03-23 16:58-INFO-
20-03-23 16:58-INFO-Epoch 0, Batch 261, Global step 261:
20-03-23 16:58-INFO-training batch loss: 0.0706; avg_loss: 0.5952
20-03-23 16:58-INFO-training batch acc: 0.9922; avg_acc: 0.8484
20-03-23 16:58-INFO-
20-03-23 16:58-INFO-Epoch 0, Batch 262, Global step 262:
20-03-23 16:58-INFO-training batch loss: 0.0232; avg_loss: 0.5930
20-03-23 16:58-INFO-training batch acc: 0.9922; avg_acc: 0.8490
20-03-23 16:58-INFO-
20-03-23 16:58-INFO-Epoch 0, Batch 263, Global step 263:
20-03-23 16:58-INFO-training batch loss: 0.0062; avg_loss: 0.5908
20-03-23 16:58-INFO-training batch acc: 1.0000; avg_acc: 0.8495
20-03-23 16:58-INFO-
20-03-23 16:58-INFO-Epoch 0, Batch 264, Global step 264:
20-03-23 16:58-INFO-training batch loss: 0.0131; avg_loss: 0.5886
20-03-23 16:58-INFO-training batch acc: 0.9922; avg_acc: 0.8501
20-03-23 16:58-INFO-
20-03-23 16:58-INFO-Epoch 0, Batch 265, Global step 265:
20-03-23 16:58-INFO-training batch loss: 0.0038; avg_loss: 0.5864
20-03-23 16:58-INFO-training batch acc: 1.0000; avg_acc: 0.8506
20-03-23 16:58-INFO-
20-03-23 16:58-INFO-Epoch 0, Batch 266, Global step 266:
20-03-23 16:58-INFO-training batch loss: 0.0423; avg_loss: 0.5843
20-03-23 16:58-INFO-training batch acc: 0.9844; avg_acc: 0.8512
20-03-23 16:58-INFO-
20-03-23 16:58-INFO-Epoch 0, Batch 267, Global step 267:
20-03-23 16:58-INFO-training batch loss: 0.0562; avg_loss: 0.5824
20-03-23 16:58-INFO-training batch acc: 0.9766; avg_acc: 0.8516
20-03-23 16:58-INFO-
20-03-23 16:58-INFO-Epoch 0, Batch 268, Global step 268:
20-03-23 16:58-INFO-training batch loss: 0.0111; avg_loss: 0.5802
20-03-23 16:58-INFO-training batch acc: 1.0000; avg_acc: 0.8522
20-03-23 16:58-INFO-
20-03-23 16:58-INFO-Epoch 0, Batch 269, Global step 269:
20-03-23 16:58-INFO-training batch loss: 0.0207; avg_loss: 0.5782
20-03-23 16:58-INFO-training batch acc: 0.9922; avg_acc: 0.8527
20-03-23 16:58-INFO-
20-03-23 16:59-INFO-Epoch 0, Batch 270, Global step 270:
20-03-23 16:59-INFO-training batch loss: 0.0119; avg_loss: 0.5761
20-03-23 16:59-INFO-training batch acc: 1.0000; avg_acc: 0.8532
20-03-23 16:59-INFO-
20-03-23 16:59-INFO-Epoch 0, Batch 271, Global step 271:
20-03-23 16:59-INFO-training batch loss: 0.0870; avg_loss: 0.5742
20-03-23 16:59-INFO-training batch acc: 0.9844; avg_acc: 0.8537
20-03-23 16:59-INFO-
20-03-23 16:59-INFO-Epoch 0, Batch 272, Global step 272:
20-03-23 16:59-INFO-training batch loss: 0.0717; avg_loss: 0.5724
20-03-23 16:59-INFO-training batch acc: 0.9844; avg_acc: 0.8542
20-03-23 16:59-INFO-
20-03-23 16:59-INFO-Epoch 0, Batch 273, Global step 273:
20-03-23 16:59-INFO-training batch loss: 0.0094; avg_loss: 0.5703
20-03-23 16:59-INFO-training batch acc: 1.0000; avg_acc: 0.8547
20-03-23 16:59-INFO-
20-03-23 16:59-INFO-Epoch 0, Batch 274, Global step 274:
20-03-23 16:59-INFO-training batch loss: 0.0291; avg_loss: 0.5684
20-03-23 16:59-INFO-training batch acc: 0.9844; avg_acc: 0.8552
20-03-23 16:59-INFO-
20-03-23 16:59-INFO-Epoch 0, Batch 275, Global step 275:
20-03-23 16:59-INFO-training batch loss: 0.0086; avg_loss: 0.5663
20-03-23 16:59-INFO-training batch acc: 1.0000; avg_acc: 0.8557
20-03-23 16:59-INFO-
20-03-23 16:59-INFO-Epoch 0, Batch 276, Global step 276:
20-03-23 16:59-INFO-training batch loss: 0.0738; avg_loss: 0.5645
20-03-23 16:59-INFO-training batch acc: 0.9922; avg_acc: 0.8562
20-03-23 16:59-INFO-
20-03-23 16:59-INFO-Epoch 0, Batch 277, Global step 277:
20-03-23 16:59-INFO-training batch loss: 0.0910; avg_loss: 0.5628
20-03-23 16:59-INFO-training batch acc: 0.9844; avg_acc: 0.8567
20-03-23 16:59-INFO-
20-03-23 16:59-INFO-Epoch 0, Batch 278, Global step 278:
20-03-23 16:59-INFO-training batch loss: 0.0101; avg_loss: 0.5608
20-03-23 16:59-INFO-training batch acc: 1.0000; avg_acc: 0.8572
20-03-23 16:59-INFO-
20-03-23 16:59-INFO-Epoch 0, Batch 279, Global step 279:
20-03-23 16:59-INFO-training batch loss: 0.0086; avg_loss: 0.5589
20-03-23 16:59-INFO-training batch acc: 1.0000; avg_acc: 0.8577
20-03-23 16:59-INFO-
20-03-23 16:59-INFO-Epoch 0, Batch 280, Global step 280:
20-03-23 16:59-INFO-training batch loss: 0.0530; avg_loss: 0.5571
20-03-23 16:59-INFO-training batch acc: 0.9922; avg_acc: 0.8582
20-03-23 16:59-INFO-
20-03-23 16:59-INFO-Epoch 0, Batch 281, Global step 281:
20-03-23 16:59-INFO-training batch loss: 0.1161; avg_loss: 0.5555
20-03-23 16:59-INFO-training batch acc: 0.9766; avg_acc: 0.8586
20-03-23 16:59-INFO-
20-03-23 16:59-INFO-Epoch 0, Batch 282, Global step 282:
20-03-23 16:59-INFO-training batch loss: 0.0181; avg_loss: 0.5536
20-03-23 16:59-INFO-training batch acc: 1.0000; avg_acc: 0.8591
20-03-23 16:59-INFO-
20-03-23 16:59-INFO-Epoch 0, Batch 283, Global step 283:
20-03-23 16:59-INFO-training batch loss: 0.0216; avg_loss: 0.5517
20-03-23 16:59-INFO-training batch acc: 1.0000; avg_acc: 0.8596
20-03-23 16:59-INFO-
20-03-23 16:59-INFO-Epoch 0, Batch 284, Global step 284:
20-03-23 16:59-INFO-training batch loss: 0.0224; avg_loss: 0.5498
20-03-23 16:59-INFO-training batch acc: 1.0000; avg_acc: 0.8601
20-03-23 16:59-INFO-
20-03-23 16:59-INFO-Epoch 0, training batch loss: 0.0224; avg_loss: 0.5498
20-03-23 16:59-INFO-Epoch 0, training batch accuracy: 1.0000; avg_accuracy: 0.8601
20-03-23 16:59-INFO-
20-03-23 16:59-INFO-Epoch 0, evaluating batch loss: 0.7833; avg_loss: 0.3162
20-03-23 16:59-INFO-Epoch 0, evaluating batch accuracy: 0.8864; avg_accuracy: 0.9429
20-03-23 16:59-INFO-
20-03-23 16:59-INFO-Epoch 1, Batch 1, Global step 285:
20-03-23 16:59-INFO-training batch loss: 0.0307; avg_loss: 0.0307
20-03-23 16:59-INFO-training batch acc: 0.9922; avg_acc: 0.9922
20-03-23 16:59-INFO-
20-03-23 16:59-INFO-Epoch 1, Batch 2, Global step 286:
20-03-23 16:59-INFO-training batch loss: 0.1078; avg_loss: 0.0692
20-03-23 16:59-INFO-training batch acc: 0.9766; avg_acc: 0.9844
20-03-23 16:59-INFO-
20-03-23 16:59-INFO-Epoch 1, Batch 3, Global step 287:
20-03-23 16:59-INFO-training batch loss: 0.0233; avg_loss: 0.0539
20-03-23 16:59-INFO-training batch acc: 0.9922; avg_acc: 0.9870
20-03-23 16:59-INFO-
20-03-23 16:59-INFO-Epoch 1, Batch 4, Global step 288:
20-03-23 16:59-INFO-training batch loss: 0.0123; avg_loss: 0.0435
20-03-23 16:59-INFO-training batch acc: 1.0000; avg_acc: 0.9902
20-03-23 16:59-INFO-
20-03-23 16:59-INFO-Epoch 1, Batch 5, Global step 289:
20-03-23 16:59-INFO-training batch loss: 0.0140; avg_loss: 0.0376
20-03-23 16:59-INFO-training batch acc: 1.0000; avg_acc: 0.9922
20-03-23 16:59-INFO-
20-03-23 16:59-INFO-Epoch 1, Batch 6, Global step 290:
20-03-23 16:59-INFO-training batch loss: 0.0135; avg_loss: 0.0336
20-03-23 16:59-INFO-training batch acc: 1.0000; avg_acc: 0.9935
20-03-23 16:59-INFO-
20-03-23 16:59-INFO-Epoch 1, Batch 7, Global step 291:
20-03-23 16:59-INFO-training batch loss: 0.0103; avg_loss: 0.0302
20-03-23 16:59-INFO-training batch acc: 1.0000; avg_acc: 0.9944
20-03-23 16:59-INFO-
20-03-23 17:00-INFO-Epoch 1, Batch 8, Global step 292:
20-03-23 17:00-INFO-training batch loss: 0.0144; avg_loss: 0.0283
20-03-23 17:00-INFO-training batch acc: 1.0000; avg_acc: 0.9951
20-03-23 17:00-INFO-
20-03-23 17:00-INFO-Epoch 1, Batch 9, Global step 293:
20-03-23 17:00-INFO-training batch loss: 0.0116; avg_loss: 0.0264
20-03-23 17:00-INFO-training batch acc: 1.0000; avg_acc: 0.9957
20-03-23 17:00-INFO-
20-03-23 17:00-INFO-Epoch 1, Batch 10, Global step 294:
20-03-23 17:00-INFO-training batch loss: 0.0026; avg_loss: 0.0240
20-03-23 17:00-INFO-training batch acc: 1.0000; avg_acc: 0.9961
20-03-23 17:00-INFO-
20-03-23 17:00-INFO-Epoch 1, Batch 11, Global step 295:
20-03-23 17:00-INFO-training batch loss: 0.0703; avg_loss: 0.0282
20-03-23 17:00-INFO-training batch acc: 0.9844; avg_acc: 0.9950
20-03-23 17:00-INFO-
20-03-23 17:00-INFO-Epoch 1, Batch 12, Global step 296:
20-03-23 17:00-INFO-training batch loss: 0.0125; avg_loss: 0.0269
20-03-23 17:00-INFO-training batch acc: 0.9922; avg_acc: 0.9948
20-03-23 17:00-INFO-
20-03-23 17:00-INFO-Epoch 1, Batch 13, Global step 297:
20-03-23 17:00-INFO-training batch loss: 0.0603; avg_loss: 0.0295
20-03-23 17:00-INFO-training batch acc: 0.9922; avg_acc: 0.9946
20-03-23 17:00-INFO-
20-03-23 17:00-INFO-Epoch 1, Batch 14, Global step 298:
20-03-23 17:00-INFO-training batch loss: 0.0954; avg_loss: 0.0342
20-03-23 17:00-INFO-training batch acc: 0.9844; avg_acc: 0.9939
20-03-23 17:00-INFO-
20-03-23 17:00-INFO-Epoch 1, Batch 15, Global step 299:
20-03-23 17:00-INFO-training batch loss: 0.0615; avg_loss: 0.0360
20-03-23 17:00-INFO-training batch acc: 0.9922; avg_acc: 0.9938
20-03-23 17:00-INFO-
20-03-23 17:00-INFO-Epoch 1, Batch 16, Global step 300:
20-03-23 17:00-INFO-training batch loss: 0.0731; avg_loss: 0.0383
20-03-23 17:00-INFO-training batch acc: 0.9844; avg_acc: 0.9932
20-03-23 17:00-INFO-
20-03-23 17:00-INFO-Epoch 1, Batch 17, Global step 301:
20-03-23 17:00-INFO-training batch loss: 0.0447; avg_loss: 0.0387
20-03-23 17:00-INFO-training batch acc: 0.9922; avg_acc: 0.9931
20-03-23 17:00-INFO-
20-03-23 17:00-INFO-Epoch 1, Batch 18, Global step 302:
20-03-23 17:00-INFO-training batch loss: 0.0225; avg_loss: 0.0378
20-03-23 17:00-INFO-training batch acc: 0.9922; avg_acc: 0.9931
20-03-23 17:00-INFO-
20-03-23 17:00-INFO-Epoch 1, Batch 19, Global step 303:
20-03-23 17:00-INFO-training batch loss: 0.0211; avg_loss: 0.0369
20-03-23 17:00-INFO-training batch acc: 0.9922; avg_acc: 0.9930
20-03-23 17:00-INFO-
20-03-23 17:00-INFO-Epoch 1, Batch 20, Global step 304:
20-03-23 17:00-INFO-training batch loss: 0.0104; avg_loss: 0.0356
20-03-23 17:00-INFO-training batch acc: 1.0000; avg_acc: 0.9934
20-03-23 17:00-INFO-
20-03-23 17:00-INFO-Epoch 1, Batch 21, Global step 305:
20-03-23 17:00-INFO-training batch loss: 0.0110; avg_loss: 0.0344
20-03-23 17:00-INFO-training batch acc: 1.0000; avg_acc: 0.9937
20-03-23 17:00-INFO-
20-03-23 17:00-INFO-Epoch 1, Batch 22, Global step 306:
20-03-23 17:00-INFO-training batch loss: 0.0171; avg_loss: 0.0336
20-03-23 17:00-INFO-training batch acc: 1.0000; avg_acc: 0.9940
20-03-23 17:00-INFO-
20-03-23 17:00-INFO-Epoch 1, Batch 23, Global step 307:
20-03-23 17:00-INFO-training batch loss: 0.0255; avg_loss: 0.0333
20-03-23 17:00-INFO-training batch acc: 0.9844; avg_acc: 0.9935
20-03-23 17:00-INFO-
20-03-23 17:00-INFO-Epoch 1, Batch 24, Global step 308:
20-03-23 17:00-INFO-training batch loss: 0.0081; avg_loss: 0.0322
20-03-23 17:00-INFO-training batch acc: 1.0000; avg_acc: 0.9938
20-03-23 17:00-INFO-
20-03-23 17:00-INFO-Epoch 1, Batch 25, Global step 309:
20-03-23 17:00-INFO-training batch loss: 0.0084; avg_loss: 0.0313
20-03-23 17:00-INFO-training batch acc: 1.0000; avg_acc: 0.9941
20-03-23 17:00-INFO-
20-03-23 17:00-INFO-Epoch 1, Batch 26, Global step 310:
20-03-23 17:00-INFO-training batch loss: 0.0155; avg_loss: 0.0307
20-03-23 17:00-INFO-training batch acc: 0.9922; avg_acc: 0.9940
20-03-23 17:00-INFO-
20-03-23 17:00-INFO-Epoch 1, Batch 27, Global step 311:
20-03-23 17:00-INFO-training batch loss: 0.0195; avg_loss: 0.0303
20-03-23 17:00-INFO-training batch acc: 0.9922; avg_acc: 0.9939
20-03-23 17:00-INFO-
20-03-23 17:00-INFO-Epoch 1, Batch 28, Global step 312:
20-03-23 17:00-INFO-training batch loss: 0.0125; avg_loss: 0.0296
20-03-23 17:00-INFO-training batch acc: 1.0000; avg_acc: 0.9941
20-03-23 17:00-INFO-
20-03-23 17:00-INFO-Epoch 1, Batch 29, Global step 313:
20-03-23 17:00-INFO-training batch loss: 0.0287; avg_loss: 0.0296
20-03-23 17:00-INFO-training batch acc: 0.9844; avg_acc: 0.9938
20-03-23 17:00-INFO-
20-03-23 17:00-INFO-Epoch 1, Batch 30, Global step 314:
20-03-23 17:00-INFO-training batch loss: 0.0180; avg_loss: 0.0292
20-03-23 17:00-INFO-training batch acc: 0.9922; avg_acc: 0.9938
20-03-23 17:00-INFO-
20-03-23 17:00-INFO-Epoch 1, Batch 31, Global step 315:
20-03-23 17:00-INFO-training batch loss: 0.0096; avg_loss: 0.0286
20-03-23 17:00-INFO-training batch acc: 1.0000; avg_acc: 0.9940
20-03-23 17:00-INFO-
20-03-23 17:00-INFO-Epoch 1, Batch 32, Global step 316:
20-03-23 17:00-INFO-training batch loss: 0.0053; avg_loss: 0.0279
20-03-23 17:00-INFO-training batch acc: 1.0000; avg_acc: 0.9941
20-03-23 17:00-INFO-
20-03-23 17:00-INFO-Epoch 1, Batch 33, Global step 317:
20-03-23 17:00-INFO-training batch loss: 0.0098; avg_loss: 0.0273
20-03-23 17:00-INFO-training batch acc: 1.0000; avg_acc: 0.9943
20-03-23 17:00-INFO-
20-03-23 17:01-INFO-Epoch 1, Batch 34, Global step 318:
20-03-23 17:01-INFO-training batch loss: 0.0422; avg_loss: 0.0277
20-03-23 17:01-INFO-training batch acc: 0.9844; avg_acc: 0.9940
20-03-23 17:01-INFO-
20-03-23 17:01-INFO-Epoch 1, Batch 35, Global step 319:
20-03-23 17:01-INFO-training batch loss: 0.0134; avg_loss: 0.0273
20-03-23 17:01-INFO-training batch acc: 0.9922; avg_acc: 0.9940
20-03-23 17:01-INFO-
20-03-23 17:01-INFO-Epoch 1, Batch 36, Global step 320:
20-03-23 17:01-INFO-training batch loss: 0.0400; avg_loss: 0.0277
20-03-23 17:01-INFO-training batch acc: 0.9844; avg_acc: 0.9937
20-03-23 17:01-INFO-
20-03-23 17:01-INFO-Epoch 1, Batch 37, Global step 321:
20-03-23 17:01-INFO-training batch loss: 0.0101; avg_loss: 0.0272
20-03-23 17:01-INFO-training batch acc: 1.0000; avg_acc: 0.9939
20-03-23 17:01-INFO-
20-03-23 17:01-INFO-Epoch 1, Batch 38, Global step 322:
20-03-23 17:01-INFO-training batch loss: 0.0132; avg_loss: 0.0268
20-03-23 17:01-INFO-training batch acc: 0.9922; avg_acc: 0.9938
20-03-23 17:01-INFO-
20-03-23 17:01-INFO-Epoch 1, Batch 39, Global step 323:
20-03-23 17:01-INFO-training batch loss: 0.0068; avg_loss: 0.0263
20-03-23 17:01-INFO-training batch acc: 1.0000; avg_acc: 0.9940
20-03-23 17:01-INFO-
20-03-23 17:01-INFO-Epoch 1, Batch 40, Global step 324:
20-03-23 17:01-INFO-training batch loss: 0.0134; avg_loss: 0.0260
20-03-23 17:01-INFO-training batch acc: 1.0000; avg_acc: 0.9941
20-03-23 17:01-INFO-
20-03-23 17:01-INFO-Epoch 1, Batch 41, Global step 325:
20-03-23 17:01-INFO-training batch loss: 0.0101; avg_loss: 0.0256
20-03-23 17:01-INFO-training batch acc: 1.0000; avg_acc: 0.9943
20-03-23 17:01-INFO-
20-03-23 17:01-INFO-Epoch 1, Batch 42, Global step 326:
20-03-23 17:01-INFO-training batch loss: 0.0180; avg_loss: 0.0254
20-03-23 17:01-INFO-training batch acc: 0.9922; avg_acc: 0.9942
20-03-23 17:01-INFO-
20-03-23 17:01-INFO-Epoch 1, Batch 43, Global step 327:
20-03-23 17:01-INFO-training batch loss: 0.0111; avg_loss: 0.0251
20-03-23 17:01-INFO-training batch acc: 1.0000; avg_acc: 0.9944
20-03-23 17:01-INFO-
20-03-23 17:01-INFO-Epoch 1, Batch 44, Global step 328:
20-03-23 17:01-INFO-training batch loss: 0.0511; avg_loss: 0.0257
20-03-23 17:01-INFO-training batch acc: 0.9766; avg_acc: 0.9940
20-03-23 17:01-INFO-
20-03-23 17:01-INFO-Epoch 1, Batch 45, Global step 329:
20-03-23 17:01-INFO-training batch loss: 0.0370; avg_loss: 0.0260
20-03-23 17:01-INFO-training batch acc: 0.9844; avg_acc: 0.9938
20-03-23 17:01-INFO-
20-03-23 17:01-INFO-Epoch 1, Batch 46, Global step 330:
20-03-23 17:01-INFO-training batch loss: 0.0204; avg_loss: 0.0258
20-03-23 17:01-INFO-training batch acc: 0.9922; avg_acc: 0.9937
20-03-23 17:01-INFO-
20-03-23 17:01-INFO-Epoch 1, Batch 47, Global step 331:
20-03-23 17:01-INFO-training batch loss: 0.0239; avg_loss: 0.0258
20-03-23 17:01-INFO-training batch acc: 0.9922; avg_acc: 0.9937
20-03-23 17:01-INFO-
20-03-23 17:01-INFO-Epoch 1, Batch 48, Global step 332:
20-03-23 17:01-INFO-training batch loss: 0.0091; avg_loss: 0.0254
20-03-23 17:01-INFO-training batch acc: 1.0000; avg_acc: 0.9938
20-03-23 17:01-INFO-
20-03-23 17:01-INFO-Epoch 1, Batch 49, Global step 333:
20-03-23 17:01-INFO-training batch loss: 0.0484; avg_loss: 0.0259
20-03-23 17:01-INFO-training batch acc: 0.9844; avg_acc: 0.9936
20-03-23 17:01-INFO-
20-03-23 17:01-INFO-Epoch 1, Batch 50, Global step 334:
20-03-23 17:01-INFO-training batch loss: 0.0041; avg_loss: 0.0255
20-03-23 17:01-INFO-training batch acc: 1.0000; avg_acc: 0.9938
20-03-23 17:01-INFO-
20-03-23 17:01-INFO-Epoch 1, Batch 51, Global step 335:
20-03-23 17:01-INFO-training batch loss: 0.0166; avg_loss: 0.0253
20-03-23 17:01-INFO-training batch acc: 0.9922; avg_acc: 0.9937
20-03-23 17:01-INFO-
20-03-23 17:01-INFO-Epoch 1, Batch 52, Global step 336:
20-03-23 17:01-INFO-training batch loss: 0.0356; avg_loss: 0.0255
20-03-23 17:01-INFO-training batch acc: 0.9922; avg_acc: 0.9937
20-03-23 17:01-INFO-
20-03-23 17:01-INFO-Epoch 1, Batch 53, Global step 337:
20-03-23 17:01-INFO-training batch loss: 0.0257; avg_loss: 0.0255
20-03-23 17:01-INFO-training batch acc: 0.9922; avg_acc: 0.9937
20-03-23 17:01-INFO-
20-03-23 17:01-INFO-Epoch 1, Batch 54, Global step 338:
20-03-23 17:01-INFO-training batch loss: 0.0100; avg_loss: 0.0252
20-03-23 17:01-INFO-training batch acc: 1.0000; avg_acc: 0.9938
20-03-23 17:01-INFO-
20-03-23 17:01-INFO-Epoch 1, Batch 55, Global step 339:
20-03-23 17:01-INFO-training batch loss: 0.0041; avg_loss: 0.0248
20-03-23 17:01-INFO-training batch acc: 1.0000; avg_acc: 0.9939
20-03-23 17:01-INFO-
20-03-23 17:01-INFO-Epoch 1, Batch 56, Global step 340:
20-03-23 17:01-INFO-training batch loss: 0.0066; avg_loss: 0.0245
20-03-23 17:01-INFO-training batch acc: 1.0000; avg_acc: 0.9940
20-03-23 17:01-INFO-
20-03-23 17:01-INFO-Epoch 1, Batch 57, Global step 341:
20-03-23 17:01-INFO-training batch loss: 0.0026; avg_loss: 0.0241
20-03-23 17:01-INFO-training batch acc: 1.0000; avg_acc: 0.9941
20-03-23 17:01-INFO-
20-03-23 17:01-INFO-Epoch 1, Batch 58, Global step 342:
20-03-23 17:01-INFO-training batch loss: 0.0034; avg_loss: 0.0238
20-03-23 17:01-INFO-training batch acc: 1.0000; avg_acc: 0.9942
20-03-23 17:01-INFO-
20-03-23 17:01-INFO-Epoch 1, Batch 59, Global step 343:
20-03-23 17:01-INFO-training batch loss: 0.0060; avg_loss: 0.0235
20-03-23 17:01-INFO-training batch acc: 1.0000; avg_acc: 0.9943
20-03-23 17:01-INFO-
20-03-23 17:01-INFO-Epoch 1, Batch 60, Global step 344:
20-03-23 17:01-INFO-training batch loss: 0.0106; avg_loss: 0.0232
20-03-23 17:01-INFO-training batch acc: 0.9922; avg_acc: 0.9943
20-03-23 17:01-INFO-
20-03-23 17:02-INFO-Epoch 1, Batch 61, Global step 345:
20-03-23 17:02-INFO-training batch loss: 0.0046; avg_loss: 0.0229
20-03-23 17:02-INFO-training batch acc: 1.0000; avg_acc: 0.9944
20-03-23 17:02-INFO-
20-03-23 17:02-INFO-Epoch 1, Batch 62, Global step 346:
20-03-23 17:02-INFO-training batch loss: 0.0215; avg_loss: 0.0229
20-03-23 17:02-INFO-training batch acc: 0.9922; avg_acc: 0.9943
20-03-23 17:02-INFO-
20-03-23 17:02-INFO-Epoch 1, Batch 63, Global step 347:
20-03-23 17:02-INFO-training batch loss: 0.0323; avg_loss: 0.0231
20-03-23 17:02-INFO-training batch acc: 0.9922; avg_acc: 0.9943
20-03-23 17:02-INFO-
20-03-23 17:02-INFO-Epoch 1, Batch 64, Global step 348:
20-03-23 17:02-INFO-training batch loss: 0.0044; avg_loss: 0.0228
20-03-23 17:02-INFO-training batch acc: 1.0000; avg_acc: 0.9944
20-03-23 17:02-INFO-
20-03-23 17:02-INFO-Epoch 1, Batch 65, Global step 349:
20-03-23 17:02-INFO-training batch loss: 0.0046; avg_loss: 0.0225
20-03-23 17:02-INFO-training batch acc: 1.0000; avg_acc: 0.9945
20-03-23 17:02-INFO-
20-03-23 17:02-INFO-Epoch 1, Batch 66, Global step 350:
20-03-23 17:02-INFO-training batch loss: 0.0064; avg_loss: 0.0223
20-03-23 17:02-INFO-training batch acc: 1.0000; avg_acc: 0.9946
20-03-23 17:02-INFO-
20-03-23 17:02-INFO-Epoch 1, Batch 67, Global step 351:
20-03-23 17:02-INFO-training batch loss: 0.0033; avg_loss: 0.0220
20-03-23 17:02-INFO-training batch acc: 1.0000; avg_acc: 0.9946
20-03-23 17:02-INFO-
20-03-23 17:02-INFO-Epoch 1, Batch 68, Global step 352:
20-03-23 17:02-INFO-training batch loss: 0.0025; avg_loss: 0.0217
20-03-23 17:02-INFO-training batch acc: 1.0000; avg_acc: 0.9947
20-03-23 17:02-INFO-
20-03-23 17:02-INFO-Epoch 1, Batch 69, Global step 353:
20-03-23 17:02-INFO-training batch loss: 0.0176; avg_loss: 0.0216
20-03-23 17:02-INFO-training batch acc: 0.9922; avg_acc: 0.9947
20-03-23 17:02-INFO-
20-03-23 17:02-INFO-Epoch 1, Batch 70, Global step 354:
20-03-23 17:02-INFO-training batch loss: 0.0271; avg_loss: 0.0217
20-03-23 17:02-INFO-training batch acc: 0.9922; avg_acc: 0.9946
20-03-23 17:02-INFO-
20-03-23 17:02-INFO-Epoch 1, Batch 71, Global step 355:
20-03-23 17:02-INFO-training batch loss: 0.0203; avg_loss: 0.0217
20-03-23 17:02-INFO-training batch acc: 0.9922; avg_acc: 0.9946
20-03-23 17:02-INFO-
20-03-23 17:02-INFO-Epoch 1, Batch 72, Global step 356:
20-03-23 17:02-INFO-training batch loss: 0.0025; avg_loss: 0.0214
20-03-23 17:02-INFO-training batch acc: 1.0000; avg_acc: 0.9947
20-03-23 17:02-INFO-
20-03-23 17:02-INFO-Epoch 1, Batch 73, Global step 357:
20-03-23 17:02-INFO-training batch loss: 0.0028; avg_loss: 0.0212
20-03-23 17:02-INFO-training batch acc: 1.0000; avg_acc: 0.9948
20-03-23 17:02-INFO-
20-03-23 17:02-INFO-Epoch 1, Batch 74, Global step 358:
20-03-23 17:02-INFO-training batch loss: 0.0039; avg_loss: 0.0209
20-03-23 17:02-INFO-training batch acc: 1.0000; avg_acc: 0.9948
20-03-23 17:02-INFO-
20-03-23 17:02-INFO-Epoch 1, Batch 75, Global step 359:
20-03-23 17:02-INFO-training batch loss: 0.0197; avg_loss: 0.0209
20-03-23 17:02-INFO-training batch acc: 0.9844; avg_acc: 0.9947
20-03-23 17:02-INFO-
20-03-23 17:02-INFO-Epoch 1, Batch 76, Global step 360:
20-03-23 17:02-INFO-training batch loss: 0.0021; avg_loss: 0.0207
20-03-23 17:02-INFO-training batch acc: 1.0000; avg_acc: 0.9948
20-03-23 17:02-INFO-
20-03-23 17:02-INFO-Epoch 1, Batch 77, Global step 361:
20-03-23 17:02-INFO-training batch loss: 0.0070; avg_loss: 0.0205
20-03-23 17:02-INFO-training batch acc: 1.0000; avg_acc: 0.9948
20-03-23 17:02-INFO-
20-03-23 17:02-INFO-Epoch 1, Batch 78, Global step 362:
20-03-23 17:02-INFO-training batch loss: 0.0036; avg_loss: 0.0203
20-03-23 17:02-INFO-training batch acc: 1.0000; avg_acc: 0.9949
20-03-23 17:02-INFO-
20-03-23 17:02-INFO-Epoch 1, Batch 79, Global step 363:
20-03-23 17:02-INFO-training batch loss: 0.0011; avg_loss: 0.0200
20-03-23 17:02-INFO-training batch acc: 1.0000; avg_acc: 0.9950
20-03-23 17:02-INFO-
20-03-23 17:02-INFO-Epoch 1, Batch 80, Global step 364:
20-03-23 17:02-INFO-training batch loss: 0.0011; avg_loss: 0.0198
20-03-23 17:02-INFO-training batch acc: 1.0000; avg_acc: 0.9950
20-03-23 17:02-INFO-
20-03-23 17:02-INFO-Epoch 1, Batch 81, Global step 365:
20-03-23 17:02-INFO-training batch loss: 0.0084; avg_loss: 0.0197
20-03-23 17:02-INFO-training batch acc: 0.9922; avg_acc: 0.9950
20-03-23 17:02-INFO-
20-03-23 17:02-INFO-Epoch 1, Batch 82, Global step 366:
20-03-23 17:02-INFO-training batch loss: 0.0039; avg_loss: 0.0195
20-03-23 17:02-INFO-training batch acc: 1.0000; avg_acc: 0.9950
20-03-23 17:02-INFO-
20-03-23 17:02-INFO-Epoch 1, Batch 83, Global step 367:
20-03-23 17:02-INFO-training batch loss: 0.0098; avg_loss: 0.0193
20-03-23 17:02-INFO-training batch acc: 0.9922; avg_acc: 0.9950
20-03-23 17:02-INFO-
20-03-23 17:02-INFO-Epoch 1, Batch 84, Global step 368:
20-03-23 17:02-INFO-training batch loss: 0.0068; avg_loss: 0.0192
20-03-23 17:02-INFO-training batch acc: 1.0000; avg_acc: 0.9951
20-03-23 17:02-INFO-
20-03-23 17:02-INFO-Epoch 1, Batch 85, Global step 369:
20-03-23 17:02-INFO-training batch loss: 0.0025; avg_loss: 0.0190
20-03-23 17:02-INFO-training batch acc: 1.0000; avg_acc: 0.9951
20-03-23 17:02-INFO-
20-03-23 17:02-INFO-Epoch 1, Batch 86, Global step 370:
20-03-23 17:02-INFO-training batch loss: 0.0050; avg_loss: 0.0188
20-03-23 17:02-INFO-training batch acc: 1.0000; avg_acc: 0.9952
20-03-23 17:02-INFO-
20-03-23 17:03-INFO-Epoch 1, Batch 87, Global step 371:
20-03-23 17:03-INFO-training batch loss: 0.0081; avg_loss: 0.0187
20-03-23 17:03-INFO-training batch acc: 1.0000; avg_acc: 0.9952
20-03-23 17:03-INFO-
20-03-23 17:03-INFO-Epoch 1, Batch 88, Global step 372:
20-03-23 17:03-INFO-training batch loss: 0.0020; avg_loss: 0.0185
20-03-23 17:03-INFO-training batch acc: 1.0000; avg_acc: 0.9953
20-03-23 17:03-INFO-
20-03-23 17:03-INFO-Epoch 1, Batch 89, Global step 373:
20-03-23 17:03-INFO-training batch loss: 0.0031; avg_loss: 0.0183
20-03-23 17:03-INFO-training batch acc: 1.0000; avg_acc: 0.9953
20-03-23 17:03-INFO-
20-03-23 17:03-INFO-Epoch 1, Batch 90, Global step 374:
20-03-23 17:03-INFO-training batch loss: 0.0121; avg_loss: 0.0183
20-03-23 17:03-INFO-training batch acc: 0.9922; avg_acc: 0.9953
20-03-23 17:03-INFO-
20-03-23 17:03-INFO-Epoch 1, Batch 91, Global step 375:
20-03-23 17:03-INFO-training batch loss: 0.0086; avg_loss: 0.0182
20-03-23 17:03-INFO-training batch acc: 1.0000; avg_acc: 0.9954
20-03-23 17:03-INFO-
20-03-23 17:03-INFO-Epoch 1, Batch 92, Global step 376:
20-03-23 17:03-INFO-training batch loss: 0.0019; avg_loss: 0.0180
20-03-23 17:03-INFO-training batch acc: 1.0000; avg_acc: 0.9954
20-03-23 17:03-INFO-
20-03-23 17:03-INFO-Epoch 1, Batch 93, Global step 377:
20-03-23 17:03-INFO-training batch loss: 0.0077; avg_loss: 0.0179
20-03-23 17:03-INFO-training batch acc: 0.9922; avg_acc: 0.9954
20-03-23 17:03-INFO-
20-03-23 17:03-INFO-Epoch 1, Batch 94, Global step 378:
20-03-23 17:03-INFO-training batch loss: 0.0021; avg_loss: 0.0177
20-03-23 17:03-INFO-training batch acc: 1.0000; avg_acc: 0.9954
20-03-23 17:03-INFO-
20-03-23 17:03-INFO-Epoch 1, Batch 95, Global step 379:
20-03-23 17:03-INFO-training batch loss: 0.0025; avg_loss: 0.0176
20-03-23 17:03-INFO-training batch acc: 1.0000; avg_acc: 0.9955
20-03-23 17:03-INFO-
20-03-23 17:03-INFO-Epoch 1, Batch 96, Global step 380:
20-03-23 17:03-INFO-training batch loss: 0.0010; avg_loss: 0.0174
20-03-23 17:03-INFO-training batch acc: 1.0000; avg_acc: 0.9955
20-03-23 17:03-INFO-
20-03-23 17:03-INFO-Epoch 1, Batch 97, Global step 381:
20-03-23 17:03-INFO-training batch loss: 0.0021; avg_loss: 0.0172
20-03-23 17:03-INFO-training batch acc: 1.0000; avg_acc: 0.9956
20-03-23 17:03-INFO-
20-03-23 17:03-INFO-Epoch 1, Batch 98, Global step 382:
20-03-23 17:03-INFO-training batch loss: 0.0181; avg_loss: 0.0172
20-03-23 17:03-INFO-training batch acc: 0.9922; avg_acc: 0.9955
20-03-23 17:03-INFO-
20-03-23 17:03-INFO-Epoch 1, Batch 99, Global step 383:
20-03-23 17:03-INFO-training batch loss: 0.0018; avg_loss: 0.0171
20-03-23 17:03-INFO-training batch acc: 1.0000; avg_acc: 0.9956
20-03-23 17:03-INFO-
20-03-23 17:03-INFO-Epoch 1, Batch 100, Global step 384:
20-03-23 17:03-INFO-training batch loss: 0.0003; avg_loss: 0.0169
20-03-23 17:03-INFO-training batch acc: 1.0000; avg_acc: 0.9956
20-03-23 17:03-INFO-
20-03-23 17:03-INFO-Epoch 1, Batch 101, Global step 385:
20-03-23 17:03-INFO-training batch loss: 0.0019; avg_loss: 0.0168
20-03-23 17:03-INFO-training batch acc: 1.0000; avg_acc: 0.9957
20-03-23 17:03-INFO-
20-03-23 17:03-INFO-Epoch 1, Batch 102, Global step 386:
20-03-23 17:03-INFO-training batch loss: 0.0023; avg_loss: 0.0166
20-03-23 17:03-INFO-training batch acc: 1.0000; avg_acc: 0.9957
20-03-23 17:03-INFO-
20-03-23 17:03-INFO-Epoch 1, Batch 103, Global step 387:
20-03-23 17:03-INFO-training batch loss: 0.0022; avg_loss: 0.0165
20-03-23 17:03-INFO-training batch acc: 1.0000; avg_acc: 0.9958
20-03-23 17:03-INFO-
20-03-23 17:03-INFO-Epoch 1, Batch 104, Global step 388:
20-03-23 17:03-INFO-training batch loss: 0.0100; avg_loss: 0.0164
20-03-23 17:03-INFO-training batch acc: 0.9922; avg_acc: 0.9957
20-03-23 17:03-INFO-
20-03-23 17:03-INFO-Epoch 1, Batch 105, Global step 389:
20-03-23 17:03-INFO-training batch loss: 0.0085; avg_loss: 0.0163
20-03-23 17:03-INFO-training batch acc: 1.0000; avg_acc: 0.9958
20-03-23 17:03-INFO-
20-03-23 17:03-INFO-Epoch 1, Batch 106, Global step 390:
20-03-23 17:03-INFO-training batch loss: 0.0016; avg_loss: 0.0162
20-03-23 17:03-INFO-training batch acc: 1.0000; avg_acc: 0.9958
20-03-23 17:03-INFO-
20-03-23 17:03-INFO-Epoch 1, Batch 107, Global step 391:
20-03-23 17:03-INFO-training batch loss: 0.0099; avg_loss: 0.0161
20-03-23 17:03-INFO-training batch acc: 1.0000; avg_acc: 0.9958
20-03-23 17:03-INFO-
20-03-23 17:03-INFO-Epoch 1, Batch 108, Global step 392:
20-03-23 17:03-INFO-training batch loss: 0.0037; avg_loss: 0.0160
20-03-23 17:03-INFO-training batch acc: 1.0000; avg_acc: 0.9959
20-03-23 17:03-INFO-
20-03-23 17:03-INFO-Epoch 1, Batch 109, Global step 393:
20-03-23 17:03-INFO-training batch loss: 0.0016; avg_loss: 0.0159
20-03-23 17:03-INFO-training batch acc: 1.0000; avg_acc: 0.9959
20-03-23 17:03-INFO-
20-03-23 17:03-INFO-Epoch 1, Batch 110, Global step 394:
20-03-23 17:03-INFO-training batch loss: 0.0009; avg_loss: 0.0158
20-03-23 17:03-INFO-training batch acc: 1.0000; avg_acc: 0.9960
20-03-23 17:03-INFO-
20-03-23 17:03-INFO-Epoch 1, Batch 111, Global step 395:
20-03-23 17:03-INFO-training batch loss: 0.0098; avg_loss: 0.0157
20-03-23 17:03-INFO-training batch acc: 1.0000; avg_acc: 0.9960
20-03-23 17:03-INFO-
20-03-23 17:03-INFO-Epoch 1, Batch 112, Global step 396:
20-03-23 17:03-INFO-training batch loss: 0.0048; avg_loss: 0.0156
20-03-23 17:03-INFO-training batch acc: 1.0000; avg_acc: 0.9960
20-03-23 17:03-INFO-
20-03-23 17:04-INFO-Epoch 1, Batch 113, Global step 397:
20-03-23 17:04-INFO-training batch loss: 0.0026; avg_loss: 0.0155
20-03-23 17:04-INFO-training batch acc: 1.0000; avg_acc: 0.9961
20-03-23 17:04-INFO-
20-03-23 17:04-INFO-Epoch 1, Batch 114, Global step 398:
20-03-23 17:04-INFO-training batch loss: 0.0094; avg_loss: 0.0154
20-03-23 17:04-INFO-training batch acc: 0.9922; avg_acc: 0.9960
20-03-23 17:04-INFO-
20-03-23 17:04-INFO-Epoch 1, Batch 115, Global step 399:
20-03-23 17:04-INFO-training batch loss: 0.0135; avg_loss: 0.0154
20-03-23 17:04-INFO-training batch acc: 0.9922; avg_acc: 0.9960
20-03-23 17:04-INFO-
20-03-23 17:04-INFO-Epoch 1, Batch 116, Global step 400:
20-03-23 17:04-INFO-training batch loss: 0.0011; avg_loss: 0.0153
20-03-23 17:04-INFO-training batch acc: 1.0000; avg_acc: 0.9960
20-03-23 17:04-INFO-
20-03-23 17:04-INFO-Epoch 1, Batch 117, Global step 401:
20-03-23 17:04-INFO-training batch loss: 0.0047; avg_loss: 0.0152
20-03-23 17:04-INFO-training batch acc: 1.0000; avg_acc: 0.9961
20-03-23 17:04-INFO-
20-03-23 17:04-INFO-Epoch 1, Batch 118, Global step 402:
20-03-23 17:04-INFO-training batch loss: 0.0081; avg_loss: 0.0152
20-03-23 17:04-INFO-training batch acc: 1.0000; avg_acc: 0.9961
20-03-23 17:04-INFO-
20-03-23 17:04-INFO-Epoch 1, Batch 119, Global step 403:
20-03-23 17:04-INFO-training batch loss: 0.0055; avg_loss: 0.0151
20-03-23 17:04-INFO-training batch acc: 1.0000; avg_acc: 0.9961
20-03-23 17:04-INFO-
20-03-23 17:04-INFO-Epoch 1, Batch 120, Global step 404:
20-03-23 17:04-INFO-training batch loss: 0.0506; avg_loss: 0.0154
20-03-23 17:04-INFO-training batch acc: 0.9766; avg_acc: 0.9960
20-03-23 17:04-INFO-
20-03-23 17:04-INFO-Epoch 1, Batch 121, Global step 405:
20-03-23 17:04-INFO-training batch loss: 0.0027; avg_loss: 0.0153
20-03-23 17:04-INFO-training batch acc: 1.0000; avg_acc: 0.9960
20-03-23 17:04-INFO-
20-03-23 17:04-INFO-Epoch 1, Batch 122, Global step 406:
20-03-23 17:04-INFO-training batch loss: 0.0016; avg_loss: 0.0152
20-03-23 17:04-INFO-training batch acc: 1.0000; avg_acc: 0.9960
20-03-23 17:04-INFO-
20-03-23 17:04-INFO-Epoch 1, Batch 123, Global step 407:
20-03-23 17:04-INFO-training batch loss: 0.0208; avg_loss: 0.0152
20-03-23 17:04-INFO-training batch acc: 0.9922; avg_acc: 0.9960
20-03-23 17:04-INFO-
20-03-23 17:04-INFO-Epoch 1, Batch 124, Global step 408:
20-03-23 17:04-INFO-training batch loss: 0.0235; avg_loss: 0.0153
20-03-23 17:04-INFO-training batch acc: 0.9922; avg_acc: 0.9960
20-03-23 17:04-INFO-
20-03-23 17:04-INFO-Epoch 1, Batch 125, Global step 409:
20-03-23 17:04-INFO-training batch loss: 0.0251; avg_loss: 0.0153
20-03-23 17:04-INFO-training batch acc: 0.9922; avg_acc: 0.9959
20-03-23 17:04-INFO-
20-03-23 17:04-INFO-Epoch 1, Batch 126, Global step 410:
20-03-23 17:04-INFO-training batch loss: 0.0174; avg_loss: 0.0154
20-03-23 17:04-INFO-training batch acc: 0.9922; avg_acc: 0.9959
20-03-23 17:04-INFO-
20-03-23 17:04-INFO-Epoch 1, Batch 127, Global step 411:
20-03-23 17:04-INFO-training batch loss: 0.0006; avg_loss: 0.0152
20-03-23 17:04-INFO-training batch acc: 1.0000; avg_acc: 0.9959
20-03-23 17:04-INFO-
20-03-23 17:04-INFO-Epoch 1, Batch 128, Global step 412:
20-03-23 17:04-INFO-training batch loss: 0.0010; avg_loss: 0.0151
20-03-23 17:04-INFO-training batch acc: 1.0000; avg_acc: 0.9960
20-03-23 17:04-INFO-
20-03-23 17:04-INFO-Epoch 1, Batch 129, Global step 413:
20-03-23 17:04-INFO-training batch loss: 0.0037; avg_loss: 0.0150
20-03-23 17:04-INFO-training batch acc: 1.0000; avg_acc: 0.9960
20-03-23 17:04-INFO-
20-03-23 17:04-INFO-Epoch 1, Batch 130, Global step 414:
20-03-23 17:04-INFO-training batch loss: 0.0028; avg_loss: 0.0149
20-03-23 17:04-INFO-training batch acc: 1.0000; avg_acc: 0.9960
20-03-23 17:04-INFO-
20-03-23 17:04-INFO-Epoch 1, Batch 131, Global step 415:
20-03-23 17:04-INFO-training batch loss: 0.0150; avg_loss: 0.0150
20-03-23 17:04-INFO-training batch acc: 0.9922; avg_acc: 0.9960
20-03-23 17:04-INFO-
20-03-23 17:04-INFO-Epoch 1, Batch 132, Global step 416:
20-03-23 17:04-INFO-training batch loss: 0.0024; avg_loss: 0.0149
20-03-23 17:04-INFO-training batch acc: 1.0000; avg_acc: 0.9960
20-03-23 17:04-INFO-
20-03-23 17:04-INFO-Epoch 1, Batch 133, Global step 417:
20-03-23 17:04-INFO-training batch loss: 0.0110; avg_loss: 0.0148
20-03-23 17:04-INFO-training batch acc: 1.0000; avg_acc: 0.9961
20-03-23 17:04-INFO-
20-03-23 17:04-INFO-Epoch 1, Batch 134, Global step 418:
20-03-23 17:04-INFO-training batch loss: 0.0046; avg_loss: 0.0148
20-03-23 17:04-INFO-training batch acc: 1.0000; avg_acc: 0.9961
20-03-23 17:04-INFO-
20-03-23 17:04-INFO-Epoch 1, Batch 135, Global step 419:
20-03-23 17:04-INFO-training batch loss: 0.0012; avg_loss: 0.0146
20-03-23 17:04-INFO-training batch acc: 1.0000; avg_acc: 0.9961
20-03-23 17:04-INFO-
20-03-23 17:04-INFO-Epoch 1, Batch 136, Global step 420:
20-03-23 17:04-INFO-training batch loss: 0.0079; avg_loss: 0.0146
20-03-23 17:04-INFO-training batch acc: 1.0000; avg_acc: 0.9962
20-03-23 17:04-INFO-
20-03-23 17:04-INFO-Epoch 1, Batch 137, Global step 421:
20-03-23 17:04-INFO-training batch loss: 0.0037; avg_loss: 0.0145
20-03-23 17:04-INFO-training batch acc: 1.0000; avg_acc: 0.9962
20-03-23 17:04-INFO-
20-03-23 17:04-INFO-Epoch 1, Batch 138, Global step 422:
20-03-23 17:04-INFO-training batch loss: 0.0026; avg_loss: 0.0144
20-03-23 17:04-INFO-training batch acc: 1.0000; avg_acc: 0.9962
20-03-23 17:04-INFO-
20-03-23 17:05-INFO-Epoch 1, Batch 139, Global step 423:
20-03-23 17:05-INFO-training batch loss: 0.0020; avg_loss: 0.0143
20-03-23 17:05-INFO-training batch acc: 1.0000; avg_acc: 0.9962
20-03-23 17:05-INFO-
20-03-23 17:05-INFO-Epoch 1, Batch 140, Global step 424:
20-03-23 17:05-INFO-training batch loss: 0.0056; avg_loss: 0.0143
20-03-23 17:05-INFO-training batch acc: 1.0000; avg_acc: 0.9963
20-03-23 17:05-INFO-
20-03-23 17:05-INFO-Epoch 1, Batch 141, Global step 425:
20-03-23 17:05-INFO-training batch loss: 0.0014; avg_loss: 0.0142
20-03-23 17:05-INFO-training batch acc: 1.0000; avg_acc: 0.9963
20-03-23 17:05-INFO-
20-03-23 17:05-INFO-Epoch 1, Batch 142, Global step 426:
20-03-23 17:05-INFO-training batch loss: 0.0022; avg_loss: 0.0141
20-03-23 17:05-INFO-training batch acc: 1.0000; avg_acc: 0.9963
20-03-23 17:05-INFO-
20-03-23 17:05-INFO-Epoch 1, Batch 143, Global step 427:
20-03-23 17:05-INFO-training batch loss: 0.0049; avg_loss: 0.0140
20-03-23 17:05-INFO-training batch acc: 1.0000; avg_acc: 0.9963
20-03-23 17:05-INFO-
20-03-23 17:05-INFO-Epoch 1, Batch 144, Global step 428:
20-03-23 17:05-INFO-training batch loss: 0.0034; avg_loss: 0.0140
20-03-23 17:05-INFO-training batch acc: 1.0000; avg_acc: 0.9964
20-03-23 17:05-INFO-
20-03-23 17:05-INFO-Epoch 1, Batch 145, Global step 429:
20-03-23 17:05-INFO-training batch loss: 0.0017; avg_loss: 0.0139
20-03-23 17:05-INFO-training batch acc: 1.0000; avg_acc: 0.9964
20-03-23 17:05-INFO-
20-03-23 17:05-INFO-Epoch 1, Batch 146, Global step 430:
20-03-23 17:05-INFO-training batch loss: 0.0007; avg_loss: 0.0138
20-03-23 17:05-INFO-training batch acc: 1.0000; avg_acc: 0.9964
20-03-23 17:05-INFO-
20-03-23 17:05-INFO-Epoch 1, Batch 147, Global step 431:
20-03-23 17:05-INFO-training batch loss: 0.0003; avg_loss: 0.0137
20-03-23 17:05-INFO-training batch acc: 1.0000; avg_acc: 0.9964
20-03-23 17:05-INFO-
20-03-23 17:05-INFO-Epoch 1, Batch 148, Global step 432:
20-03-23 17:05-INFO-training batch loss: 0.0026; avg_loss: 0.0136
20-03-23 17:05-INFO-training batch acc: 1.0000; avg_acc: 0.9965
20-03-23 17:05-INFO-
20-03-23 17:05-INFO-Epoch 1, Batch 149, Global step 433:
20-03-23 17:05-INFO-training batch loss: 0.0081; avg_loss: 0.0136
20-03-23 17:05-INFO-training batch acc: 0.9922; avg_acc: 0.9964
20-03-23 17:05-INFO-
20-03-23 17:05-INFO-Epoch 1, Batch 150, Global step 434:
20-03-23 17:05-INFO-training batch loss: 0.0209; avg_loss: 0.0136
20-03-23 17:05-INFO-training batch acc: 0.9922; avg_acc: 0.9964
20-03-23 17:05-INFO-
20-03-23 17:05-INFO-Epoch 1, Batch 151, Global step 435:
20-03-23 17:05-INFO-training batch loss: 0.0020; avg_loss: 0.0136
20-03-23 17:05-INFO-training batch acc: 1.0000; avg_acc: 0.9964
20-03-23 17:05-INFO-
20-03-23 17:05-INFO-Epoch 1, Batch 152, Global step 436:
20-03-23 17:05-INFO-training batch loss: 0.0006; avg_loss: 0.0135
20-03-23 17:05-INFO-training batch acc: 1.0000; avg_acc: 0.9965
20-03-23 17:05-INFO-
20-03-23 17:05-INFO-Epoch 1, Batch 153, Global step 437:
20-03-23 17:05-INFO-training batch loss: 0.0012; avg_loss: 0.0134
20-03-23 17:05-INFO-training batch acc: 1.0000; avg_acc: 0.9965
20-03-23 17:05-INFO-
20-03-23 17:05-INFO-Epoch 1, Batch 154, Global step 438:
20-03-23 17:05-INFO-training batch loss: 0.0017; avg_loss: 0.0133
20-03-23 17:05-INFO-training batch acc: 1.0000; avg_acc: 0.9965
20-03-23 17:05-INFO-
20-03-23 17:05-INFO-Epoch 1, Batch 155, Global step 439:
20-03-23 17:05-INFO-training batch loss: 0.0006; avg_loss: 0.0132
20-03-23 17:05-INFO-training batch acc: 1.0000; avg_acc: 0.9965
20-03-23 17:05-INFO-
20-03-23 17:05-INFO-Epoch 1, Batch 156, Global step 440:
20-03-23 17:05-INFO-training batch loss: 0.0013; avg_loss: 0.0132
20-03-23 17:05-INFO-training batch acc: 1.0000; avg_acc: 0.9965
20-03-23 17:05-INFO-
20-03-23 17:05-INFO-Epoch 1, Batch 157, Global step 441:
20-03-23 17:05-INFO-training batch loss: 0.0209; avg_loss: 0.0132
20-03-23 17:05-INFO-training batch acc: 0.9844; avg_acc: 0.9965
20-03-23 17:05-INFO-
20-03-23 17:05-INFO-Epoch 1, Batch 158, Global step 442:
20-03-23 17:05-INFO-training batch loss: 0.0015; avg_loss: 0.0131
20-03-23 17:05-INFO-training batch acc: 1.0000; avg_acc: 0.9965
20-03-23 17:05-INFO-
20-03-23 17:05-INFO-Epoch 1, Batch 159, Global step 443:
20-03-23 17:05-INFO-training batch loss: 0.0002; avg_loss: 0.0131
20-03-23 17:05-INFO-training batch acc: 1.0000; avg_acc: 0.9965
20-03-23 17:05-INFO-
20-03-23 17:05-INFO-Epoch 1, Batch 160, Global step 444:
20-03-23 17:05-INFO-training batch loss: 0.0051; avg_loss: 0.0130
20-03-23 17:05-INFO-training batch acc: 1.0000; avg_acc: 0.9965
20-03-23 17:05-INFO-
20-03-23 17:05-INFO-Epoch 1, Batch 161, Global step 445:
20-03-23 17:05-INFO-training batch loss: 0.0039; avg_loss: 0.0129
20-03-23 17:05-INFO-training batch acc: 1.0000; avg_acc: 0.9966
20-03-23 17:05-INFO-
20-03-23 17:05-INFO-Epoch 1, Batch 162, Global step 446:
20-03-23 17:05-INFO-training batch loss: 0.0055; avg_loss: 0.0129
20-03-23 17:05-INFO-training batch acc: 1.0000; avg_acc: 0.9966
20-03-23 17:05-INFO-
20-03-23 17:05-INFO-Epoch 1, Batch 163, Global step 447:
20-03-23 17:05-INFO-training batch loss: 0.0143; avg_loss: 0.0129
20-03-23 17:05-INFO-training batch acc: 0.9922; avg_acc: 0.9965
20-03-23 17:05-INFO-
20-03-23 17:05-INFO-Epoch 1, Batch 164, Global step 448:
20-03-23 17:05-INFO-training batch loss: 0.0246; avg_loss: 0.0130
20-03-23 17:05-INFO-training batch acc: 0.9922; avg_acc: 0.9965
20-03-23 17:05-INFO-
20-03-23 17:06-INFO-Epoch 1, Batch 165, Global step 449:
20-03-23 17:06-INFO-training batch loss: 0.0020; avg_loss: 0.0129
20-03-23 17:06-INFO-training batch acc: 1.0000; avg_acc: 0.9965
20-03-23 17:06-INFO-
20-03-23 17:06-INFO-Epoch 1, Batch 166, Global step 450:
20-03-23 17:06-INFO-training batch loss: 0.0080; avg_loss: 0.0129
20-03-23 17:06-INFO-training batch acc: 1.0000; avg_acc: 0.9966
20-03-23 17:06-INFO-
20-03-23 17:06-INFO-Epoch 1, Batch 167, Global step 451:
20-03-23 17:06-INFO-training batch loss: 0.0006; avg_loss: 0.0128
20-03-23 17:06-INFO-training batch acc: 1.0000; avg_acc: 0.9966
20-03-23 17:06-INFO-
20-03-23 17:06-INFO-Epoch 1, Batch 168, Global step 452:
20-03-23 17:06-INFO-training batch loss: 0.0024; avg_loss: 0.0127
20-03-23 17:06-INFO-training batch acc: 1.0000; avg_acc: 0.9966
20-03-23 17:06-INFO-
20-03-23 17:06-INFO-Epoch 1, Batch 169, Global step 453:
20-03-23 17:06-INFO-training batch loss: 0.0012; avg_loss: 0.0127
20-03-23 17:06-INFO-training batch acc: 1.0000; avg_acc: 0.9966
20-03-23 17:06-INFO-
20-03-23 17:06-INFO-Epoch 1, Batch 170, Global step 454:
20-03-23 17:06-INFO-training batch loss: 0.0207; avg_loss: 0.0127
20-03-23 17:06-INFO-training batch acc: 0.9922; avg_acc: 0.9966
20-03-23 17:06-INFO-
20-03-23 17:06-INFO-Epoch 1, Batch 171, Global step 455:
20-03-23 17:06-INFO-training batch loss: 0.0019; avg_loss: 0.0127
20-03-23 17:06-INFO-training batch acc: 1.0000; avg_acc: 0.9966
20-03-23 17:06-INFO-
20-03-23 17:06-INFO-Epoch 1, Batch 172, Global step 456:
20-03-23 17:06-INFO-training batch loss: 0.0311; avg_loss: 0.0128
20-03-23 17:06-INFO-training batch acc: 0.9766; avg_acc: 0.9965
20-03-23 17:06-INFO-
20-03-23 17:06-INFO-Epoch 1, Batch 173, Global step 457:
20-03-23 17:06-INFO-training batch loss: 0.0070; avg_loss: 0.0127
20-03-23 17:06-INFO-training batch acc: 1.0000; avg_acc: 0.9965
20-03-23 17:06-INFO-
20-03-23 17:06-INFO-Epoch 1, Batch 174, Global step 458:
20-03-23 17:06-INFO-training batch loss: 0.0113; avg_loss: 0.0127
20-03-23 17:06-INFO-training batch acc: 0.9922; avg_acc: 0.9965
20-03-23 17:06-INFO-
20-03-23 17:06-INFO-Epoch 1, Batch 175, Global step 459:
20-03-23 17:06-INFO-training batch loss: 0.0261; avg_loss: 0.0128
20-03-23 17:06-INFO-training batch acc: 0.9922; avg_acc: 0.9965
20-03-23 17:06-INFO-
20-03-23 17:06-INFO-Epoch 1, Batch 176, Global step 460:
20-03-23 17:06-INFO-training batch loss: 0.0054; avg_loss: 0.0128
20-03-23 17:06-INFO-training batch acc: 1.0000; avg_acc: 0.9965
20-03-23 17:06-INFO-
20-03-23 17:06-INFO-Epoch 1, Batch 177, Global step 461:
20-03-23 17:06-INFO-training batch loss: 0.0079; avg_loss: 0.0127
20-03-23 17:06-INFO-training batch acc: 1.0000; avg_acc: 0.9965
20-03-23 17:06-INFO-
20-03-23 17:06-INFO-Epoch 1, Batch 178, Global step 462:
20-03-23 17:06-INFO-training batch loss: 0.0012; avg_loss: 0.0127
20-03-23 17:06-INFO-training batch acc: 1.0000; avg_acc: 0.9965
20-03-23 17:06-INFO-
20-03-23 17:06-INFO-Epoch 1, Batch 179, Global step 463:
20-03-23 17:06-INFO-training batch loss: 0.0050; avg_loss: 0.0126
20-03-23 17:06-INFO-training batch acc: 1.0000; avg_acc: 0.9966
20-03-23 17:06-INFO-
20-03-23 17:06-INFO-Epoch 1, Batch 180, Global step 464:
20-03-23 17:06-INFO-training batch loss: 0.0305; avg_loss: 0.0127
20-03-23 17:06-INFO-training batch acc: 0.9844; avg_acc: 0.9965
20-03-23 17:06-INFO-
20-03-23 17:06-INFO-Epoch 1, Batch 181, Global step 465:
20-03-23 17:06-INFO-training batch loss: 0.0026; avg_loss: 0.0127
20-03-23 17:06-INFO-training batch acc: 1.0000; avg_acc: 0.9965
20-03-23 17:06-INFO-
20-03-23 17:06-INFO-Epoch 1, Batch 182, Global step 466:
20-03-23 17:06-INFO-training batch loss: 0.0103; avg_loss: 0.0127
20-03-23 17:06-INFO-training batch acc: 1.0000; avg_acc: 0.9965
20-03-23 17:06-INFO-
20-03-23 17:06-INFO-Epoch 1, Batch 183, Global step 467:
20-03-23 17:06-INFO-training batch loss: 0.0096; avg_loss: 0.0126
20-03-23 17:06-INFO-training batch acc: 1.0000; avg_acc: 0.9965
20-03-23 17:06-INFO-
20-03-23 17:06-INFO-Epoch 1, Batch 184, Global step 468:
20-03-23 17:06-INFO-training batch loss: 0.0058; avg_loss: 0.0126
20-03-23 17:06-INFO-training batch acc: 1.0000; avg_acc: 0.9966
20-03-23 17:06-INFO-
20-03-23 17:06-INFO-Epoch 1, Batch 185, Global step 469:
20-03-23 17:06-INFO-training batch loss: 0.0024; avg_loss: 0.0125
20-03-23 17:06-INFO-training batch acc: 1.0000; avg_acc: 0.9966
20-03-23 17:06-INFO-
20-03-23 17:06-INFO-Epoch 1, Batch 186, Global step 470:
20-03-23 17:06-INFO-training batch loss: 0.0017; avg_loss: 0.0125
20-03-23 17:06-INFO-training batch acc: 1.0000; avg_acc: 0.9966
20-03-23 17:06-INFO-
20-03-23 17:06-INFO-Epoch 1, Batch 187, Global step 471:
20-03-23 17:06-INFO-training batch loss: 0.0068; avg_loss: 0.0125
20-03-23 17:06-INFO-training batch acc: 1.0000; avg_acc: 0.9966
20-03-23 17:06-INFO-
20-03-23 17:06-INFO-Epoch 1, Batch 188, Global step 472:
20-03-23 17:06-INFO-training batch loss: 0.0032; avg_loss: 0.0124
20-03-23 17:06-INFO-training batch acc: 1.0000; avg_acc: 0.9966
20-03-23 17:06-INFO-
20-03-23 17:06-INFO-Epoch 1, Batch 189, Global step 473:
20-03-23 17:06-INFO-training batch loss: 0.0080; avg_loss: 0.0124
20-03-23 17:06-INFO-training batch acc: 1.0000; avg_acc: 0.9967
20-03-23 17:06-INFO-
20-03-23 17:06-INFO-Epoch 1, Batch 190, Global step 474:
20-03-23 17:06-INFO-training batch loss: 0.0145; avg_loss: 0.0124
20-03-23 17:06-INFO-training batch acc: 0.9922; avg_acc: 0.9966
20-03-23 17:06-INFO-
20-03-23 17:07-INFO-Epoch 1, Batch 191, Global step 475:
20-03-23 17:07-INFO-training batch loss: 0.0066; avg_loss: 0.0124
20-03-23 17:07-INFO-training batch acc: 1.0000; avg_acc: 0.9966
20-03-23 17:07-INFO-
20-03-23 17:07-INFO-Epoch 1, Batch 192, Global step 476:
20-03-23 17:07-INFO-training batch loss: 0.0045; avg_loss: 0.0123
20-03-23 17:07-INFO-training batch acc: 1.0000; avg_acc: 0.9967
20-03-23 17:07-INFO-
20-03-23 17:07-INFO-Epoch 1, Batch 193, Global step 477:
20-03-23 17:07-INFO-training batch loss: 0.0134; avg_loss: 0.0123
20-03-23 17:07-INFO-training batch acc: 0.9922; avg_acc: 0.9966
20-03-23 17:07-INFO-
20-03-23 17:07-INFO-Epoch 1, Batch 194, Global step 478:
20-03-23 17:07-INFO-training batch loss: 0.0155; avg_loss: 0.0123
20-03-23 17:07-INFO-training batch acc: 0.9922; avg_acc: 0.9966
20-03-23 17:07-INFO-
20-03-23 17:07-INFO-Epoch 1, Batch 195, Global step 479:
20-03-23 17:07-INFO-training batch loss: 0.0024; avg_loss: 0.0123
20-03-23 17:07-INFO-training batch acc: 1.0000; avg_acc: 0.9966
20-03-23 17:07-INFO-
20-03-23 17:07-INFO-Epoch 1, Batch 196, Global step 480:
20-03-23 17:07-INFO-training batch loss: 0.0019; avg_loss: 0.0122
20-03-23 17:07-INFO-training batch acc: 1.0000; avg_acc: 0.9967
20-03-23 17:07-INFO-
20-03-23 17:07-INFO-Epoch 1, Batch 197, Global step 481:
20-03-23 17:07-INFO-training batch loss: 0.0035; avg_loss: 0.0122
20-03-23 17:07-INFO-training batch acc: 1.0000; avg_acc: 0.9967
20-03-23 17:07-INFO-
20-03-23 17:07-INFO-Epoch 1, Batch 198, Global step 482:
20-03-23 17:07-INFO-training batch loss: 0.0011; avg_loss: 0.0121
20-03-23 17:07-INFO-training batch acc: 1.0000; avg_acc: 0.9967
20-03-23 17:07-INFO-
20-03-23 17:07-INFO-Epoch 1, Batch 199, Global step 483:
20-03-23 17:07-INFO-training batch loss: 0.0015; avg_loss: 0.0121
20-03-23 17:07-INFO-training batch acc: 1.0000; avg_acc: 0.9967
20-03-23 17:07-INFO-
20-03-23 17:07-INFO-Epoch 1, Batch 200, Global step 484:
20-03-23 17:07-INFO-training batch loss: 0.0166; avg_loss: 0.0121
20-03-23 17:07-INFO-training batch acc: 0.9844; avg_acc: 0.9966
20-03-23 17:07-INFO-
20-03-23 17:07-INFO-Epoch 1, Batch 201, Global step 485:
20-03-23 17:07-INFO-training batch loss: 0.0006; avg_loss: 0.0121
20-03-23 17:07-INFO-training batch acc: 1.0000; avg_acc: 0.9967
20-03-23 17:07-INFO-
20-03-23 17:07-INFO-Epoch 1, Batch 202, Global step 486:
20-03-23 17:07-INFO-training batch loss: 0.0007; avg_loss: 0.0120
20-03-23 17:07-INFO-training batch acc: 1.0000; avg_acc: 0.9967
20-03-23 17:07-INFO-
20-03-23 17:07-INFO-Epoch 1, Batch 203, Global step 487:
20-03-23 17:07-INFO-training batch loss: 0.0090; avg_loss: 0.0120
20-03-23 17:07-INFO-training batch acc: 0.9922; avg_acc: 0.9967
20-03-23 17:07-INFO-
20-03-23 17:07-INFO-Epoch 1, Batch 204, Global step 488:
20-03-23 17:07-INFO-training batch loss: 0.0055; avg_loss: 0.0120
20-03-23 17:07-INFO-training batch acc: 1.0000; avg_acc: 0.9967
20-03-23 17:07-INFO-
20-03-23 17:07-INFO-Epoch 1, Batch 205, Global step 489:
20-03-23 17:07-INFO-training batch loss: 0.0006; avg_loss: 0.0119
20-03-23 17:07-INFO-training batch acc: 1.0000; avg_acc: 0.9967
20-03-23 17:07-INFO-
20-03-23 17:07-INFO-Epoch 1, Batch 206, Global step 490:
20-03-23 17:07-INFO-training batch loss: 0.0028; avg_loss: 0.0119
20-03-23 17:07-INFO-training batch acc: 1.0000; avg_acc: 0.9967
20-03-23 17:07-INFO-
20-03-23 17:07-INFO-Epoch 1, Batch 207, Global step 491:
20-03-23 17:07-INFO-training batch loss: 0.0029; avg_loss: 0.0118
20-03-23 17:07-INFO-training batch acc: 1.0000; avg_acc: 0.9967
20-03-23 17:07-INFO-
20-03-23 17:07-INFO-Epoch 1, Batch 208, Global step 492:
20-03-23 17:07-INFO-training batch loss: 0.0254; avg_loss: 0.0119
20-03-23 17:07-INFO-training batch acc: 0.9922; avg_acc: 0.9967
20-03-23 17:07-INFO-
20-03-23 17:07-INFO-Epoch 1, Batch 209, Global step 493:
20-03-23 17:07-INFO-training batch loss: 0.0006; avg_loss: 0.0118
20-03-23 17:07-INFO-training batch acc: 1.0000; avg_acc: 0.9967
20-03-23 17:07-INFO-
20-03-23 17:07-INFO-Epoch 1, Batch 210, Global step 494:
20-03-23 17:07-INFO-training batch loss: 0.0003; avg_loss: 0.0118
20-03-23 17:07-INFO-training batch acc: 1.0000; avg_acc: 0.9967
20-03-23 17:07-INFO-
20-03-23 17:07-INFO-Epoch 1, Batch 211, Global step 495:
20-03-23 17:07-INFO-training batch loss: 0.0057; avg_loss: 0.0117
20-03-23 17:07-INFO-training batch acc: 1.0000; avg_acc: 0.9967
20-03-23 17:07-INFO-
20-03-23 17:07-INFO-Epoch 1, Batch 212, Global step 496:
20-03-23 17:07-INFO-training batch loss: 0.0036; avg_loss: 0.0117
20-03-23 17:07-INFO-training batch acc: 1.0000; avg_acc: 0.9968
20-03-23 17:07-INFO-
20-03-23 17:07-INFO-Epoch 1, Batch 213, Global step 497:
20-03-23 17:07-INFO-training batch loss: 0.0133; avg_loss: 0.0117
20-03-23 17:07-INFO-training batch acc: 0.9922; avg_acc: 0.9967
20-03-23 17:07-INFO-
20-03-23 17:07-INFO-Epoch 1, Batch 214, Global step 498:
20-03-23 17:07-INFO-training batch loss: 0.0008; avg_loss: 0.0117
20-03-23 17:07-INFO-training batch acc: 1.0000; avg_acc: 0.9968
20-03-23 17:07-INFO-
20-03-23 17:07-INFO-Epoch 1, Batch 215, Global step 499:
20-03-23 17:07-INFO-training batch loss: 0.0020; avg_loss: 0.0116
20-03-23 17:07-INFO-training batch acc: 1.0000; avg_acc: 0.9968
20-03-23 17:07-INFO-
20-03-23 17:07-INFO-Epoch 1, Batch 216, Global step 500:
20-03-23 17:07-INFO-training batch loss: 0.0019; avg_loss: 0.0116
20-03-23 17:07-INFO-training batch acc: 1.0000; avg_acc: 0.9968
20-03-23 17:07-INFO-
20-03-23 17:07-INFO-Epoch 1, Batch 217, Global step 501:
20-03-23 17:07-INFO-training batch loss: 0.0118; avg_loss: 0.0116
20-03-23 17:07-INFO-training batch acc: 0.9922; avg_acc: 0.9968
20-03-23 17:07-INFO-
20-03-23 17:08-INFO-Epoch 1, Batch 218, Global step 502:
20-03-23 17:08-INFO-training batch loss: 0.0054; avg_loss: 0.0115
20-03-23 17:08-INFO-training batch acc: 1.0000; avg_acc: 0.9968
20-03-23 17:08-INFO-
20-03-23 17:08-INFO-Epoch 1, Batch 219, Global step 503:
20-03-23 17:08-INFO-training batch loss: 0.0005; avg_loss: 0.0115
20-03-23 17:08-INFO-training batch acc: 1.0000; avg_acc: 0.9968
20-03-23 17:08-INFO-
20-03-23 17:08-INFO-Epoch 1, Batch 220, Global step 504:
20-03-23 17:08-INFO-training batch loss: 0.0060; avg_loss: 0.0115
20-03-23 17:08-INFO-training batch acc: 1.0000; avg_acc: 0.9968
20-03-23 17:08-INFO-
20-03-23 17:08-INFO-Epoch 1, Batch 221, Global step 505:
20-03-23 17:08-INFO-training batch loss: 0.0031; avg_loss: 0.0114
20-03-23 17:08-INFO-training batch acc: 1.0000; avg_acc: 0.9968
20-03-23 17:08-INFO-
20-03-23 17:08-INFO-Epoch 1, Batch 222, Global step 506:
20-03-23 17:08-INFO-training batch loss: 0.0031; avg_loss: 0.0114
20-03-23 17:08-INFO-training batch acc: 1.0000; avg_acc: 0.9968
20-03-23 17:08-INFO-
20-03-23 17:08-INFO-Epoch 1, Batch 223, Global step 507:
20-03-23 17:08-INFO-training batch loss: 0.0054; avg_loss: 0.0114
20-03-23 17:08-INFO-training batch acc: 1.0000; avg_acc: 0.9968
20-03-23 17:08-INFO-
20-03-23 17:08-INFO-Epoch 1, Batch 224, Global step 508:
20-03-23 17:08-INFO-training batch loss: 0.0022; avg_loss: 0.0113
20-03-23 17:08-INFO-training batch acc: 1.0000; avg_acc: 0.9969
20-03-23 17:08-INFO-
20-03-23 17:08-INFO-Epoch 1, Batch 225, Global step 509:
20-03-23 17:08-INFO-training batch loss: 0.0026; avg_loss: 0.0113
20-03-23 17:08-INFO-training batch acc: 1.0000; avg_acc: 0.9969
20-03-23 17:08-INFO-
20-03-23 17:08-INFO-Epoch 1, Batch 226, Global step 510:
20-03-23 17:08-INFO-training batch loss: 0.0005; avg_loss: 0.0112
20-03-23 17:08-INFO-training batch acc: 1.0000; avg_acc: 0.9969
20-03-23 17:08-INFO-
20-03-23 17:08-INFO-Epoch 1, Batch 227, Global step 511:
20-03-23 17:08-INFO-training batch loss: 0.0026; avg_loss: 0.0112
20-03-23 17:08-INFO-training batch acc: 1.0000; avg_acc: 0.9969
20-03-23 17:08-INFO-
20-03-23 17:08-INFO-Epoch 1, Batch 228, Global step 512:
20-03-23 17:08-INFO-training batch loss: 0.0019; avg_loss: 0.0112
20-03-23 17:08-INFO-training batch acc: 1.0000; avg_acc: 0.9969
20-03-23 17:08-INFO-
20-03-23 17:08-INFO-Epoch 1, Batch 229, Global step 513:
20-03-23 17:08-INFO-training batch loss: 0.0023; avg_loss: 0.0111
20-03-23 17:08-INFO-training batch acc: 1.0000; avg_acc: 0.9969
20-03-23 17:08-INFO-
20-03-23 17:08-INFO-Epoch 1, Batch 230, Global step 514:
20-03-23 17:08-INFO-training batch loss: 0.0028; avg_loss: 0.0111
20-03-23 17:08-INFO-training batch acc: 1.0000; avg_acc: 0.9969
20-03-23 17:08-INFO-
20-03-23 17:08-INFO-Epoch 1, Batch 231, Global step 515:
20-03-23 17:08-INFO-training batch loss: 0.0040; avg_loss: 0.0111
20-03-23 17:08-INFO-training batch acc: 1.0000; avg_acc: 0.9970
20-03-23 17:08-INFO-
20-03-23 17:08-INFO-Epoch 1, Batch 232, Global step 516:
20-03-23 17:08-INFO-training batch loss: 0.0004; avg_loss: 0.0110
20-03-23 17:08-INFO-training batch acc: 1.0000; avg_acc: 0.9970
20-03-23 17:08-INFO-
20-03-23 17:08-INFO-Epoch 1, Batch 233, Global step 517:
20-03-23 17:08-INFO-training batch loss: 0.0015; avg_loss: 0.0110
20-03-23 17:08-INFO-training batch acc: 1.0000; avg_acc: 0.9970
20-03-23 17:08-INFO-
20-03-23 17:08-INFO-Epoch 1, Batch 234, Global step 518:
20-03-23 17:08-INFO-training batch loss: 0.0012; avg_loss: 0.0109
20-03-23 17:08-INFO-training batch acc: 1.0000; avg_acc: 0.9970
20-03-23 17:08-INFO-
20-03-23 17:08-INFO-Epoch 1, Batch 235, Global step 519:
20-03-23 17:08-INFO-training batch loss: 0.0003; avg_loss: 0.0109
20-03-23 17:08-INFO-training batch acc: 1.0000; avg_acc: 0.9970
20-03-23 17:08-INFO-
20-03-23 17:08-INFO-Epoch 1, Batch 236, Global step 520:
20-03-23 17:08-INFO-training batch loss: 0.0004; avg_loss: 0.0108
20-03-23 17:08-INFO-training batch acc: 1.0000; avg_acc: 0.9970
20-03-23 17:08-INFO-
20-03-23 17:08-INFO-Epoch 1, Batch 237, Global step 521:
20-03-23 17:08-INFO-training batch loss: 0.0020; avg_loss: 0.0108
20-03-23 17:08-INFO-training batch acc: 1.0000; avg_acc: 0.9970
20-03-23 17:08-INFO-
20-03-23 17:08-INFO-Epoch 1, Batch 238, Global step 522:
20-03-23 17:08-INFO-training batch loss: 0.0011; avg_loss: 0.0108
20-03-23 17:08-INFO-training batch acc: 1.0000; avg_acc: 0.9970
20-03-23 17:08-INFO-
20-03-23 17:08-INFO-Epoch 1, Batch 239, Global step 523:
20-03-23 17:08-INFO-training batch loss: 0.0011; avg_loss: 0.0107
20-03-23 17:08-INFO-training batch acc: 1.0000; avg_acc: 0.9971
20-03-23 17:08-INFO-
20-03-23 17:08-INFO-Epoch 1, Batch 240, Global step 524:
20-03-23 17:08-INFO-training batch loss: 0.0007; avg_loss: 0.0107
20-03-23 17:08-INFO-training batch acc: 1.0000; avg_acc: 0.9971
20-03-23 17:08-INFO-
20-03-23 17:08-INFO-Epoch 1, Batch 241, Global step 525:
20-03-23 17:08-INFO-training batch loss: 0.0003; avg_loss: 0.0106
20-03-23 17:08-INFO-training batch acc: 1.0000; avg_acc: 0.9971
20-03-23 17:08-INFO-
20-03-23 17:08-INFO-Epoch 1, Batch 242, Global step 526:
20-03-23 17:08-INFO-training batch loss: 0.0005; avg_loss: 0.0106
20-03-23 17:08-INFO-training batch acc: 1.0000; avg_acc: 0.9971
20-03-23 17:08-INFO-
20-03-23 17:08-INFO-Epoch 1, Batch 243, Global step 527:
20-03-23 17:08-INFO-training batch loss: 0.0006; avg_loss: 0.0105
20-03-23 17:08-INFO-training batch acc: 1.0000; avg_acc: 0.9971
20-03-23 17:08-INFO-
20-03-23 17:09-INFO-Epoch 1, Batch 244, Global step 528:
20-03-23 17:09-INFO-training batch loss: 0.0013; avg_loss: 0.0105
20-03-23 17:09-INFO-training batch acc: 1.0000; avg_acc: 0.9971
20-03-23 17:09-INFO-
20-03-23 17:09-INFO-Epoch 1, Batch 245, Global step 529:
20-03-23 17:09-INFO-training batch loss: 0.0005; avg_loss: 0.0105
20-03-23 17:09-INFO-training batch acc: 1.0000; avg_acc: 0.9971
20-03-23 17:09-INFO-
20-03-23 17:09-INFO-Epoch 1, Batch 246, Global step 530:
20-03-23 17:09-INFO-training batch loss: 0.0001; avg_loss: 0.0104
20-03-23 17:09-INFO-training batch acc: 1.0000; avg_acc: 0.9971
20-03-23 17:09-INFO-
20-03-23 17:09-INFO-Epoch 1, Batch 247, Global step 531:
20-03-23 17:09-INFO-training batch loss: 0.0006; avg_loss: 0.0104
20-03-23 17:09-INFO-training batch acc: 1.0000; avg_acc: 0.9972
20-03-23 17:09-INFO-
20-03-23 17:09-INFO-Epoch 1, Batch 248, Global step 532:
20-03-23 17:09-INFO-training batch loss: 0.0027; avg_loss: 0.0104
20-03-23 17:09-INFO-training batch acc: 1.0000; avg_acc: 0.9972
20-03-23 17:09-INFO-
20-03-23 17:09-INFO-Epoch 1, Batch 249, Global step 533:
20-03-23 17:09-INFO-training batch loss: 0.0005; avg_loss: 0.0103
20-03-23 17:09-INFO-training batch acc: 1.0000; avg_acc: 0.9972
20-03-23 17:09-INFO-
20-03-23 17:09-INFO-Epoch 1, Batch 250, Global step 534:
20-03-23 17:09-INFO-training batch loss: 0.0004; avg_loss: 0.0103
20-03-23 17:09-INFO-training batch acc: 1.0000; avg_acc: 0.9972
20-03-23 17:09-INFO-
20-03-23 17:09-INFO-Epoch 1, Batch 251, Global step 535:
20-03-23 17:09-INFO-training batch loss: 0.0010; avg_loss: 0.0102
20-03-23 17:09-INFO-training batch acc: 1.0000; avg_acc: 0.9972
20-03-23 17:09-INFO-
20-03-23 17:09-INFO-Epoch 1, Batch 252, Global step 536:
20-03-23 17:09-INFO-training batch loss: 0.0004; avg_loss: 0.0102
20-03-23 17:09-INFO-training batch acc: 1.0000; avg_acc: 0.9972
20-03-23 17:09-INFO-
20-03-23 17:09-INFO-Epoch 1, Batch 253, Global step 537:
20-03-23 17:09-INFO-training batch loss: 0.0002; avg_loss: 0.0102
20-03-23 17:09-INFO-training batch acc: 1.0000; avg_acc: 0.9972
20-03-23 17:09-INFO-
20-03-23 17:09-INFO-Epoch 1, Batch 254, Global step 538:
20-03-23 17:09-INFO-training batch loss: 0.0006; avg_loss: 0.0101
20-03-23 17:09-INFO-training batch acc: 1.0000; avg_acc: 0.9972
20-03-23 17:09-INFO-
20-03-23 17:09-INFO-Epoch 1, Batch 255, Global step 539:
20-03-23 17:09-INFO-training batch loss: 0.0001; avg_loss: 0.0101
20-03-23 17:09-INFO-training batch acc: 1.0000; avg_acc: 0.9972
20-03-23 17:09-INFO-
20-03-23 17:09-INFO-Epoch 1, Batch 256, Global step 540:
20-03-23 17:09-INFO-training batch loss: 0.0002; avg_loss: 0.0100
20-03-23 17:09-INFO-training batch acc: 1.0000; avg_acc: 0.9973
20-03-23 17:09-INFO-
20-03-23 17:09-INFO-Epoch 1, Batch 257, Global step 541:
20-03-23 17:09-INFO-training batch loss: 0.0002; avg_loss: 0.0100
20-03-23 17:09-INFO-training batch acc: 1.0000; avg_acc: 0.9973
20-03-23 17:09-INFO-
20-03-23 17:09-INFO-Epoch 1, Batch 258, Global step 542:
20-03-23 17:09-INFO-training batch loss: 0.0001; avg_loss: 0.0100
20-03-23 17:09-INFO-training batch acc: 1.0000; avg_acc: 0.9973
20-03-23 17:09-INFO-
20-03-23 17:09-INFO-Epoch 1, Batch 259, Global step 543:
20-03-23 17:09-INFO-training batch loss: 0.0003; avg_loss: 0.0099
20-03-23 17:09-INFO-training batch acc: 1.0000; avg_acc: 0.9973
20-03-23 17:09-INFO-
20-03-23 17:09-INFO-Epoch 1, Batch 260, Global step 544:
20-03-23 17:09-INFO-training batch loss: 0.0004; avg_loss: 0.0099
20-03-23 17:09-INFO-training batch acc: 1.0000; avg_acc: 0.9973
20-03-23 17:09-INFO-
20-03-23 17:09-INFO-Epoch 1, Batch 261, Global step 545:
20-03-23 17:09-INFO-training batch loss: 0.0002; avg_loss: 0.0099
20-03-23 17:09-INFO-training batch acc: 1.0000; avg_acc: 0.9973
20-03-23 17:09-INFO-
20-03-23 17:09-INFO-Epoch 1, Batch 262, Global step 546:
20-03-23 17:09-INFO-training batch loss: 0.0003; avg_loss: 0.0098
20-03-23 17:09-INFO-training batch acc: 1.0000; avg_acc: 0.9973
20-03-23 17:09-INFO-
20-03-23 17:09-INFO-Epoch 1, Batch 263, Global step 547:
20-03-23 17:09-INFO-training batch loss: 0.0003; avg_loss: 0.0098
20-03-23 17:09-INFO-training batch acc: 1.0000; avg_acc: 0.9973
20-03-23 17:09-INFO-
20-03-23 17:09-INFO-Epoch 1, Batch 264, Global step 548:
20-03-23 17:09-INFO-training batch loss: 0.0009; avg_loss: 0.0098
20-03-23 17:09-INFO-training batch acc: 1.0000; avg_acc: 0.9973
20-03-23 17:09-INFO-
20-03-23 17:09-INFO-Epoch 1, Batch 265, Global step 549:
20-03-23 17:09-INFO-training batch loss: 0.0001; avg_loss: 0.0097
20-03-23 17:09-INFO-training batch acc: 1.0000; avg_acc: 0.9973
20-03-23 17:09-INFO-
20-03-23 17:09-INFO-Epoch 1, Batch 266, Global step 550:
20-03-23 17:09-INFO-training batch loss: 0.0001; avg_loss: 0.0097
20-03-23 17:09-INFO-training batch acc: 1.0000; avg_acc: 0.9974
20-03-23 17:09-INFO-
20-03-23 17:09-INFO-Epoch 1, Batch 267, Global step 551:
20-03-23 17:09-INFO-training batch loss: 0.0071; avg_loss: 0.0097
20-03-23 17:09-INFO-training batch acc: 0.9922; avg_acc: 0.9973
20-03-23 17:09-INFO-
20-03-23 17:09-INFO-Epoch 1, Batch 268, Global step 552:
20-03-23 17:09-INFO-training batch loss: 0.0001; avg_loss: 0.0096
20-03-23 17:09-INFO-training batch acc: 1.0000; avg_acc: 0.9973
20-03-23 17:09-INFO-
20-03-23 17:09-INFO-Epoch 1, Batch 269, Global step 553:
20-03-23 17:09-INFO-training batch loss: 0.0008; avg_loss: 0.0096
20-03-23 17:09-INFO-training batch acc: 1.0000; avg_acc: 0.9974
20-03-23 17:09-INFO-
20-03-23 17:09-INFO-Epoch 1, Batch 270, Global step 554:
20-03-23 17:09-INFO-training batch loss: 0.0003; avg_loss: 0.0096
20-03-23 17:09-INFO-training batch acc: 1.0000; avg_acc: 0.9974
20-03-23 17:09-INFO-
20-03-23 17:10-INFO-Epoch 1, Batch 271, Global step 555:
20-03-23 17:10-INFO-training batch loss: 0.0003; avg_loss: 0.0095
20-03-23 17:10-INFO-training batch acc: 1.0000; avg_acc: 0.9974
20-03-23 17:10-INFO-
20-03-23 17:10-INFO-Epoch 1, Batch 272, Global step 556:
20-03-23 17:10-INFO-training batch loss: 0.0010; avg_loss: 0.0095
20-03-23 17:10-INFO-training batch acc: 1.0000; avg_acc: 0.9974
20-03-23 17:10-INFO-
20-03-23 17:10-INFO-Epoch 1, Batch 273, Global step 557:
20-03-23 17:10-INFO-training batch loss: 0.0004; avg_loss: 0.0095
20-03-23 17:10-INFO-training batch acc: 1.0000; avg_acc: 0.9974
20-03-23 17:10-INFO-
20-03-23 17:10-INFO-Epoch 1, Batch 274, Global step 558:
20-03-23 17:10-INFO-training batch loss: 0.0004; avg_loss: 0.0094
20-03-23 17:10-INFO-training batch acc: 1.0000; avg_acc: 0.9974
20-03-23 17:10-INFO-
20-03-23 17:10-INFO-Epoch 1, Batch 275, Global step 559:
20-03-23 17:10-INFO-training batch loss: 0.0005; avg_loss: 0.0094
20-03-23 17:10-INFO-training batch acc: 1.0000; avg_acc: 0.9974
20-03-23 17:10-INFO-
20-03-23 17:10-INFO-Epoch 1, Batch 276, Global step 560:
20-03-23 17:10-INFO-training batch loss: 0.0007; avg_loss: 0.0094
20-03-23 17:10-INFO-training batch acc: 1.0000; avg_acc: 0.9974
20-03-23 17:10-INFO-
20-03-23 17:10-INFO-Epoch 1, Batch 277, Global step 561:
20-03-23 17:10-INFO-training batch loss: 0.0035; avg_loss: 0.0093
20-03-23 17:10-INFO-training batch acc: 1.0000; avg_acc: 0.9974
20-03-23 17:10-INFO-
20-03-23 17:10-INFO-Epoch 1, Batch 278, Global step 562:
20-03-23 17:10-INFO-training batch loss: 0.0017; avg_loss: 0.0093
20-03-23 17:10-INFO-training batch acc: 1.0000; avg_acc: 0.9974
20-03-23 17:10-INFO-
20-03-23 17:10-INFO-Epoch 1, Batch 279, Global step 563:
20-03-23 17:10-INFO-training batch loss: 0.0019; avg_loss: 0.0093
20-03-23 17:10-INFO-training batch acc: 1.0000; avg_acc: 0.9975
20-03-23 17:10-INFO-
20-03-23 17:10-INFO-Epoch 1, Batch 280, Global step 564:
20-03-23 17:10-INFO-training batch loss: 0.0002; avg_loss: 0.0093
20-03-23 17:10-INFO-training batch acc: 1.0000; avg_acc: 0.9975
20-03-23 17:10-INFO-
20-03-23 17:10-INFO-Epoch 1, Batch 281, Global step 565:
20-03-23 17:10-INFO-training batch loss: 0.0002; avg_loss: 0.0092
20-03-23 17:10-INFO-training batch acc: 1.0000; avg_acc: 0.9975
20-03-23 17:10-INFO-
20-03-23 17:10-INFO-Epoch 1, Batch 282, Global step 566:
20-03-23 17:10-INFO-training batch loss: 0.0006; avg_loss: 0.0092
20-03-23 17:10-INFO-training batch acc: 1.0000; avg_acc: 0.9975
20-03-23 17:10-INFO-
20-03-23 17:10-INFO-Epoch 1, Batch 283, Global step 567:
20-03-23 17:10-INFO-training batch loss: 0.0005; avg_loss: 0.0092
20-03-23 17:10-INFO-training batch acc: 1.0000; avg_acc: 0.9975
20-03-23 17:10-INFO-
20-03-23 17:10-INFO-Epoch 1, Batch 284, Global step 568:
20-03-23 17:10-INFO-training batch loss: 0.0001; avg_loss: 0.0091
20-03-23 17:10-INFO-training batch acc: 1.0000; avg_acc: 0.9975
20-03-23 17:10-INFO-
20-03-23 17:10-INFO-Epoch 1, training batch loss: 0.0001; avg_loss: 0.0091
20-03-23 17:10-INFO-Epoch 1, training batch accuracy: 1.0000; avg_accuracy: 0.9975
20-03-23 17:10-INFO-
20-03-23 17:10-INFO-Epoch 1, evaluating batch loss: 1.6819; avg_loss: 0.5843
20-03-23 17:10-INFO-Epoch 1, evaluating batch accuracy: 0.8636; avg_accuracy: 0.9462
20-03-23 17:10-INFO-
20-03-23 17:10-INFO-Epoch 2, Batch 1, Global step 569:
20-03-23 17:10-INFO-training batch loss: 0.0006; avg_loss: 0.0006
20-03-23 17:10-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:10-INFO-
20-03-23 17:10-INFO-Epoch 2, Batch 2, Global step 570:
20-03-23 17:10-INFO-training batch loss: 0.0001; avg_loss: 0.0003
20-03-23 17:10-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:10-INFO-
20-03-23 17:10-INFO-Epoch 2, Batch 3, Global step 571:
20-03-23 17:10-INFO-training batch loss: 0.0002; avg_loss: 0.0003
20-03-23 17:10-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:10-INFO-
20-03-23 17:10-INFO-Epoch 2, Batch 4, Global step 572:
20-03-23 17:10-INFO-training batch loss: 0.0007; avg_loss: 0.0004
20-03-23 17:10-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:10-INFO-
20-03-23 17:10-INFO-Epoch 2, Batch 5, Global step 573:
20-03-23 17:10-INFO-training batch loss: 0.0030; avg_loss: 0.0009
20-03-23 17:10-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:10-INFO-
20-03-23 17:10-INFO-Epoch 2, Batch 6, Global step 574:
20-03-23 17:10-INFO-training batch loss: 0.0010; avg_loss: 0.0009
20-03-23 17:10-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:10-INFO-
20-03-23 17:10-INFO-Epoch 2, Batch 7, Global step 575:
20-03-23 17:10-INFO-training batch loss: 0.0003; avg_loss: 0.0008
20-03-23 17:10-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:10-INFO-
20-03-23 17:10-INFO-Epoch 2, Batch 8, Global step 576:
20-03-23 17:10-INFO-training batch loss: 0.0001; avg_loss: 0.0008
20-03-23 17:10-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:10-INFO-
20-03-23 17:11-INFO-Epoch 2, Batch 9, Global step 577:
20-03-23 17:11-INFO-training batch loss: 0.0003; avg_loss: 0.0007
20-03-23 17:11-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:11-INFO-
20-03-23 17:11-INFO-Epoch 2, Batch 10, Global step 578:
20-03-23 17:11-INFO-training batch loss: 0.0003; avg_loss: 0.0007
20-03-23 17:11-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:11-INFO-
20-03-23 17:11-INFO-Epoch 2, Batch 11, Global step 579:
20-03-23 17:11-INFO-training batch loss: 0.0002; avg_loss: 0.0006
20-03-23 17:11-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:11-INFO-
20-03-23 17:11-INFO-Epoch 2, Batch 12, Global step 580:
20-03-23 17:11-INFO-training batch loss: 0.0002; avg_loss: 0.0006
20-03-23 17:11-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:11-INFO-
20-03-23 17:11-INFO-Epoch 2, Batch 13, Global step 581:
20-03-23 17:11-INFO-training batch loss: 0.0004; avg_loss: 0.0006
20-03-23 17:11-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:11-INFO-
20-03-23 17:11-INFO-Epoch 2, Batch 14, Global step 582:
20-03-23 17:11-INFO-training batch loss: 0.0008; avg_loss: 0.0006
20-03-23 17:11-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:11-INFO-
20-03-23 17:11-INFO-Epoch 2, Batch 15, Global step 583:
20-03-23 17:11-INFO-training batch loss: 0.0005; avg_loss: 0.0006
20-03-23 17:11-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:11-INFO-
20-03-23 17:11-INFO-Epoch 2, Batch 16, Global step 584:
20-03-23 17:11-INFO-training batch loss: 0.0003; avg_loss: 0.0006
20-03-23 17:11-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:11-INFO-
20-03-23 17:11-INFO-Epoch 2, Batch 17, Global step 585:
20-03-23 17:11-INFO-training batch loss: 0.0003; avg_loss: 0.0005
20-03-23 17:11-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:11-INFO-
20-03-23 17:11-INFO-Epoch 2, Batch 18, Global step 586:
20-03-23 17:11-INFO-training batch loss: 0.0001; avg_loss: 0.0005
20-03-23 17:11-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:11-INFO-
20-03-23 17:11-INFO-Epoch 2, Batch 19, Global step 587:
20-03-23 17:11-INFO-training batch loss: 0.0004; avg_loss: 0.0005
20-03-23 17:11-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:11-INFO-
20-03-23 17:11-INFO-Epoch 2, Batch 20, Global step 588:
20-03-23 17:11-INFO-training batch loss: 0.0002; avg_loss: 0.0005
20-03-23 17:11-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:11-INFO-
20-03-23 17:11-INFO-Epoch 2, Batch 21, Global step 589:
20-03-23 17:11-INFO-training batch loss: 0.0002; avg_loss: 0.0005
20-03-23 17:11-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:11-INFO-
20-03-23 17:11-INFO-Epoch 2, Batch 22, Global step 590:
20-03-23 17:11-INFO-training batch loss: 0.0013; avg_loss: 0.0005
20-03-23 17:11-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:11-INFO-
20-03-23 17:11-INFO-Epoch 2, Batch 23, Global step 591:
20-03-23 17:11-INFO-training batch loss: 0.0002; avg_loss: 0.0005
20-03-23 17:11-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:11-INFO-
20-03-23 17:11-INFO-Epoch 2, Batch 24, Global step 592:
20-03-23 17:11-INFO-training batch loss: 0.0003; avg_loss: 0.0005
20-03-23 17:11-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:11-INFO-
20-03-23 17:11-INFO-Epoch 2, Batch 25, Global step 593:
20-03-23 17:11-INFO-training batch loss: 0.0004; avg_loss: 0.0005
20-03-23 17:11-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:11-INFO-
20-03-23 17:11-INFO-Epoch 2, Batch 26, Global step 594:
20-03-23 17:11-INFO-training batch loss: 0.0004; avg_loss: 0.0005
20-03-23 17:11-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:11-INFO-
20-03-23 17:11-INFO-Epoch 2, Batch 27, Global step 595:
20-03-23 17:11-INFO-training batch loss: 0.0002; avg_loss: 0.0005
20-03-23 17:11-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:11-INFO-
20-03-23 17:11-INFO-Epoch 2, Batch 28, Global step 596:
20-03-23 17:11-INFO-training batch loss: 0.0004; avg_loss: 0.0005
20-03-23 17:11-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:11-INFO-
20-03-23 17:11-INFO-Epoch 2, Batch 29, Global step 597:
20-03-23 17:11-INFO-training batch loss: 0.0006; avg_loss: 0.0005
20-03-23 17:11-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:11-INFO-
20-03-23 17:11-INFO-Epoch 2, Batch 30, Global step 598:
20-03-23 17:11-INFO-training batch loss: 0.0002; avg_loss: 0.0005
20-03-23 17:11-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:11-INFO-
20-03-23 17:11-INFO-Epoch 2, Batch 31, Global step 599:
20-03-23 17:11-INFO-training batch loss: 0.0007; avg_loss: 0.0005
20-03-23 17:11-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:11-INFO-
20-03-23 17:11-INFO-Epoch 2, Batch 32, Global step 600:
20-03-23 17:11-INFO-training batch loss: 0.0003; avg_loss: 0.0005
20-03-23 17:11-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:11-INFO-
20-03-23 17:11-INFO-Epoch 2, Batch 33, Global step 601:
20-03-23 17:11-INFO-training batch loss: 0.0009; avg_loss: 0.0005
20-03-23 17:11-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:11-INFO-
20-03-23 17:11-INFO-Epoch 2, Batch 34, Global step 602:
20-03-23 17:11-INFO-training batch loss: 0.0003; avg_loss: 0.0005
20-03-23 17:11-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:11-INFO-
20-03-23 17:11-INFO-Epoch 2, Batch 35, Global step 603:
20-03-23 17:11-INFO-training batch loss: 0.0001; avg_loss: 0.0005
20-03-23 17:11-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:11-INFO-
20-03-23 17:12-INFO-Epoch 2, Batch 36, Global step 604:
20-03-23 17:12-INFO-training batch loss: 0.0003; avg_loss: 0.0005
20-03-23 17:12-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:12-INFO-
20-03-23 17:12-INFO-Epoch 2, Batch 37, Global step 605:
20-03-23 17:12-INFO-training batch loss: 0.0003; avg_loss: 0.0005
20-03-23 17:12-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:12-INFO-
20-03-23 17:12-INFO-Epoch 2, Batch 38, Global step 606:
20-03-23 17:12-INFO-training batch loss: 0.0001; avg_loss: 0.0004
20-03-23 17:12-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:12-INFO-
20-03-23 17:12-INFO-Epoch 2, Batch 39, Global step 607:
20-03-23 17:12-INFO-training batch loss: 0.0008; avg_loss: 0.0005
20-03-23 17:12-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:12-INFO-
20-03-23 17:12-INFO-Epoch 2, Batch 40, Global step 608:
20-03-23 17:12-INFO-training batch loss: 0.0002; avg_loss: 0.0005
20-03-23 17:12-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:12-INFO-
20-03-23 17:12-INFO-Epoch 2, Batch 41, Global step 609:
20-03-23 17:12-INFO-training batch loss: 0.0002; avg_loss: 0.0004
20-03-23 17:12-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:12-INFO-
20-03-23 17:12-INFO-Epoch 2, Batch 42, Global step 610:
20-03-23 17:12-INFO-training batch loss: 0.0002; avg_loss: 0.0004
20-03-23 17:12-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:12-INFO-
20-03-23 17:12-INFO-Epoch 2, Batch 43, Global step 611:
20-03-23 17:12-INFO-training batch loss: 0.0001; avg_loss: 0.0004
20-03-23 17:12-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:12-INFO-
20-03-23 17:12-INFO-Epoch 2, Batch 44, Global step 612:
20-03-23 17:12-INFO-training batch loss: 0.0004; avg_loss: 0.0004
20-03-23 17:12-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:12-INFO-
20-03-23 17:12-INFO-Epoch 2, Batch 45, Global step 613:
20-03-23 17:12-INFO-training batch loss: 0.0016; avg_loss: 0.0005
20-03-23 17:12-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:12-INFO-
20-03-23 17:12-INFO-Epoch 2, Batch 46, Global step 614:
20-03-23 17:12-INFO-training batch loss: 0.0001; avg_loss: 0.0004
20-03-23 17:12-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:12-INFO-
20-03-23 17:12-INFO-Epoch 2, Batch 47, Global step 615:
20-03-23 17:12-INFO-training batch loss: 0.0004; avg_loss: 0.0004
20-03-23 17:12-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:12-INFO-
20-03-23 17:12-INFO-Epoch 2, Batch 48, Global step 616:
20-03-23 17:12-INFO-training batch loss: 0.0005; avg_loss: 0.0004
20-03-23 17:12-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:12-INFO-
20-03-23 17:12-INFO-Epoch 2, Batch 49, Global step 617:
20-03-23 17:12-INFO-training batch loss: 0.0009; avg_loss: 0.0005
20-03-23 17:12-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:12-INFO-
20-03-23 17:12-INFO-Epoch 2, Batch 50, Global step 618:
20-03-23 17:12-INFO-training batch loss: 0.0001; avg_loss: 0.0004
20-03-23 17:12-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:12-INFO-
20-03-23 17:12-INFO-Epoch 2, Batch 51, Global step 619:
20-03-23 17:12-INFO-training batch loss: 0.0002; avg_loss: 0.0004
20-03-23 17:12-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:12-INFO-
20-03-23 17:12-INFO-Epoch 2, Batch 52, Global step 620:
20-03-23 17:12-INFO-training batch loss: 0.0001; avg_loss: 0.0004
20-03-23 17:12-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:12-INFO-
20-03-23 17:12-INFO-Epoch 2, Batch 53, Global step 621:
20-03-23 17:12-INFO-training batch loss: 0.0043; avg_loss: 0.0005
20-03-23 17:12-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:12-INFO-
20-03-23 17:12-INFO-Epoch 2, Batch 54, Global step 622:
20-03-23 17:12-INFO-training batch loss: 0.0002; avg_loss: 0.0005
20-03-23 17:12-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:12-INFO-
20-03-23 17:12-INFO-Epoch 2, Batch 55, Global step 623:
20-03-23 17:12-INFO-training batch loss: 0.0001; avg_loss: 0.0005
20-03-23 17:12-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:12-INFO-
20-03-23 17:12-INFO-Epoch 2, Batch 56, Global step 624:
20-03-23 17:12-INFO-training batch loss: 0.0005; avg_loss: 0.0005
20-03-23 17:12-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:12-INFO-
20-03-23 17:12-INFO-Epoch 2, Batch 57, Global step 625:
20-03-23 17:12-INFO-training batch loss: 0.0002; avg_loss: 0.0005
20-03-23 17:12-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:12-INFO-
20-03-23 17:12-INFO-Epoch 2, Batch 58, Global step 626:
20-03-23 17:12-INFO-training batch loss: 0.0001; avg_loss: 0.0005
20-03-23 17:12-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:12-INFO-
20-03-23 17:12-INFO-Epoch 2, Batch 59, Global step 627:
20-03-23 17:12-INFO-training batch loss: 0.0031; avg_loss: 0.0005
20-03-23 17:12-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:12-INFO-
20-03-23 17:12-INFO-Epoch 2, Batch 60, Global step 628:
20-03-23 17:12-INFO-training batch loss: 0.0002; avg_loss: 0.0005
20-03-23 17:12-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:12-INFO-
20-03-23 17:12-INFO-Epoch 2, Batch 61, Global step 629:
20-03-23 17:12-INFO-training batch loss: 0.0003; avg_loss: 0.0005
20-03-23 17:12-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:12-INFO-
20-03-23 17:13-INFO-Epoch 2, Batch 62, Global step 630:
20-03-23 17:13-INFO-training batch loss: 0.0001; avg_loss: 0.0005
20-03-23 17:13-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:13-INFO-
20-03-23 17:13-INFO-Epoch 2, Batch 63, Global step 631:
20-03-23 17:13-INFO-training batch loss: 0.0006; avg_loss: 0.0005
20-03-23 17:13-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:13-INFO-
20-03-23 17:13-INFO-Epoch 2, Batch 64, Global step 632:
20-03-23 17:13-INFO-training batch loss: 0.0006; avg_loss: 0.0005
20-03-23 17:13-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:13-INFO-
20-03-23 17:13-INFO-Epoch 2, Batch 65, Global step 633:
20-03-23 17:13-INFO-training batch loss: 0.0003; avg_loss: 0.0005
20-03-23 17:13-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:13-INFO-
20-03-23 17:13-INFO-Epoch 2, Batch 66, Global step 634:
20-03-23 17:13-INFO-training batch loss: 0.0001; avg_loss: 0.0005
20-03-23 17:13-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:13-INFO-
20-03-23 17:13-INFO-Epoch 2, Batch 67, Global step 635:
20-03-23 17:13-INFO-training batch loss: 0.0003; avg_loss: 0.0005
20-03-23 17:13-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:13-INFO-
20-03-23 17:13-INFO-Epoch 2, Batch 68, Global step 636:
20-03-23 17:13-INFO-training batch loss: 0.0004; avg_loss: 0.0005
20-03-23 17:13-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:13-INFO-
20-03-23 17:13-INFO-Epoch 2, Batch 69, Global step 637:
20-03-23 17:13-INFO-training batch loss: 0.0005; avg_loss: 0.0005
20-03-23 17:13-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:13-INFO-
20-03-23 17:13-INFO-Epoch 2, Batch 70, Global step 638:
20-03-23 17:13-INFO-training batch loss: 0.0003; avg_loss: 0.0005
20-03-23 17:13-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:13-INFO-
20-03-23 17:13-INFO-Epoch 2, Batch 71, Global step 639:
20-03-23 17:13-INFO-training batch loss: 0.0014; avg_loss: 0.0005
20-03-23 17:13-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:13-INFO-
20-03-23 17:13-INFO-Epoch 2, Batch 72, Global step 640:
20-03-23 17:13-INFO-training batch loss: 0.0010; avg_loss: 0.0005
20-03-23 17:13-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:13-INFO-
20-03-23 17:13-INFO-Epoch 2, Batch 73, Global step 641:
20-03-23 17:13-INFO-training batch loss: 0.0001; avg_loss: 0.0005
20-03-23 17:13-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:13-INFO-
20-03-23 17:13-INFO-Epoch 2, Batch 74, Global step 642:
20-03-23 17:13-INFO-training batch loss: 0.0002; avg_loss: 0.0005
20-03-23 17:13-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:13-INFO-
20-03-23 17:13-INFO-Epoch 2, Batch 75, Global step 643:
20-03-23 17:13-INFO-training batch loss: 0.0001; avg_loss: 0.0005
20-03-23 17:13-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:13-INFO-
20-03-23 17:13-INFO-Epoch 2, Batch 76, Global step 644:
20-03-23 17:13-INFO-training batch loss: 0.0015; avg_loss: 0.0005
20-03-23 17:13-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:13-INFO-
20-03-23 17:13-INFO-Epoch 2, Batch 77, Global step 645:
20-03-23 17:13-INFO-training batch loss: 0.0002; avg_loss: 0.0005
20-03-23 17:13-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:13-INFO-
20-03-23 17:13-INFO-Epoch 2, Batch 78, Global step 646:
20-03-23 17:13-INFO-training batch loss: 0.0003; avg_loss: 0.0005
20-03-23 17:13-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:13-INFO-
20-03-23 17:13-INFO-Epoch 2, Batch 79, Global step 647:
20-03-23 17:13-INFO-training batch loss: 0.0003; avg_loss: 0.0005
20-03-23 17:13-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:13-INFO-
20-03-23 17:13-INFO-Epoch 2, Batch 80, Global step 648:
20-03-23 17:13-INFO-training batch loss: 0.0004; avg_loss: 0.0005
20-03-23 17:13-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:13-INFO-
20-03-23 17:13-INFO-Epoch 2, Batch 81, Global step 649:
20-03-23 17:13-INFO-training batch loss: 0.0006; avg_loss: 0.0005
20-03-23 17:13-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:13-INFO-
20-03-23 17:13-INFO-Epoch 2, Batch 82, Global step 650:
20-03-23 17:13-INFO-training batch loss: 0.0003; avg_loss: 0.0005
20-03-23 17:13-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:13-INFO-
20-03-23 17:13-INFO-Epoch 2, Batch 83, Global step 651:
20-03-23 17:13-INFO-training batch loss: 0.0005; avg_loss: 0.0005
20-03-23 17:13-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:13-INFO-
20-03-23 17:13-INFO-Epoch 2, Batch 84, Global step 652:
20-03-23 17:13-INFO-training batch loss: 0.0002; avg_loss: 0.0005
20-03-23 17:13-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:13-INFO-
20-03-23 17:13-INFO-Epoch 2, Batch 85, Global step 653:
20-03-23 17:13-INFO-training batch loss: 0.0001; avg_loss: 0.0005
20-03-23 17:13-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:13-INFO-
20-03-23 17:13-INFO-Epoch 2, Batch 86, Global step 654:
20-03-23 17:13-INFO-training batch loss: 0.0013; avg_loss: 0.0005
20-03-23 17:13-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:13-INFO-
20-03-23 17:13-INFO-Epoch 2, Batch 87, Global step 655:
20-03-23 17:13-INFO-training batch loss: 0.0011; avg_loss: 0.0005
20-03-23 17:13-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:13-INFO-
20-03-23 17:13-INFO-Epoch 2, Batch 88, Global step 656:
20-03-23 17:13-INFO-training batch loss: 0.0001; avg_loss: 0.0005
20-03-23 17:13-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:13-INFO-
20-03-23 17:14-INFO-Epoch 2, Batch 89, Global step 657:
20-03-23 17:14-INFO-training batch loss: 0.0001; avg_loss: 0.0005
20-03-23 17:14-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:14-INFO-
20-03-23 17:14-INFO-Epoch 2, Batch 90, Global step 658:
20-03-23 17:14-INFO-training batch loss: 0.0002; avg_loss: 0.0005
20-03-23 17:14-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:14-INFO-
20-03-23 17:14-INFO-Epoch 2, Batch 91, Global step 659:
20-03-23 17:14-INFO-training batch loss: 0.0002; avg_loss: 0.0005
20-03-23 17:14-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:14-INFO-
20-03-23 17:14-INFO-Epoch 2, Batch 92, Global step 660:
20-03-23 17:14-INFO-training batch loss: 0.0007; avg_loss: 0.0005
20-03-23 17:14-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:14-INFO-
20-03-23 17:14-INFO-Epoch 2, Batch 93, Global step 661:
20-03-23 17:14-INFO-training batch loss: 0.0004; avg_loss: 0.0005
20-03-23 17:14-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:14-INFO-
20-03-23 17:14-INFO-Epoch 2, Batch 94, Global step 662:
20-03-23 17:14-INFO-training batch loss: 0.0005; avg_loss: 0.0005
20-03-23 17:14-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:14-INFO-
20-03-23 17:14-INFO-Epoch 2, Batch 95, Global step 663:
20-03-23 17:14-INFO-training batch loss: 0.0005; avg_loss: 0.0005
20-03-23 17:14-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:14-INFO-
20-03-23 17:14-INFO-Epoch 2, Batch 96, Global step 664:
20-03-23 17:14-INFO-training batch loss: 0.0004; avg_loss: 0.0005
20-03-23 17:14-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:14-INFO-
20-03-23 17:14-INFO-Epoch 2, Batch 97, Global step 665:
20-03-23 17:14-INFO-training batch loss: 0.0001; avg_loss: 0.0005
20-03-23 17:14-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:14-INFO-
20-03-23 17:14-INFO-Epoch 2, Batch 98, Global step 666:
20-03-23 17:14-INFO-training batch loss: 0.0004; avg_loss: 0.0005
20-03-23 17:14-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:14-INFO-
20-03-23 17:14-INFO-Epoch 2, Batch 99, Global step 667:
20-03-23 17:14-INFO-training batch loss: 0.0002; avg_loss: 0.0005
20-03-23 17:14-INFO-training batch acc: 1.0000; avg_acc: 1.0000
20-03-23 17:14-INFO-
20-03-23 17:14-INFO-Epoch 2, Batch 100, Global step 668:
20-03-23 17:14-INFO-training batch loss: 0.0089; avg_loss: 0.0006
20-03-23 17:14-INFO-training batch acc: 0.9922; avg_acc: 0.9999
20-03-23 17:14-INFO-
20-03-23 17:14-INFO-Epoch 2, Batch 101, Global step 669:
20-03-23 17:14-INFO-training batch loss: 0.0002; avg_loss: 0.0006
20-03-23 17:14-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:14-INFO-
20-03-23 17:14-INFO-Epoch 2, Batch 102, Global step 670:
20-03-23 17:14-INFO-training batch loss: 0.0002; avg_loss: 0.0006
20-03-23 17:14-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:14-INFO-
20-03-23 17:14-INFO-Epoch 2, Batch 103, Global step 671:
20-03-23 17:14-INFO-training batch loss: 0.0003; avg_loss: 0.0006
20-03-23 17:14-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:14-INFO-
20-03-23 17:14-INFO-Epoch 2, Batch 104, Global step 672:
20-03-23 17:14-INFO-training batch loss: 0.0005; avg_loss: 0.0006
20-03-23 17:14-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:14-INFO-
20-03-23 17:14-INFO-Epoch 2, Batch 105, Global step 673:
20-03-23 17:14-INFO-training batch loss: 0.0049; avg_loss: 0.0006
20-03-23 17:14-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:14-INFO-
20-03-23 17:14-INFO-Epoch 2, Batch 106, Global step 674:
20-03-23 17:14-INFO-training batch loss: 0.0029; avg_loss: 0.0006
20-03-23 17:14-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:14-INFO-
20-03-23 17:14-INFO-Epoch 2, Batch 107, Global step 675:
20-03-23 17:14-INFO-training batch loss: 0.0103; avg_loss: 0.0007
20-03-23 17:14-INFO-training batch acc: 0.9922; avg_acc: 0.9999
20-03-23 17:14-INFO-
20-03-23 17:14-INFO-Epoch 2, Batch 108, Global step 676:
20-03-23 17:14-INFO-training batch loss: 0.0010; avg_loss: 0.0007
20-03-23 17:14-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:14-INFO-
20-03-23 17:14-INFO-Epoch 2, Batch 109, Global step 677:
20-03-23 17:14-INFO-training batch loss: 0.0009; avg_loss: 0.0007
20-03-23 17:14-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:14-INFO-
20-03-23 17:14-INFO-Epoch 2, Batch 110, Global step 678:
20-03-23 17:14-INFO-training batch loss: 0.0002; avg_loss: 0.0007
20-03-23 17:14-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:14-INFO-
20-03-23 17:14-INFO-Epoch 2, Batch 111, Global step 679:
20-03-23 17:14-INFO-training batch loss: 0.0007; avg_loss: 0.0007
20-03-23 17:14-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:14-INFO-
20-03-23 17:14-INFO-Epoch 2, Batch 112, Global step 680:
20-03-23 17:14-INFO-training batch loss: 0.0002; avg_loss: 0.0007
20-03-23 17:14-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:14-INFO-
20-03-23 17:14-INFO-Epoch 2, Batch 113, Global step 681:
20-03-23 17:14-INFO-training batch loss: 0.0004; avg_loss: 0.0007
20-03-23 17:14-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:14-INFO-
20-03-23 17:14-INFO-Epoch 2, Batch 114, Global step 682:
20-03-23 17:14-INFO-training batch loss: 0.0073; avg_loss: 0.0008
20-03-23 17:14-INFO-training batch acc: 0.9922; avg_acc: 0.9998
20-03-23 17:14-INFO-
20-03-23 17:14-INFO-Epoch 2, Batch 115, Global step 683:
20-03-23 17:14-INFO-training batch loss: 0.0008; avg_loss: 0.0008
20-03-23 17:14-INFO-training batch acc: 1.0000; avg_acc: 0.9998
20-03-23 17:14-INFO-
20-03-23 17:15-INFO-Epoch 2, Batch 116, Global step 684:
20-03-23 17:15-INFO-training batch loss: 0.0029; avg_loss: 0.0008
20-03-23 17:15-INFO-training batch acc: 1.0000; avg_acc: 0.9998
20-03-23 17:15-INFO-
20-03-23 17:15-INFO-Epoch 2, Batch 117, Global step 685:
20-03-23 17:15-INFO-training batch loss: 0.0025; avg_loss: 0.0008
20-03-23 17:15-INFO-training batch acc: 1.0000; avg_acc: 0.9998
20-03-23 17:15-INFO-
20-03-23 17:15-INFO-Epoch 2, Batch 118, Global step 686:
20-03-23 17:15-INFO-training batch loss: 0.0014; avg_loss: 0.0008
20-03-23 17:15-INFO-training batch acc: 1.0000; avg_acc: 0.9998
20-03-23 17:15-INFO-
20-03-23 17:15-INFO-Epoch 2, Batch 119, Global step 687:
20-03-23 17:15-INFO-training batch loss: 0.0017; avg_loss: 0.0008
20-03-23 17:15-INFO-training batch acc: 1.0000; avg_acc: 0.9998
20-03-23 17:15-INFO-
20-03-23 17:15-INFO-Epoch 2, Batch 120, Global step 688:
20-03-23 17:15-INFO-training batch loss: 0.0007; avg_loss: 0.0008
20-03-23 17:15-INFO-training batch acc: 1.0000; avg_acc: 0.9998
20-03-23 17:15-INFO-
20-03-23 17:15-INFO-Epoch 2, Batch 121, Global step 689:
20-03-23 17:15-INFO-training batch loss: 0.0003; avg_loss: 0.0008
20-03-23 17:15-INFO-training batch acc: 1.0000; avg_acc: 0.9998
20-03-23 17:15-INFO-
20-03-23 17:15-INFO-Epoch 2, Batch 122, Global step 690:
20-03-23 17:15-INFO-training batch loss: 0.0002; avg_loss: 0.0008
20-03-23 17:15-INFO-training batch acc: 1.0000; avg_acc: 0.9998
20-03-23 17:15-INFO-
20-03-23 17:15-INFO-Epoch 2, Batch 123, Global step 691:
20-03-23 17:15-INFO-training batch loss: 0.0004; avg_loss: 0.0008
20-03-23 17:15-INFO-training batch acc: 1.0000; avg_acc: 0.9998
20-03-23 17:15-INFO-
20-03-23 17:15-INFO-Epoch 2, Batch 124, Global step 692:
20-03-23 17:15-INFO-training batch loss: 0.0002; avg_loss: 0.0008
20-03-23 17:15-INFO-training batch acc: 1.0000; avg_acc: 0.9998
20-03-23 17:15-INFO-
20-03-23 17:15-INFO-Epoch 2, Batch 125, Global step 693:
20-03-23 17:15-INFO-training batch loss: 0.0002; avg_loss: 0.0008
20-03-23 17:15-INFO-training batch acc: 1.0000; avg_acc: 0.9998
20-03-23 17:15-INFO-
20-03-23 17:15-INFO-Epoch 2, Batch 126, Global step 694:
20-03-23 17:15-INFO-training batch loss: 0.0002; avg_loss: 0.0008
20-03-23 17:15-INFO-training batch acc: 1.0000; avg_acc: 0.9998
20-03-23 17:15-INFO-
20-03-23 17:15-INFO-Epoch 2, Batch 127, Global step 695:
20-03-23 17:15-INFO-training batch loss: 0.0004; avg_loss: 0.0008
20-03-23 17:15-INFO-training batch acc: 1.0000; avg_acc: 0.9998
20-03-23 17:15-INFO-
20-03-23 17:15-INFO-Epoch 2, Batch 128, Global step 696:
20-03-23 17:15-INFO-training batch loss: 0.0006; avg_loss: 0.0008
20-03-23 17:15-INFO-training batch acc: 1.0000; avg_acc: 0.9998
20-03-23 17:15-INFO-
20-03-23 17:15-INFO-Epoch 2, Batch 129, Global step 697:
20-03-23 17:15-INFO-training batch loss: 0.0010; avg_loss: 0.0008
20-03-23 17:15-INFO-training batch acc: 1.0000; avg_acc: 0.9998
20-03-23 17:15-INFO-
20-03-23 17:15-INFO-Epoch 2, Batch 130, Global step 698:
20-03-23 17:15-INFO-training batch loss: 0.0003; avg_loss: 0.0008
20-03-23 17:15-INFO-training batch acc: 1.0000; avg_acc: 0.9998
20-03-23 17:15-INFO-
20-03-23 17:15-INFO-Epoch 2, Batch 131, Global step 699:
20-03-23 17:15-INFO-training batch loss: 0.0004; avg_loss: 0.0008
20-03-23 17:15-INFO-training batch acc: 1.0000; avg_acc: 0.9998
20-03-23 17:15-INFO-
20-03-23 17:15-INFO-Epoch 2, Batch 132, Global step 700:
20-03-23 17:15-INFO-training batch loss: 0.0008; avg_loss: 0.0008
20-03-23 17:15-INFO-training batch acc: 1.0000; avg_acc: 0.9998
20-03-23 17:15-INFO-
20-03-23 17:15-INFO-Epoch 2, Batch 133, Global step 701:
20-03-23 17:15-INFO-training batch loss: 0.0011; avg_loss: 0.0008
20-03-23 17:15-INFO-training batch acc: 1.0000; avg_acc: 0.9998
20-03-23 17:15-INFO-
20-03-23 17:15-INFO-Epoch 2, Batch 134, Global step 702:
20-03-23 17:15-INFO-training batch loss: 0.0006; avg_loss: 0.0008
20-03-23 17:15-INFO-training batch acc: 1.0000; avg_acc: 0.9998
20-03-23 17:15-INFO-
20-03-23 17:15-INFO-Epoch 2, Batch 135, Global step 703:
20-03-23 17:15-INFO-training batch loss: 0.0009; avg_loss: 0.0008
20-03-23 17:15-INFO-training batch acc: 1.0000; avg_acc: 0.9998
20-03-23 17:15-INFO-
20-03-23 17:15-INFO-Epoch 2, Batch 136, Global step 704:
20-03-23 17:15-INFO-training batch loss: 0.0006; avg_loss: 0.0008
20-03-23 17:15-INFO-training batch acc: 1.0000; avg_acc: 0.9998
20-03-23 17:15-INFO-
20-03-23 17:15-INFO-Epoch 2, Batch 137, Global step 705:
20-03-23 17:15-INFO-training batch loss: 0.0002; avg_loss: 0.0008
20-03-23 17:15-INFO-training batch acc: 1.0000; avg_acc: 0.9998
20-03-23 17:15-INFO-
20-03-23 17:15-INFO-Epoch 2, Batch 138, Global step 706:
20-03-23 17:15-INFO-training batch loss: 0.0005; avg_loss: 0.0008
20-03-23 17:15-INFO-training batch acc: 1.0000; avg_acc: 0.9998
20-03-23 17:15-INFO-
20-03-23 17:15-INFO-Epoch 2, Batch 139, Global step 707:
20-03-23 17:15-INFO-training batch loss: 0.0004; avg_loss: 0.0008
20-03-23 17:15-INFO-training batch acc: 1.0000; avg_acc: 0.9998
20-03-23 17:15-INFO-
20-03-23 17:15-INFO-Epoch 2, Batch 140, Global step 708:
20-03-23 17:15-INFO-training batch loss: 0.0002; avg_loss: 0.0008
20-03-23 17:15-INFO-training batch acc: 1.0000; avg_acc: 0.9998
20-03-23 17:15-INFO-
20-03-23 17:15-INFO-Epoch 2, Batch 141, Global step 709:
20-03-23 17:15-INFO-training batch loss: 0.0004; avg_loss: 0.0008
20-03-23 17:15-INFO-training batch acc: 1.0000; avg_acc: 0.9998
20-03-23 17:15-INFO-
20-03-23 17:16-INFO-Epoch 2, Batch 142, Global step 710:
20-03-23 17:16-INFO-training batch loss: 0.0009; avg_loss: 0.0008
20-03-23 17:16-INFO-training batch acc: 1.0000; avg_acc: 0.9998
20-03-23 17:16-INFO-
20-03-23 17:16-INFO-Epoch 2, Batch 143, Global step 711:
20-03-23 17:16-INFO-training batch loss: 0.0005; avg_loss: 0.0008
20-03-23 17:16-INFO-training batch acc: 1.0000; avg_acc: 0.9998
20-03-23 17:16-INFO-
20-03-23 17:16-INFO-Epoch 2, Batch 144, Global step 712:
20-03-23 17:16-INFO-training batch loss: 0.0003; avg_loss: 0.0008
20-03-23 17:16-INFO-training batch acc: 1.0000; avg_acc: 0.9998
20-03-23 17:16-INFO-
20-03-23 17:16-INFO-Epoch 2, Batch 145, Global step 713:
20-03-23 17:16-INFO-training batch loss: 0.0002; avg_loss: 0.0008
20-03-23 17:16-INFO-training batch acc: 1.0000; avg_acc: 0.9998
20-03-23 17:16-INFO-
20-03-23 17:16-INFO-Epoch 2, Batch 146, Global step 714:
20-03-23 17:16-INFO-training batch loss: 0.0001; avg_loss: 0.0007
20-03-23 17:16-INFO-training batch acc: 1.0000; avg_acc: 0.9998
20-03-23 17:16-INFO-
20-03-23 17:16-INFO-Epoch 2, Batch 147, Global step 715:
20-03-23 17:16-INFO-training batch loss: 0.0004; avg_loss: 0.0007
20-03-23 17:16-INFO-training batch acc: 1.0000; avg_acc: 0.9998
20-03-23 17:16-INFO-
20-03-23 17:16-INFO-Epoch 2, Batch 148, Global step 716:
20-03-23 17:16-INFO-training batch loss: 0.0001; avg_loss: 0.0007
20-03-23 17:16-INFO-training batch acc: 1.0000; avg_acc: 0.9998
20-03-23 17:16-INFO-
20-03-23 17:16-INFO-Epoch 2, Batch 149, Global step 717:
20-03-23 17:16-INFO-training batch loss: 0.0003; avg_loss: 0.0007
20-03-23 17:16-INFO-training batch acc: 1.0000; avg_acc: 0.9998
20-03-23 17:16-INFO-
20-03-23 17:16-INFO-Epoch 2, Batch 150, Global step 718:
20-03-23 17:16-INFO-training batch loss: 0.0002; avg_loss: 0.0007
20-03-23 17:16-INFO-training batch acc: 1.0000; avg_acc: 0.9998
20-03-23 17:16-INFO-
20-03-23 17:16-INFO-Epoch 2, Batch 151, Global step 719:
20-03-23 17:16-INFO-training batch loss: 0.0004; avg_loss: 0.0007
20-03-23 17:16-INFO-training batch acc: 1.0000; avg_acc: 0.9998
20-03-23 17:16-INFO-
20-03-23 17:16-INFO-Epoch 2, Batch 152, Global step 720:
20-03-23 17:16-INFO-training batch loss: 0.0002; avg_loss: 0.0007
20-03-23 17:16-INFO-training batch acc: 1.0000; avg_acc: 0.9998
20-03-23 17:16-INFO-
20-03-23 17:16-INFO-Epoch 2, Batch 153, Global step 721:
20-03-23 17:16-INFO-training batch loss: 0.0003; avg_loss: 0.0007
20-03-23 17:16-INFO-training batch acc: 1.0000; avg_acc: 0.9998
20-03-23 17:16-INFO-
20-03-23 17:16-INFO-Epoch 2, Batch 154, Global step 722:
20-03-23 17:16-INFO-training batch loss: 0.0003; avg_loss: 0.0007
20-03-23 17:16-INFO-training batch acc: 1.0000; avg_acc: 0.9998
20-03-23 17:16-INFO-
20-03-23 17:16-INFO-Epoch 2, Batch 155, Global step 723:
20-03-23 17:16-INFO-training batch loss: 0.0003; avg_loss: 0.0007
20-03-23 17:16-INFO-training batch acc: 1.0000; avg_acc: 0.9998
20-03-23 17:16-INFO-
20-03-23 17:16-INFO-Epoch 2, Batch 156, Global step 724:
20-03-23 17:16-INFO-training batch loss: 0.0001; avg_loss: 0.0007
20-03-23 17:16-INFO-training batch acc: 1.0000; avg_acc: 0.9998
20-03-23 17:16-INFO-
20-03-23 17:16-INFO-Epoch 2, Batch 157, Global step 725:
20-03-23 17:16-INFO-training batch loss: 0.0003; avg_loss: 0.0007
20-03-23 17:16-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:16-INFO-
20-03-23 17:16-INFO-Epoch 2, Batch 158, Global step 726:
20-03-23 17:16-INFO-training batch loss: 0.0002; avg_loss: 0.0007
20-03-23 17:16-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:16-INFO-
20-03-23 17:16-INFO-Epoch 2, Batch 159, Global step 727:
20-03-23 17:16-INFO-training batch loss: 0.0003; avg_loss: 0.0007
20-03-23 17:16-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:16-INFO-
20-03-23 17:16-INFO-Epoch 2, Batch 160, Global step 728:
20-03-23 17:16-INFO-training batch loss: 0.0001; avg_loss: 0.0007
20-03-23 17:16-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:16-INFO-
20-03-23 17:16-INFO-Epoch 2, Batch 161, Global step 729:
20-03-23 17:16-INFO-training batch loss: 0.0008; avg_loss: 0.0007
20-03-23 17:16-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:16-INFO-
20-03-23 17:16-INFO-Epoch 2, Batch 162, Global step 730:
20-03-23 17:16-INFO-training batch loss: 0.0001; avg_loss: 0.0007
20-03-23 17:16-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:16-INFO-
20-03-23 17:16-INFO-Epoch 2, Batch 163, Global step 731:
20-03-23 17:16-INFO-training batch loss: 0.0003; avg_loss: 0.0007
20-03-23 17:16-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:16-INFO-
20-03-23 17:16-INFO-Epoch 2, Batch 164, Global step 732:
20-03-23 17:16-INFO-training batch loss: 0.0002; avg_loss: 0.0007
20-03-23 17:16-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:16-INFO-
20-03-23 17:16-INFO-Epoch 2, Batch 165, Global step 733:
20-03-23 17:16-INFO-training batch loss: 0.0005; avg_loss: 0.0007
20-03-23 17:16-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:16-INFO-
20-03-23 17:16-INFO-Epoch 2, Batch 166, Global step 734:
20-03-23 17:16-INFO-training batch loss: 0.0004; avg_loss: 0.0007
20-03-23 17:16-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:16-INFO-
20-03-23 17:16-INFO-Epoch 2, Batch 167, Global step 735:
20-03-23 17:16-INFO-training batch loss: 0.0004; avg_loss: 0.0007
20-03-23 17:16-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:16-INFO-
20-03-23 17:16-INFO-Epoch 2, Batch 168, Global step 736:
20-03-23 17:16-INFO-training batch loss: 0.0003; avg_loss: 0.0007
20-03-23 17:16-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:16-INFO-
20-03-23 17:17-INFO-Epoch 2, Batch 169, Global step 737:
20-03-23 17:17-INFO-training batch loss: 0.0003; avg_loss: 0.0007
20-03-23 17:17-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:17-INFO-
20-03-23 17:17-INFO-Epoch 2, Batch 170, Global step 738:
20-03-23 17:17-INFO-training batch loss: 0.0002; avg_loss: 0.0007
20-03-23 17:17-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:17-INFO-
20-03-23 17:17-INFO-Epoch 2, Batch 171, Global step 739:
20-03-23 17:17-INFO-training batch loss: 0.0001; avg_loss: 0.0007
20-03-23 17:17-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:17-INFO-
20-03-23 17:17-INFO-Epoch 2, Batch 172, Global step 740:
20-03-23 17:17-INFO-training batch loss: 0.0014; avg_loss: 0.0007
20-03-23 17:17-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:17-INFO-
20-03-23 17:17-INFO-Epoch 2, Batch 173, Global step 741:
20-03-23 17:17-INFO-training batch loss: 0.0003; avg_loss: 0.0007
20-03-23 17:17-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:17-INFO-
20-03-23 17:17-INFO-Epoch 2, Batch 174, Global step 742:
20-03-23 17:17-INFO-training batch loss: 0.0002; avg_loss: 0.0007
20-03-23 17:17-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:17-INFO-
20-03-23 17:17-INFO-Epoch 2, Batch 175, Global step 743:
20-03-23 17:17-INFO-training batch loss: 0.0002; avg_loss: 0.0007
20-03-23 17:17-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:17-INFO-
20-03-23 17:17-INFO-Epoch 2, Batch 176, Global step 744:
20-03-23 17:17-INFO-training batch loss: 0.0002; avg_loss: 0.0007
20-03-23 17:17-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:17-INFO-
20-03-23 17:17-INFO-Epoch 2, Batch 177, Global step 745:
20-03-23 17:17-INFO-training batch loss: 0.0002; avg_loss: 0.0007
20-03-23 17:17-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:17-INFO-
20-03-23 17:17-INFO-Epoch 2, Batch 178, Global step 746:
20-03-23 17:17-INFO-training batch loss: 0.0002; avg_loss: 0.0007
20-03-23 17:17-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:17-INFO-
20-03-23 17:17-INFO-Epoch 2, Batch 179, Global step 747:
20-03-23 17:17-INFO-training batch loss: 0.0001; avg_loss: 0.0007
20-03-23 17:17-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:17-INFO-
20-03-23 17:17-INFO-Epoch 2, Batch 180, Global step 748:
20-03-23 17:17-INFO-training batch loss: 0.0004; avg_loss: 0.0007
20-03-23 17:17-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:17-INFO-
20-03-23 17:17-INFO-Epoch 2, Batch 181, Global step 749:
20-03-23 17:17-INFO-training batch loss: 0.0002; avg_loss: 0.0007
20-03-23 17:17-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:17-INFO-
20-03-23 17:17-INFO-Epoch 2, Batch 182, Global step 750:
20-03-23 17:17-INFO-training batch loss: 0.0003; avg_loss: 0.0007
20-03-23 17:17-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:17-INFO-
20-03-23 17:17-INFO-Epoch 2, Batch 183, Global step 751:
20-03-23 17:17-INFO-training batch loss: 0.0001; avg_loss: 0.0007
20-03-23 17:17-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:17-INFO-
20-03-23 17:17-INFO-Epoch 2, Batch 184, Global step 752:
20-03-23 17:17-INFO-training batch loss: 0.0002; avg_loss: 0.0007
20-03-23 17:17-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:17-INFO-
20-03-23 17:17-INFO-Epoch 2, Batch 185, Global step 753:
20-03-23 17:17-INFO-training batch loss: 0.0002; avg_loss: 0.0007
20-03-23 17:17-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:17-INFO-
20-03-23 17:17-INFO-Epoch 2, Batch 186, Global step 754:
20-03-23 17:17-INFO-training batch loss: 0.0001; avg_loss: 0.0006
20-03-23 17:17-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:17-INFO-
20-03-23 17:17-INFO-Epoch 2, Batch 187, Global step 755:
20-03-23 17:17-INFO-training batch loss: 0.0003; avg_loss: 0.0006
20-03-23 17:17-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:17-INFO-
20-03-23 17:17-INFO-Epoch 2, Batch 188, Global step 756:
20-03-23 17:17-INFO-training batch loss: 0.0003; avg_loss: 0.0006
20-03-23 17:17-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:17-INFO-
20-03-23 17:17-INFO-Epoch 2, Batch 189, Global step 757:
20-03-23 17:17-INFO-training batch loss: 0.0001; avg_loss: 0.0006
20-03-23 17:17-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:17-INFO-
20-03-23 17:17-INFO-Epoch 2, Batch 190, Global step 758:
20-03-23 17:17-INFO-training batch loss: 0.0002; avg_loss: 0.0006
20-03-23 17:17-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:17-INFO-
20-03-23 17:17-INFO-Epoch 2, Batch 191, Global step 759:
20-03-23 17:17-INFO-training batch loss: 0.0015; avg_loss: 0.0006
20-03-23 17:17-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:17-INFO-
20-03-23 17:17-INFO-Epoch 2, Batch 192, Global step 760:
20-03-23 17:17-INFO-training batch loss: 0.0003; avg_loss: 0.0006
20-03-23 17:17-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:17-INFO-
20-03-23 17:17-INFO-Epoch 2, Batch 193, Global step 761:
20-03-23 17:17-INFO-training batch loss: 0.0004; avg_loss: 0.0006
20-03-23 17:17-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:17-INFO-
20-03-23 17:17-INFO-Epoch 2, Batch 194, Global step 762:
20-03-23 17:17-INFO-training batch loss: 0.0002; avg_loss: 0.0006
20-03-23 17:17-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:17-INFO-
20-03-23 17:17-INFO-Epoch 2, Batch 195, Global step 763:
20-03-23 17:17-INFO-training batch loss: 0.0003; avg_loss: 0.0006
20-03-23 17:17-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:17-INFO-
20-03-23 17:18-INFO-Epoch 2, Batch 196, Global step 764:
20-03-23 17:18-INFO-training batch loss: 0.0004; avg_loss: 0.0006
20-03-23 17:18-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:18-INFO-
20-03-23 17:18-INFO-Epoch 2, Batch 197, Global step 765:
20-03-23 17:18-INFO-training batch loss: 0.0002; avg_loss: 0.0006
20-03-23 17:18-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:18-INFO-
20-03-23 17:18-INFO-Epoch 2, Batch 198, Global step 766:
20-03-23 17:18-INFO-training batch loss: 0.0003; avg_loss: 0.0006
20-03-23 17:18-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:18-INFO-
20-03-23 17:18-INFO-Epoch 2, Batch 199, Global step 767:
20-03-23 17:18-INFO-training batch loss: 0.0003; avg_loss: 0.0006
20-03-23 17:18-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:18-INFO-
20-03-23 17:18-INFO-Epoch 2, Batch 200, Global step 768:
20-03-23 17:18-INFO-training batch loss: 0.0002; avg_loss: 0.0006
20-03-23 17:18-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:18-INFO-
20-03-23 17:18-INFO-Epoch 2, Batch 201, Global step 769:
20-03-23 17:18-INFO-training batch loss: 0.0002; avg_loss: 0.0006
20-03-23 17:18-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:18-INFO-
20-03-23 17:18-INFO-Epoch 2, Batch 202, Global step 770:
20-03-23 17:18-INFO-training batch loss: 0.0007; avg_loss: 0.0006
20-03-23 17:18-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:18-INFO-
20-03-23 17:18-INFO-Epoch 2, Batch 203, Global step 771:
20-03-23 17:18-INFO-training batch loss: 0.0002; avg_loss: 0.0006
20-03-23 17:18-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:18-INFO-
20-03-23 17:18-INFO-Epoch 2, Batch 204, Global step 772:
20-03-23 17:18-INFO-training batch loss: 0.0002; avg_loss: 0.0006
20-03-23 17:18-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:18-INFO-
20-03-23 17:18-INFO-Epoch 2, Batch 205, Global step 773:
20-03-23 17:18-INFO-training batch loss: 0.0002; avg_loss: 0.0006
20-03-23 17:18-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:18-INFO-
20-03-23 17:18-INFO-Epoch 2, Batch 206, Global step 774:
20-03-23 17:18-INFO-training batch loss: 0.0002; avg_loss: 0.0006
20-03-23 17:18-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:18-INFO-
20-03-23 17:18-INFO-Epoch 2, Batch 207, Global step 775:
20-03-23 17:18-INFO-training batch loss: 0.0004; avg_loss: 0.0006
20-03-23 17:18-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:18-INFO-
20-03-23 17:18-INFO-Epoch 2, Batch 208, Global step 776:
20-03-23 17:18-INFO-training batch loss: 0.0002; avg_loss: 0.0006
20-03-23 17:18-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:18-INFO-
20-03-23 17:18-INFO-Epoch 2, Batch 209, Global step 777:
20-03-23 17:18-INFO-training batch loss: 0.0001; avg_loss: 0.0006
20-03-23 17:18-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:18-INFO-
20-03-23 17:18-INFO-Epoch 2, Batch 210, Global step 778:
20-03-23 17:18-INFO-training batch loss: 0.0005; avg_loss: 0.0006
20-03-23 17:18-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:18-INFO-
20-03-23 17:18-INFO-Epoch 2, Batch 211, Global step 779:
20-03-23 17:18-INFO-training batch loss: 0.0001; avg_loss: 0.0006
20-03-23 17:18-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:18-INFO-
20-03-23 17:18-INFO-Epoch 2, Batch 212, Global step 780:
20-03-23 17:18-INFO-training batch loss: 0.0001; avg_loss: 0.0006
20-03-23 17:18-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:18-INFO-
20-03-23 17:18-INFO-Epoch 2, Batch 213, Global step 781:
20-03-23 17:18-INFO-training batch loss: 0.0002; avg_loss: 0.0006
20-03-23 17:18-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:18-INFO-
20-03-23 17:18-INFO-Epoch 2, Batch 214, Global step 782:
20-03-23 17:18-INFO-training batch loss: 0.0001; avg_loss: 0.0006
20-03-23 17:18-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:18-INFO-
20-03-23 17:18-INFO-Epoch 2, Batch 215, Global step 783:
20-03-23 17:18-INFO-training batch loss: 0.0005; avg_loss: 0.0006
20-03-23 17:18-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:18-INFO-
20-03-23 17:18-INFO-Epoch 2, Batch 216, Global step 784:
20-03-23 17:18-INFO-training batch loss: 0.0001; avg_loss: 0.0006
20-03-23 17:18-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:18-INFO-
20-03-23 17:18-INFO-Epoch 2, Batch 217, Global step 785:
20-03-23 17:18-INFO-training batch loss: 0.0004; avg_loss: 0.0006
20-03-23 17:18-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:18-INFO-
20-03-23 17:18-INFO-Epoch 2, Batch 218, Global step 786:
20-03-23 17:18-INFO-training batch loss: 0.0002; avg_loss: 0.0006
20-03-23 17:18-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:18-INFO-
20-03-23 17:18-INFO-Epoch 2, Batch 219, Global step 787:
20-03-23 17:18-INFO-training batch loss: 0.0002; avg_loss: 0.0006
20-03-23 17:18-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:18-INFO-
20-03-23 17:18-INFO-Epoch 2, Batch 220, Global step 788:
20-03-23 17:18-INFO-training batch loss: 0.0001; avg_loss: 0.0006
20-03-23 17:18-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:18-INFO-
20-03-23 17:18-INFO-Epoch 2, Batch 221, Global step 789:
20-03-23 17:18-INFO-training batch loss: 0.0001; avg_loss: 0.0006
20-03-23 17:18-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:18-INFO-
20-03-23 17:19-INFO-Epoch 2, Batch 222, Global step 790:
20-03-23 17:19-INFO-training batch loss: 0.0004; avg_loss: 0.0006
20-03-23 17:19-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:19-INFO-
20-03-23 17:19-INFO-Epoch 2, Batch 223, Global step 791:
20-03-23 17:19-INFO-training batch loss: 0.0002; avg_loss: 0.0006
20-03-23 17:19-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:19-INFO-
20-03-23 17:19-INFO-Epoch 2, Batch 224, Global step 792:
20-03-23 17:19-INFO-training batch loss: 0.0001; avg_loss: 0.0006
20-03-23 17:19-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:19-INFO-
20-03-23 17:19-INFO-Epoch 2, Batch 225, Global step 793:
20-03-23 17:19-INFO-training batch loss: 0.0003; avg_loss: 0.0006
20-03-23 17:19-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:19-INFO-
20-03-23 17:19-INFO-Epoch 2, Batch 226, Global step 794:
20-03-23 17:19-INFO-training batch loss: 0.0002; avg_loss: 0.0006
20-03-23 17:19-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:19-INFO-
20-03-23 17:19-INFO-Epoch 2, Batch 227, Global step 795:
20-03-23 17:19-INFO-training batch loss: 0.0001; avg_loss: 0.0006
20-03-23 17:19-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:19-INFO-
20-03-23 17:19-INFO-Epoch 2, Batch 228, Global step 796:
20-03-23 17:19-INFO-training batch loss: 0.0002; avg_loss: 0.0006
20-03-23 17:19-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:19-INFO-
20-03-23 17:19-INFO-Epoch 2, Batch 229, Global step 797:
20-03-23 17:19-INFO-training batch loss: 0.0002; avg_loss: 0.0006
20-03-23 17:19-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:19-INFO-
20-03-23 17:19-INFO-Epoch 2, Batch 230, Global step 798:
20-03-23 17:19-INFO-training batch loss: 0.0002; avg_loss: 0.0006
20-03-23 17:19-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:19-INFO-
20-03-23 17:19-INFO-Epoch 2, Batch 231, Global step 799:
20-03-23 17:19-INFO-training batch loss: 0.0002; avg_loss: 0.0006
20-03-23 17:19-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:19-INFO-
20-03-23 17:19-INFO-Epoch 2, Batch 232, Global step 800:
20-03-23 17:19-INFO-training batch loss: 0.0001; avg_loss: 0.0006
20-03-23 17:19-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:19-INFO-
20-03-23 17:19-INFO-Epoch 2, Batch 233, Global step 801:
20-03-23 17:19-INFO-training batch loss: 0.0001; avg_loss: 0.0006
20-03-23 17:19-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:19-INFO-
20-03-23 17:19-INFO-Epoch 2, Batch 234, Global step 802:
20-03-23 17:19-INFO-training batch loss: 0.0002; avg_loss: 0.0006
20-03-23 17:19-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:19-INFO-
20-03-23 17:19-INFO-Epoch 2, Batch 235, Global step 803:
20-03-23 17:19-INFO-training batch loss: 0.0002; avg_loss: 0.0006
20-03-23 17:19-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:19-INFO-
20-03-23 17:19-INFO-Epoch 2, Batch 236, Global step 804:
20-03-23 17:19-INFO-training batch loss: 0.0006; avg_loss: 0.0006
20-03-23 17:19-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:19-INFO-
20-03-23 17:19-INFO-Epoch 2, Batch 237, Global step 805:
20-03-23 17:19-INFO-training batch loss: 0.0002; avg_loss: 0.0006
20-03-23 17:19-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:19-INFO-
20-03-23 17:19-INFO-Epoch 2, Batch 238, Global step 806:
20-03-23 17:19-INFO-training batch loss: 0.0002; avg_loss: 0.0006
20-03-23 17:19-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:19-INFO-
20-03-23 17:19-INFO-Epoch 2, Batch 239, Global step 807:
20-03-23 17:19-INFO-training batch loss: 0.0003; avg_loss: 0.0006
20-03-23 17:19-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:19-INFO-
20-03-23 17:19-INFO-Epoch 2, Batch 240, Global step 808:
20-03-23 17:19-INFO-training batch loss: 0.0004; avg_loss: 0.0006
20-03-23 17:19-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:19-INFO-
20-03-23 17:19-INFO-Epoch 2, Batch 241, Global step 809:
20-03-23 17:19-INFO-training batch loss: 0.0002; avg_loss: 0.0006
20-03-23 17:19-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:19-INFO-
20-03-23 17:19-INFO-Epoch 2, Batch 242, Global step 810:
20-03-23 17:19-INFO-training batch loss: 0.0002; avg_loss: 0.0006
20-03-23 17:19-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:19-INFO-
20-03-23 17:19-INFO-Epoch 2, Batch 243, Global step 811:
20-03-23 17:19-INFO-training batch loss: 0.0001; avg_loss: 0.0006
20-03-23 17:19-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:19-INFO-
20-03-23 17:19-INFO-Epoch 2, Batch 244, Global step 812:
20-03-23 17:19-INFO-training batch loss: 0.0002; avg_loss: 0.0006
20-03-23 17:19-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:19-INFO-
20-03-23 17:19-INFO-Epoch 2, Batch 245, Global step 813:
20-03-23 17:19-INFO-training batch loss: 0.0001; avg_loss: 0.0006
20-03-23 17:19-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:19-INFO-
20-03-23 17:19-INFO-Epoch 2, Batch 246, Global step 814:
20-03-23 17:19-INFO-training batch loss: 0.0002; avg_loss: 0.0006
20-03-23 17:19-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:19-INFO-
20-03-23 17:19-INFO-Epoch 2, Batch 247, Global step 815:
20-03-23 17:19-INFO-training batch loss: 0.0002; avg_loss: 0.0006
20-03-23 17:19-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:19-INFO-
20-03-23 17:19-INFO-Epoch 2, Batch 248, Global step 816:
20-03-23 17:19-INFO-training batch loss: 0.0009; avg_loss: 0.0006
20-03-23 17:19-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:19-INFO-
20-03-23 17:20-INFO-Epoch 2, Batch 249, Global step 817:
20-03-23 17:20-INFO-training batch loss: 0.0008; avg_loss: 0.0006
20-03-23 17:20-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:20-INFO-
20-03-23 17:20-INFO-Epoch 2, Batch 250, Global step 818:
20-03-23 17:20-INFO-training batch loss: 0.0003; avg_loss: 0.0006
20-03-23 17:20-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:20-INFO-
20-03-23 17:20-INFO-Epoch 2, Batch 251, Global step 819:
20-03-23 17:20-INFO-training batch loss: 0.0001; avg_loss: 0.0006
20-03-23 17:20-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:20-INFO-
20-03-23 17:20-INFO-Epoch 2, Batch 252, Global step 820:
20-03-23 17:20-INFO-training batch loss: 0.0001; avg_loss: 0.0005
20-03-23 17:20-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:20-INFO-
20-03-23 17:20-INFO-Epoch 2, Batch 253, Global step 821:
20-03-23 17:20-INFO-training batch loss: 0.0002; avg_loss: 0.0005
20-03-23 17:20-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:20-INFO-
20-03-23 17:20-INFO-Epoch 2, Batch 254, Global step 822:
20-03-23 17:20-INFO-training batch loss: 0.0004; avg_loss: 0.0005
20-03-23 17:20-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:20-INFO-
20-03-23 17:20-INFO-Epoch 2, Batch 255, Global step 823:
20-03-23 17:20-INFO-training batch loss: 0.0007; avg_loss: 0.0005
20-03-23 17:20-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:20-INFO-
20-03-23 17:20-INFO-Epoch 2, Batch 256, Global step 824:
20-03-23 17:20-INFO-training batch loss: 0.0002; avg_loss: 0.0005
20-03-23 17:20-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:20-INFO-
20-03-23 17:20-INFO-Epoch 2, Batch 257, Global step 825:
20-03-23 17:20-INFO-training batch loss: 0.0003; avg_loss: 0.0005
20-03-23 17:20-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:20-INFO-
20-03-23 17:20-INFO-Epoch 2, Batch 258, Global step 826:
20-03-23 17:20-INFO-training batch loss: 0.0002; avg_loss: 0.0005
20-03-23 17:20-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:20-INFO-
20-03-23 17:20-INFO-Epoch 2, Batch 259, Global step 827:
20-03-23 17:20-INFO-training batch loss: 0.0004; avg_loss: 0.0005
20-03-23 17:20-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:20-INFO-
20-03-23 17:20-INFO-Epoch 2, Batch 260, Global step 828:
20-03-23 17:20-INFO-training batch loss: 0.0001; avg_loss: 0.0005
20-03-23 17:20-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:20-INFO-
20-03-23 17:20-INFO-Epoch 2, Batch 261, Global step 829:
20-03-23 17:20-INFO-training batch loss: 0.0001; avg_loss: 0.0005
20-03-23 17:20-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:20-INFO-
20-03-23 17:20-INFO-Epoch 2, Batch 262, Global step 830:
20-03-23 17:20-INFO-training batch loss: 0.0003; avg_loss: 0.0005
20-03-23 17:20-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:20-INFO-
20-03-23 17:20-INFO-Epoch 2, Batch 263, Global step 831:
20-03-23 17:20-INFO-training batch loss: 0.0005; avg_loss: 0.0005
20-03-23 17:20-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:20-INFO-
20-03-23 17:20-INFO-Epoch 2, Batch 264, Global step 832:
20-03-23 17:20-INFO-training batch loss: 0.0002; avg_loss: 0.0005
20-03-23 17:20-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:20-INFO-
20-03-23 17:20-INFO-Epoch 2, Batch 265, Global step 833:
20-03-23 17:20-INFO-training batch loss: 0.0002; avg_loss: 0.0005
20-03-23 17:20-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:20-INFO-
20-03-23 17:20-INFO-Epoch 2, Batch 266, Global step 834:
20-03-23 17:20-INFO-training batch loss: 0.0006; avg_loss: 0.0005
20-03-23 17:20-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:20-INFO-
20-03-23 17:20-INFO-Epoch 2, Batch 267, Global step 835:
20-03-23 17:20-INFO-training batch loss: 0.0002; avg_loss: 0.0005
20-03-23 17:20-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:20-INFO-
20-03-23 17:20-INFO-Epoch 2, Batch 268, Global step 836:
20-03-23 17:20-INFO-training batch loss: 0.0001; avg_loss: 0.0005
20-03-23 17:20-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:20-INFO-
20-03-23 17:20-INFO-Epoch 2, Batch 269, Global step 837:
20-03-23 17:20-INFO-training batch loss: 0.0001; avg_loss: 0.0005
20-03-23 17:20-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:20-INFO-
20-03-23 17:20-INFO-Epoch 2, Batch 270, Global step 838:
20-03-23 17:20-INFO-training batch loss: 0.0002; avg_loss: 0.0005
20-03-23 17:20-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:20-INFO-
20-03-23 17:20-INFO-Epoch 2, Batch 271, Global step 839:
20-03-23 17:20-INFO-training batch loss: 0.0002; avg_loss: 0.0005
20-03-23 17:20-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:20-INFO-
20-03-23 17:20-INFO-Epoch 2, Batch 272, Global step 840:
20-03-23 17:20-INFO-training batch loss: 0.0002; avg_loss: 0.0005
20-03-23 17:20-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:20-INFO-
20-03-23 17:20-INFO-Epoch 2, Batch 273, Global step 841:
20-03-23 17:20-INFO-training batch loss: 0.0005; avg_loss: 0.0005
20-03-23 17:20-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:20-INFO-
20-03-23 17:20-INFO-Epoch 2, Batch 274, Global step 842:
20-03-23 17:20-INFO-training batch loss: 0.0004; avg_loss: 0.0005
20-03-23 17:20-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:20-INFO-
20-03-23 17:21-INFO-Epoch 2, Batch 275, Global step 843:
20-03-23 17:21-INFO-training batch loss: 0.0002; avg_loss: 0.0005
20-03-23 17:21-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:21-INFO-
20-03-23 17:21-INFO-Epoch 2, Batch 276, Global step 844:
20-03-23 17:21-INFO-training batch loss: 0.0002; avg_loss: 0.0005
20-03-23 17:21-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:21-INFO-
20-03-23 17:21-INFO-Epoch 2, Batch 277, Global step 845:
20-03-23 17:21-INFO-training batch loss: 0.0006; avg_loss: 0.0005
20-03-23 17:21-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:21-INFO-
20-03-23 17:21-INFO-Epoch 2, Batch 278, Global step 846:
20-03-23 17:21-INFO-training batch loss: 0.0002; avg_loss: 0.0005
20-03-23 17:21-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:21-INFO-
20-03-23 17:21-INFO-Epoch 2, Batch 279, Global step 847:
20-03-23 17:21-INFO-training batch loss: 0.0002; avg_loss: 0.0005
20-03-23 17:21-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:21-INFO-
20-03-23 17:21-INFO-Epoch 2, Batch 280, Global step 848:
20-03-23 17:21-INFO-training batch loss: 0.0002; avg_loss: 0.0005
20-03-23 17:21-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:21-INFO-
20-03-23 17:21-INFO-Epoch 2, Batch 281, Global step 849:
20-03-23 17:21-INFO-training batch loss: 0.0004; avg_loss: 0.0005
20-03-23 17:21-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:21-INFO-
20-03-23 17:21-INFO-Epoch 2, Batch 282, Global step 850:
20-03-23 17:21-INFO-training batch loss: 0.0007; avg_loss: 0.0005
20-03-23 17:21-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:21-INFO-
20-03-23 17:21-INFO-Epoch 2, Batch 283, Global step 851:
20-03-23 17:21-INFO-training batch loss: 0.0004; avg_loss: 0.0005
20-03-23 17:21-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:21-INFO-
20-03-23 17:21-INFO-Epoch 2, Batch 284, Global step 852:
20-03-23 17:21-INFO-training batch loss: 0.0003; avg_loss: 0.0005
20-03-23 17:21-INFO-training batch acc: 1.0000; avg_acc: 0.9999
20-03-23 17:21-INFO-
20-03-23 17:21-INFO-Epoch 2, training batch loss: 0.0003; avg_loss: 0.0005
20-03-23 17:21-INFO-Epoch 2, training batch accuracy: 1.0000; avg_accuracy: 0.9999
20-03-23 17:21-INFO-
20-03-23 17:21-INFO-Epoch 2, evaluating batch loss: 1.8272; avg_loss: 0.6118
20-03-23 17:21-INFO-Epoch 2, evaluating batch accuracy: 0.8864; avg_accuracy: 0.9523
20-03-23 17:21-INFO-
