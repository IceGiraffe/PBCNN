20-03-22 22:29-INFO-{'session_length': 24, 'height': 32, 'width': 32, 'num_labels': 8, 'learning_rate': 0.005, 'filter_sizes': [3, 4, 5, 6], 'num_filters': 64, 'filter_sizes_hierarchical': [3, 4, 5], 'num_fitlers_hierarchical': 64, 'is_train': True, 'early_stop': True, 'is_tuning': False}
20-03-22 22:29-WARNING-From ../utils.py:127: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

20-03-22 22:29-WARNING-From ../model/train.py:105: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.

20-03-22 22:29-WARNING-From ../model/siamese_network.py:26: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

20-03-22 22:29-WARNING-From ../model/siamese_network.py:34: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

20-03-22 22:29-WARNING-From ../model/siamese_network.py:34: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.

20-03-22 22:29-WARNING-From ../model/utils/utils.py:26: conv1d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv1D` instead.
20-03-22 22:29-WARNING-Entity <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7f7f2e1bb410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7f7f2e1bb410>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-22 22:29-WARNING-From ../model/utils/utils.py:45: max_pooling1d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.MaxPooling1D instead.
20-03-22 22:29-WARNING-Entity <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7f7f2e171550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7f7f2e171550>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-22 22:29-WARNING-Entity <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7f7f2e171910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7f7f2e171910>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-22 22:29-WARNING-Entity <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7f7f2e1bba10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7f7f2e1bba10>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-22 22:29-WARNING-Entity <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7f7f2e171990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7f7f2e171990>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-22 22:29-WARNING-Entity <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7f7f2e171810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7f7f2e171810>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-22 22:29-WARNING-Entity <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7f7f2e1bbb90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7f7f2e1bbb90>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-22 22:29-WARNING-Entity <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7f7f2e171750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7f7f2e171750>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-22 22:29-WARNING-From ../model/utils/modules.py:205: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.dropout instead.
20-03-22 22:29-WARNING-Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7f7f2e1b6ad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7f7f2e1b6ad0>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-22 22:29-WARNING-Variable *= will be deprecated. Use `var.assign(var * other)` if you want assignment to the variable value or `x = x * y` if you want a new python Tensor object.
20-03-22 22:29-WARNING-Entity <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7f7f2e1bb690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7f7f2e1bb690>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-22 22:29-WARNING-Entity <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7f7f2d4e2210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7f7f2d4e2210>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-22 22:29-WARNING-Entity <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7f7f2e12b0d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7f7f2e12b0d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-22 22:29-WARNING-Entity <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7f7f2e12be10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7f7f2e12be10>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-22 22:29-WARNING-Entity <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7f7f2d5a7110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7f7f2d5a7110>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-22 22:29-WARNING-Entity <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7f7f2d5a7110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7f7f2d5a7110>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-22 22:29-WARNING-From ../model/utils/modules.py:240: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.dense instead.
20-03-22 22:29-WARNING-Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f7f2d5a7090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f7f2d5a7090>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-22 22:29-WARNING-From ../model/utils/modules.py:242: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).
20-03-22 22:29-WARNING-Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f7f2e1253d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f7f2e1253d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-22 22:29-WARNING-From ../model/utils/modules.py:245: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
20-03-22 22:29-WARNING-Entity <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7f7f2d5a7a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7f7f2d5a7a90>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-22 22:29-WARNING-Entity <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7f7f2d51dfd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7f7f2d51dfd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-22 22:29-WARNING-Entity <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7f7f2e1a2110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7f7f2e1a2110>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-22 22:29-WARNING-Entity <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7f7f2d51d590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7f7f2d51d590>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-22 22:29-WARNING-Entity <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7f7f2d530b10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7f7f2d530b10>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-22 22:29-WARNING-Entity <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7f7f2d530090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7f7f2d530090>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-22 22:29-WARNING-Entity <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7f7f2e1a2510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7f7f2e1a2510>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-22 22:29-WARNING-Entity <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7f7f2d56c350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7f7f2d56c350>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-22 22:29-WARNING-Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7f7f2d3eb1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7f7f2d3eb1d0>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-22 22:29-WARNING-Entity <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7f7f2d3f6d10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7f7f2d3f6d10>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-22 22:29-WARNING-Entity <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7f7f2d44d650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7f7f2d44d650>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-22 22:29-WARNING-Entity <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7f7f2d43ca50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7f7f2d43ca50>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-22 22:29-WARNING-Entity <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7f7f2d3efd50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7f7f2d3efd50>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-22 22:29-WARNING-Entity <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7f7f2d39bc90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7f7f2d39bc90>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-22 22:29-WARNING-Entity <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7f7f2d39bf10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7f7f2d39bf10>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-22 22:29-WARNING-Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f7f2d43c350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f7f2d43c350>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-22 22:29-WARNING-Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f7f2d3f0d50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f7f2d3f0d50>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-22 22:29-WARNING-Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f7f2d2af390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f7f2d2af390>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-22 22:29-WARNING-Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f7f2e117d10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f7f2e117d10>>: AssertionError: Bad argument number for Name: 3, expecting 4
20-03-22 22:29-WARNING-From ../model/base_model.py:132: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.

20-03-22 22:29-WARNING-From /root/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/clip_ops.py:286: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
20-03-22 22:29-WARNING-From ../model/train.py:113: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.

20-03-22 22:29-INFO-Epoch 0, Batch 1, Global step 1:
20-03-22 22:29-INFO-training batch loss: 1.9677; avg_loss: 1.9677
20-03-22 22:29-INFO-training batch acc: 0.3984; avg_acc: 0.3984
20-03-22 22:29-INFO-
20-03-22 22:29-INFO-Epoch 0, Batch 2, Global step 2:
20-03-22 22:29-INFO-training batch loss: 44.1061; avg_loss: 23.0369
20-03-22 22:29-INFO-training batch acc: 0.5938; avg_acc: 0.4961
20-03-22 22:29-INFO-
20-03-22 22:29-INFO-Epoch 0, Batch 3, Global step 3:
20-03-22 22:29-INFO-training batch loss: 30.5125; avg_loss: 25.5288
20-03-22 22:29-INFO-training batch acc: 0.5547; avg_acc: 0.5156
20-03-22 22:29-INFO-
20-03-22 22:29-INFO-Epoch 0, Batch 4, Global step 4:
20-03-22 22:29-INFO-training batch loss: 6.6208; avg_loss: 20.8018
20-03-22 22:29-INFO-training batch acc: 0.6562; avg_acc: 0.5508
20-03-22 22:29-INFO-
20-03-22 22:29-INFO-Epoch 0, Batch 5, Global step 5:
20-03-22 22:29-INFO-training batch loss: 4.3690; avg_loss: 17.5152
20-03-22 22:29-INFO-training batch acc: 0.4219; avg_acc: 0.5250
20-03-22 22:29-INFO-
20-03-22 22:29-INFO-Epoch 0, Batch 6, Global step 6:
20-03-22 22:29-INFO-training batch loss: 1.4734; avg_loss: 14.8416
20-03-22 22:29-INFO-training batch acc: 0.3359; avg_acc: 0.4935
20-03-22 22:29-INFO-
20-03-22 22:29-INFO-Epoch 0, Batch 7, Global step 7:
20-03-22 22:29-INFO-training batch loss: 2.0430; avg_loss: 13.0132
20-03-22 22:29-INFO-training batch acc: 0.6406; avg_acc: 0.5145
20-03-22 22:29-INFO-
20-03-22 22:29-INFO-Epoch 0, Batch 8, Global step 8:
20-03-22 22:29-INFO-training batch loss: 1.7246; avg_loss: 11.6022
20-03-22 22:29-INFO-training batch acc: 0.5469; avg_acc: 0.5186
20-03-22 22:29-INFO-
20-03-22 22:29-INFO-Epoch 0, Batch 9, Global step 9:
20-03-22 22:29-INFO-training batch loss: 0.6363; avg_loss: 10.3837
20-03-22 22:29-INFO-training batch acc: 0.6641; avg_acc: 0.5347
20-03-22 22:29-INFO-
20-03-22 22:30-INFO-Epoch 0, Batch 10, Global step 10:
20-03-22 22:30-INFO-training batch loss: 0.6842; avg_loss: 9.4138
20-03-22 22:30-INFO-training batch acc: 0.5859; avg_acc: 0.5398
20-03-22 22:30-INFO-
20-03-22 22:30-INFO-Epoch 0, Batch 11, Global step 11:
20-03-22 22:30-INFO-training batch loss: 0.6180; avg_loss: 8.6142
20-03-22 22:30-INFO-training batch acc: 0.5625; avg_acc: 0.5419
20-03-22 22:30-INFO-
20-03-22 22:30-INFO-Epoch 0, Batch 12, Global step 12:
20-03-22 22:30-INFO-training batch loss: 0.6863; avg_loss: 7.9535
20-03-22 22:30-INFO-training batch acc: 0.6094; avg_acc: 0.5475
20-03-22 22:30-INFO-
20-03-22 22:30-INFO-Epoch 0, Batch 13, Global step 13:
20-03-22 22:30-INFO-training batch loss: 0.6539; avg_loss: 7.3920
20-03-22 22:30-INFO-training batch acc: 0.5938; avg_acc: 0.5511
20-03-22 22:30-INFO-
20-03-22 22:30-INFO-Epoch 0, Batch 14, Global step 14:
20-03-22 22:30-INFO-training batch loss: 0.6460; avg_loss: 6.9101
20-03-22 22:30-INFO-training batch acc: 0.5781; avg_acc: 0.5530
20-03-22 22:30-INFO-
20-03-22 22:30-INFO-Epoch 0, Batch 15, Global step 15:
20-03-22 22:30-INFO-training batch loss: 0.6685; avg_loss: 6.4940
20-03-22 22:30-INFO-training batch acc: 0.6484; avg_acc: 0.5594
20-03-22 22:30-INFO-
20-03-22 22:30-INFO-Epoch 0, Batch 16, Global step 16:
20-03-22 22:30-INFO-training batch loss: 0.6067; avg_loss: 6.1261
20-03-22 22:30-INFO-training batch acc: 0.6641; avg_acc: 0.5659
20-03-22 22:30-INFO-
20-03-22 22:30-INFO-Epoch 0, Batch 17, Global step 17:
20-03-22 22:30-INFO-training batch loss: 0.6692; avg_loss: 5.8051
20-03-22 22:30-INFO-training batch acc: 0.5312; avg_acc: 0.5639
20-03-22 22:30-INFO-
20-03-22 22:30-INFO-Epoch 0, Batch 18, Global step 18:
20-03-22 22:30-INFO-training batch loss: 0.6558; avg_loss: 5.5190
20-03-22 22:30-INFO-training batch acc: 0.6016; avg_acc: 0.5660
20-03-22 22:30-INFO-
20-03-22 22:30-INFO-Epoch 0, Batch 19, Global step 19:
20-03-22 22:30-INFO-training batch loss: 0.6622; avg_loss: 5.2634
20-03-22 22:30-INFO-training batch acc: 0.6484; avg_acc: 0.5703
20-03-22 22:30-INFO-
20-03-22 22:30-INFO-Epoch 0, Batch 20, Global step 20:
20-03-22 22:30-INFO-training batch loss: 0.5995; avg_loss: 5.0302
20-03-22 22:30-INFO-training batch acc: 0.6875; avg_acc: 0.5762
20-03-22 22:30-INFO-
20-03-22 22:30-INFO-Epoch 0, Batch 21, Global step 21:
20-03-22 22:30-INFO-training batch loss: 0.6136; avg_loss: 4.8199
20-03-22 22:30-INFO-training batch acc: 0.7812; avg_acc: 0.5859
20-03-22 22:30-INFO-
20-03-22 22:30-INFO-Epoch 0, Batch 22, Global step 22:
20-03-22 22:30-INFO-training batch loss: 0.6621; avg_loss: 4.6309
20-03-22 22:30-INFO-training batch acc: 0.6250; avg_acc: 0.5877
20-03-22 22:30-INFO-
20-03-22 22:30-INFO-Epoch 0, Batch 23, Global step 23:
20-03-22 22:30-INFO-training batch loss: 0.6402; avg_loss: 4.4574
20-03-22 22:30-INFO-training batch acc: 0.6875; avg_acc: 0.5921
20-03-22 22:30-INFO-
20-03-22 22:30-INFO-Epoch 0, Batch 24, Global step 24:
20-03-22 22:30-INFO-training batch loss: 0.5801; avg_loss: 4.2958
20-03-22 22:30-INFO-training batch acc: 0.7109; avg_acc: 0.5970
20-03-22 22:30-INFO-
20-03-22 22:30-INFO-Epoch 0, Batch 25, Global step 25:
20-03-22 22:30-INFO-training batch loss: 0.5696; avg_loss: 4.1468
20-03-22 22:30-INFO-training batch acc: 0.6094; avg_acc: 0.5975
20-03-22 22:30-INFO-
20-03-22 22:30-INFO-Epoch 0, Batch 26, Global step 26:
20-03-22 22:30-INFO-training batch loss: 0.5519; avg_loss: 4.0085
20-03-22 22:30-INFO-training batch acc: 0.6172; avg_acc: 0.5983
20-03-22 22:30-INFO-
20-03-22 22:30-INFO-Epoch 0, Batch 27, Global step 27:
20-03-22 22:30-INFO-training batch loss: 0.5404; avg_loss: 3.8801
20-03-22 22:30-INFO-training batch acc: 0.5859; avg_acc: 0.5978
20-03-22 22:30-INFO-
20-03-22 22:30-INFO-Epoch 0, Batch 28, Global step 28:
20-03-22 22:30-INFO-training batch loss: 0.5384; avg_loss: 3.7607
20-03-22 22:30-INFO-training batch acc: 0.6328; avg_acc: 0.5991
20-03-22 22:30-INFO-
20-03-22 22:30-INFO-Epoch 0, Batch 29, Global step 29:
20-03-22 22:30-INFO-training batch loss: 0.5161; avg_loss: 3.6488
20-03-22 22:30-INFO-training batch acc: 0.7891; avg_acc: 0.6056
20-03-22 22:30-INFO-
20-03-22 22:30-INFO-Epoch 0, Batch 30, Global step 30:
20-03-22 22:30-INFO-training batch loss: 0.4505; avg_loss: 3.5422
20-03-22 22:30-INFO-training batch acc: 0.7812; avg_acc: 0.6115
20-03-22 22:30-INFO-
20-03-22 22:30-INFO-Epoch 0, Batch 31, Global step 31:
20-03-22 22:30-INFO-training batch loss: 0.5015; avg_loss: 3.4441
20-03-22 22:30-INFO-training batch acc: 0.7266; avg_acc: 0.6152
20-03-22 22:30-INFO-
20-03-22 22:30-INFO-Epoch 0, Batch 32, Global step 32:
20-03-22 22:30-INFO-training batch loss: 0.4130; avg_loss: 3.3494
20-03-22 22:30-INFO-training batch acc: 0.8203; avg_acc: 0.6216
20-03-22 22:30-INFO-
20-03-22 22:30-INFO-Epoch 0, Batch 33, Global step 33:
20-03-22 22:30-INFO-training batch loss: 0.4667; avg_loss: 3.2621
20-03-22 22:30-INFO-training batch acc: 0.6953; avg_acc: 0.6238
20-03-22 22:30-INFO-
20-03-22 22:30-INFO-Epoch 0, Batch 34, Global step 34:
20-03-22 22:30-INFO-training batch loss: 0.4413; avg_loss: 3.1791
20-03-22 22:30-INFO-training batch acc: 0.7500; avg_acc: 0.6275
20-03-22 22:30-INFO-
20-03-22 22:30-INFO-Epoch 0, Batch 35, Global step 35:
20-03-22 22:30-INFO-training batch loss: 0.4057; avg_loss: 3.0999
20-03-22 22:30-INFO-training batch acc: 0.7656; avg_acc: 0.6315
20-03-22 22:30-INFO-
20-03-22 22:31-INFO-Epoch 0, Batch 36, Global step 36:
20-03-22 22:31-INFO-training batch loss: 0.4894; avg_loss: 3.0273
20-03-22 22:31-INFO-training batch acc: 0.7266; avg_acc: 0.6341
20-03-22 22:31-INFO-
20-03-22 22:31-INFO-Epoch 0, Batch 37, Global step 37:
20-03-22 22:31-INFO-training batch loss: 0.5273; avg_loss: 2.9598
20-03-22 22:31-INFO-training batch acc: 0.6328; avg_acc: 0.6341
20-03-22 22:31-INFO-
20-03-22 22:31-INFO-Epoch 0, Batch 38, Global step 38:
20-03-22 22:31-INFO-training batch loss: 0.5464; avg_loss: 2.8963
20-03-22 22:31-INFO-training batch acc: 0.7109; avg_acc: 0.6361
20-03-22 22:31-INFO-
20-03-22 22:31-INFO-Epoch 0, Batch 39, Global step 39:
20-03-22 22:31-INFO-training batch loss: 0.4893; avg_loss: 2.8345
20-03-22 22:31-INFO-training batch acc: 0.6641; avg_acc: 0.6368
20-03-22 22:31-INFO-
20-03-22 22:31-INFO-Epoch 0, Batch 40, Global step 40:
20-03-22 22:31-INFO-training batch loss: 0.4232; avg_loss: 2.7743
20-03-22 22:31-INFO-training batch acc: 0.7656; avg_acc: 0.6400
20-03-22 22:31-INFO-
20-03-22 22:31-INFO-Epoch 0, Batch 41, Global step 41:
20-03-22 22:31-INFO-training batch loss: 0.4523; avg_loss: 2.7176
20-03-22 22:31-INFO-training batch acc: 0.7422; avg_acc: 0.6425
20-03-22 22:31-INFO-
20-03-22 22:31-INFO-Epoch 0, Batch 42, Global step 42:
20-03-22 22:31-INFO-training batch loss: 0.4252; avg_loss: 2.6630
20-03-22 22:31-INFO-training batch acc: 0.7578; avg_acc: 0.6453
20-03-22 22:31-INFO-
20-03-22 22:31-INFO-Epoch 0, Batch 43, Global step 43:
20-03-22 22:31-INFO-training batch loss: 0.3970; avg_loss: 2.6103
20-03-22 22:31-INFO-training batch acc: 0.7812; avg_acc: 0.6484
20-03-22 22:31-INFO-
20-03-22 22:31-INFO-Epoch 0, Batch 44, Global step 44:
20-03-22 22:31-INFO-training batch loss: 0.3705; avg_loss: 2.5594
20-03-22 22:31-INFO-training batch acc: 0.7500; avg_acc: 0.6507
20-03-22 22:31-INFO-
20-03-22 22:31-INFO-Epoch 0, Batch 45, Global step 45:
20-03-22 22:31-INFO-training batch loss: 0.4378; avg_loss: 2.5123
20-03-22 22:31-INFO-training batch acc: 0.7109; avg_acc: 0.6521
20-03-22 22:31-INFO-
20-03-22 22:31-INFO-Epoch 0, Batch 46, Global step 46:
20-03-22 22:31-INFO-training batch loss: 0.4790; avg_loss: 2.4681
20-03-22 22:31-INFO-training batch acc: 0.6875; avg_acc: 0.6529
20-03-22 22:31-INFO-
20-03-22 22:31-INFO-Epoch 0, Batch 47, Global step 47:
20-03-22 22:31-INFO-training batch loss: 0.4508; avg_loss: 2.4252
20-03-22 22:31-INFO-training batch acc: 0.7109; avg_acc: 0.6541
20-03-22 22:31-INFO-
20-03-22 22:31-INFO-Epoch 0, Batch 48, Global step 48:
20-03-22 22:31-INFO-training batch loss: 0.3779; avg_loss: 2.3825
20-03-22 22:31-INFO-training batch acc: 0.7344; avg_acc: 0.6558
20-03-22 22:31-INFO-
20-03-22 22:31-INFO-Epoch 0, Batch 49, Global step 49:
20-03-22 22:31-INFO-training batch loss: 0.4415; avg_loss: 2.3429
20-03-22 22:31-INFO-training batch acc: 0.7578; avg_acc: 0.6578
20-03-22 22:31-INFO-
20-03-22 22:31-INFO-Epoch 0, Batch 50, Global step 50:
20-03-22 22:31-INFO-training batch loss: 0.3358; avg_loss: 2.3028
20-03-22 22:31-INFO-training batch acc: 0.8594; avg_acc: 0.6619
20-03-22 22:31-INFO-
20-03-22 22:31-INFO-Epoch 0, Batch 51, Global step 51:
20-03-22 22:31-INFO-training batch loss: 0.4751; avg_loss: 2.2669
20-03-22 22:31-INFO-training batch acc: 0.7812; avg_acc: 0.6642
20-03-22 22:31-INFO-
20-03-22 22:31-INFO-Epoch 0, Batch 52, Global step 52:
20-03-22 22:31-INFO-training batch loss: 0.3124; avg_loss: 2.2293
20-03-22 22:31-INFO-training batch acc: 0.8359; avg_acc: 0.6675
20-03-22 22:31-INFO-
20-03-22 22:31-INFO-Epoch 0, Batch 53, Global step 53:
20-03-22 22:31-INFO-training batch loss: 0.3754; avg_loss: 2.1944
20-03-22 22:31-INFO-training batch acc: 0.7969; avg_acc: 0.6700
20-03-22 22:31-INFO-
20-03-22 22:31-INFO-Epoch 0, Batch 54, Global step 54:
20-03-22 22:31-INFO-training batch loss: 0.4345; avg_loss: 2.1618
20-03-22 22:31-INFO-training batch acc: 0.8203; avg_acc: 0.6727
20-03-22 22:31-INFO-
20-03-22 22:31-INFO-Epoch 0, Batch 55, Global step 55:
20-03-22 22:31-INFO-training batch loss: 0.4770; avg_loss: 2.1311
20-03-22 22:31-INFO-training batch acc: 0.7422; avg_acc: 0.6740
20-03-22 22:31-INFO-
20-03-22 22:31-INFO-Epoch 0, Batch 56, Global step 56:
20-03-22 22:31-INFO-training batch loss: 0.5319; avg_loss: 2.1026
20-03-22 22:31-INFO-training batch acc: 0.5703; avg_acc: 0.6722
20-03-22 22:31-INFO-
20-03-22 22:31-INFO-Epoch 0, Batch 57, Global step 57:
20-03-22 22:31-INFO-training batch loss: 0.4957; avg_loss: 2.0744
20-03-22 22:31-INFO-training batch acc: 0.7031; avg_acc: 0.6727
20-03-22 22:31-INFO-
20-03-22 22:31-INFO-Epoch 0, Batch 58, Global step 58:
20-03-22 22:31-INFO-training batch loss: 0.4867; avg_loss: 2.0470
20-03-22 22:31-INFO-training batch acc: 0.6328; avg_acc: 0.6720
20-03-22 22:31-INFO-
20-03-22 22:31-INFO-Epoch 0, Batch 59, Global step 59:
20-03-22 22:31-INFO-training batch loss: 0.4608; avg_loss: 2.0201
20-03-22 22:31-INFO-training batch acc: 0.6641; avg_acc: 0.6719
20-03-22 22:31-INFO-
20-03-22 22:31-INFO-Epoch 0, Batch 60, Global step 60:
20-03-22 22:31-INFO-training batch loss: 0.4262; avg_loss: 1.9936
20-03-22 22:31-INFO-training batch acc: 0.7422; avg_acc: 0.6730
20-03-22 22:31-INFO-
20-03-22 22:31-INFO-Epoch 0, Batch 61, Global step 61:
20-03-22 22:31-INFO-training batch loss: 0.3054; avg_loss: 1.9659
20-03-22 22:31-INFO-training batch acc: 0.8359; avg_acc: 0.6757
20-03-22 22:31-INFO-
20-03-22 22:32-INFO-Epoch 0, Batch 62, Global step 62:
20-03-22 22:32-INFO-training batch loss: 0.4121; avg_loss: 1.9408
20-03-22 22:32-INFO-training batch acc: 0.8594; avg_acc: 0.6787
20-03-22 22:32-INFO-
20-03-22 22:32-INFO-Epoch 0, Batch 63, Global step 63:
20-03-22 22:32-INFO-training batch loss: 0.4956; avg_loss: 1.9179
20-03-22 22:32-INFO-training batch acc: 0.7656; avg_acc: 0.6801
20-03-22 22:32-INFO-
20-03-22 22:32-INFO-Epoch 0, Batch 64, Global step 64:
20-03-22 22:32-INFO-training batch loss: 0.2457; avg_loss: 1.8918
20-03-22 22:32-INFO-training batch acc: 0.9141; avg_acc: 0.6837
20-03-22 22:32-INFO-
20-03-22 22:32-INFO-Epoch 0, Batch 65, Global step 65:
20-03-22 22:32-INFO-training batch loss: 0.3179; avg_loss: 1.8676
20-03-22 22:32-INFO-training batch acc: 0.9062; avg_acc: 0.6871
20-03-22 22:32-INFO-
20-03-22 22:32-INFO-Epoch 0, Batch 66, Global step 66:
20-03-22 22:32-INFO-training batch loss: 0.4289; avg_loss: 1.8458
20-03-22 22:32-INFO-training batch acc: 0.8203; avg_acc: 0.6892
20-03-22 22:32-INFO-
20-03-22 22:32-INFO-Epoch 0, Batch 67, Global step 67:
20-03-22 22:32-INFO-training batch loss: 0.3374; avg_loss: 1.8232
20-03-22 22:32-INFO-training batch acc: 0.8516; avg_acc: 0.6916
20-03-22 22:32-INFO-
20-03-22 22:32-INFO-Epoch 0, Batch 68, Global step 68:
20-03-22 22:32-INFO-training batch loss: 0.2772; avg_loss: 1.8005
20-03-22 22:32-INFO-training batch acc: 0.8828; avg_acc: 0.6944
20-03-22 22:32-INFO-
20-03-22 22:32-INFO-Epoch 0, Batch 69, Global step 69:
20-03-22 22:32-INFO-training batch loss: 0.4023; avg_loss: 1.7802
20-03-22 22:32-INFO-training batch acc: 0.8672; avg_acc: 0.6969
20-03-22 22:32-INFO-
20-03-22 22:32-INFO-Epoch 0, Batch 70, Global step 70:
20-03-22 22:32-INFO-training batch loss: 0.2849; avg_loss: 1.7589
20-03-22 22:32-INFO-training batch acc: 0.8672; avg_acc: 0.6993
20-03-22 22:32-INFO-
20-03-22 22:32-INFO-Epoch 0, Batch 71, Global step 71:
20-03-22 22:32-INFO-training batch loss: 0.2829; avg_loss: 1.7381
20-03-22 22:32-INFO-training batch acc: 0.8672; avg_acc: 0.7017
20-03-22 22:32-INFO-
20-03-22 22:32-INFO-Epoch 0, Batch 72, Global step 72:
20-03-22 22:32-INFO-training batch loss: 0.2553; avg_loss: 1.7175
20-03-22 22:32-INFO-training batch acc: 0.8828; avg_acc: 0.7042
20-03-22 22:32-INFO-
20-03-22 22:32-INFO-Epoch 0, Batch 73, Global step 73:
20-03-22 22:32-INFO-training batch loss: 0.2554; avg_loss: 1.6975
20-03-22 22:32-INFO-training batch acc: 0.9141; avg_acc: 0.7071
20-03-22 22:32-INFO-
20-03-22 22:32-INFO-Epoch 0, Batch 74, Global step 74:
20-03-22 22:32-INFO-training batch loss: 0.2278; avg_loss: 1.6776
20-03-22 22:32-INFO-training batch acc: 0.9219; avg_acc: 0.7100
20-03-22 22:32-INFO-
20-03-22 22:32-INFO-Epoch 0, Batch 75, Global step 75:
20-03-22 22:32-INFO-training batch loss: 0.2967; avg_loss: 1.6592
20-03-22 22:32-INFO-training batch acc: 0.8828; avg_acc: 0.7123
20-03-22 22:32-INFO-
20-03-22 22:32-INFO-Epoch 0, Batch 76, Global step 76:
20-03-22 22:32-INFO-training batch loss: 0.2333; avg_loss: 1.6404
20-03-22 22:32-INFO-training batch acc: 0.8906; avg_acc: 0.7146
20-03-22 22:32-INFO-
20-03-22 22:32-INFO-Epoch 0, Batch 77, Global step 77:
20-03-22 22:32-INFO-training batch loss: 0.2155; avg_loss: 1.6219
20-03-22 22:32-INFO-training batch acc: 0.9297; avg_acc: 0.7174
20-03-22 22:32-INFO-
20-03-22 22:32-INFO-Epoch 0, Batch 78, Global step 78:
20-03-22 22:32-INFO-training batch loss: 0.2169; avg_loss: 1.6039
20-03-22 22:32-INFO-training batch acc: 0.9141; avg_acc: 0.7200
20-03-22 22:32-INFO-
20-03-22 22:32-INFO-Epoch 0, Batch 79, Global step 79:
20-03-22 22:32-INFO-training batch loss: 0.1575; avg_loss: 1.5856
20-03-22 22:32-INFO-training batch acc: 0.9297; avg_acc: 0.7226
20-03-22 22:32-INFO-
20-03-22 22:32-INFO-Epoch 0, Batch 80, Global step 80:
20-03-22 22:32-INFO-training batch loss: 0.1697; avg_loss: 1.5679
20-03-22 22:32-INFO-training batch acc: 0.9062; avg_acc: 0.7249
20-03-22 22:32-INFO-
20-03-22 22:32-INFO-Epoch 0, Batch 81, Global step 81:
20-03-22 22:32-INFO-training batch loss: 0.1758; avg_loss: 1.5507
20-03-22 22:32-INFO-training batch acc: 0.9219; avg_acc: 0.7273
20-03-22 22:32-INFO-
20-03-22 22:32-INFO-Epoch 0, Batch 82, Global step 82:
20-03-22 22:32-INFO-training batch loss: 0.2481; avg_loss: 1.5348
20-03-22 22:32-INFO-training batch acc: 0.9297; avg_acc: 0.7298
20-03-22 22:32-INFO-
20-03-22 22:32-INFO-Epoch 0, Batch 83, Global step 83:
20-03-22 22:32-INFO-training batch loss: 0.3556; avg_loss: 1.5206
20-03-22 22:32-INFO-training batch acc: 0.9141; avg_acc: 0.7320
20-03-22 22:32-INFO-
20-03-22 22:32-INFO-Epoch 0, Batch 84, Global step 84:
20-03-22 22:32-INFO-training batch loss: 0.2410; avg_loss: 1.5054
20-03-22 22:32-INFO-training batch acc: 0.9297; avg_acc: 0.7344
20-03-22 22:32-INFO-
20-03-22 22:32-INFO-Epoch 0, Batch 85, Global step 85:
20-03-22 22:32-INFO-training batch loss: 0.1456; avg_loss: 1.4894
20-03-22 22:32-INFO-training batch acc: 0.9609; avg_acc: 0.7370
20-03-22 22:32-INFO-
20-03-22 22:32-INFO-Epoch 0, Batch 86, Global step 86:
20-03-22 22:32-INFO-training batch loss: 0.1254; avg_loss: 1.4735
20-03-22 22:32-INFO-training batch acc: 0.9531; avg_acc: 0.7396
20-03-22 22:32-INFO-
20-03-22 22:32-INFO-Epoch 0, Batch 87, Global step 87:
20-03-22 22:32-INFO-training batch loss: 0.2168; avg_loss: 1.4591
20-03-22 22:32-INFO-training batch acc: 0.9219; avg_acc: 0.7416
20-03-22 22:32-INFO-
20-03-22 22:32-INFO-Epoch 0, Batch 88, Global step 88:
20-03-22 22:32-INFO-training batch loss: 0.1092; avg_loss: 1.4438
20-03-22 22:32-INFO-training batch acc: 0.9609; avg_acc: 0.7441
20-03-22 22:32-INFO-
20-03-22 22:33-INFO-Epoch 0, Batch 89, Global step 89:
20-03-22 22:33-INFO-training batch loss: 0.1210; avg_loss: 1.4289
20-03-22 22:33-INFO-training batch acc: 0.9844; avg_acc: 0.7468
20-03-22 22:33-INFO-
20-03-22 22:33-INFO-Epoch 0, Batch 90, Global step 90:
20-03-22 22:33-INFO-training batch loss: 0.1290; avg_loss: 1.4144
20-03-22 22:33-INFO-training batch acc: 0.9766; avg_acc: 0.7494
20-03-22 22:33-INFO-
20-03-22 22:33-INFO-Epoch 0, Batch 91, Global step 91:
20-03-22 22:33-INFO-training batch loss: 0.2024; avg_loss: 1.4011
20-03-22 22:33-INFO-training batch acc: 0.9297; avg_acc: 0.7514
20-03-22 22:33-INFO-
20-03-22 22:33-INFO-Epoch 0, Batch 92, Global step 92:
20-03-22 22:33-INFO-training batch loss: 0.1200; avg_loss: 1.3872
20-03-22 22:33-INFO-training batch acc: 0.9453; avg_acc: 0.7535
20-03-22 22:33-INFO-
20-03-22 22:33-INFO-Epoch 0, Batch 93, Global step 93:
20-03-22 22:33-INFO-training batch loss: 0.1649; avg_loss: 1.3741
20-03-22 22:33-INFO-training batch acc: 0.9453; avg_acc: 0.7555
20-03-22 22:33-INFO-
20-03-22 22:33-INFO-Epoch 0, Batch 94, Global step 94:
20-03-22 22:33-INFO-training batch loss: 0.1816; avg_loss: 1.3614
20-03-22 22:33-INFO-training batch acc: 0.9844; avg_acc: 0.7580
20-03-22 22:33-INFO-
20-03-22 22:33-INFO-Epoch 0, Batch 95, Global step 95:
20-03-22 22:33-INFO-training batch loss: 0.1227; avg_loss: 1.3483
20-03-22 22:33-INFO-training batch acc: 0.9688; avg_acc: 0.7602
20-03-22 22:33-INFO-
20-03-22 22:33-INFO-Epoch 0, Batch 96, Global step 96:
20-03-22 22:33-INFO-training batch loss: 0.2765; avg_loss: 1.3372
20-03-22 22:33-INFO-training batch acc: 0.9531; avg_acc: 0.7622
20-03-22 22:33-INFO-
20-03-22 22:33-INFO-Epoch 0, Batch 97, Global step 97:
20-03-22 22:33-INFO-training batch loss: 0.2024; avg_loss: 1.3255
20-03-22 22:33-INFO-training batch acc: 0.9219; avg_acc: 0.7639
20-03-22 22:33-INFO-
20-03-22 22:33-INFO-Epoch 0, Batch 98, Global step 98:
20-03-22 22:33-INFO-training batch loss: 0.1808; avg_loss: 1.3138
20-03-22 22:33-INFO-training batch acc: 0.9609; avg_acc: 0.7659
20-03-22 22:33-INFO-
20-03-22 22:33-INFO-Epoch 0, Batch 99, Global step 99:
20-03-22 22:33-INFO-training batch loss: 0.1120; avg_loss: 1.3017
20-03-22 22:33-INFO-training batch acc: 0.9688; avg_acc: 0.7679
20-03-22 22:33-INFO-
20-03-22 22:33-INFO-Epoch 0, Batch 100, Global step 100:
20-03-22 22:33-INFO-training batch loss: 0.0954; avg_loss: 1.2896
20-03-22 22:33-INFO-training batch acc: 0.9766; avg_acc: 0.7700
20-03-22 22:33-INFO-
20-03-22 22:33-INFO-Epoch 0, Batch 101, Global step 101:
20-03-22 22:33-INFO-training batch loss: 0.1362; avg_loss: 1.2782
20-03-22 22:33-INFO-training batch acc: 0.9844; avg_acc: 0.7721
20-03-22 22:33-INFO-
20-03-22 22:33-INFO-Epoch 0, Batch 102, Global step 102:
20-03-22 22:33-INFO-training batch loss: 0.1847; avg_loss: 1.2674
20-03-22 22:33-INFO-training batch acc: 0.9609; avg_acc: 0.7740
20-03-22 22:33-INFO-
20-03-22 22:33-INFO-Epoch 0, Batch 103, Global step 103:
20-03-22 22:33-INFO-training batch loss: 0.0422; avg_loss: 1.2556
20-03-22 22:33-INFO-training batch acc: 1.0000; avg_acc: 0.7762
20-03-22 22:33-INFO-
20-03-22 22:33-INFO-Epoch 0, Batch 104, Global step 104:
20-03-22 22:33-INFO-training batch loss: 0.1103; avg_loss: 1.2445
20-03-22 22:33-INFO-training batch acc: 0.9766; avg_acc: 0.7781
20-03-22 22:33-INFO-
20-03-22 22:33-INFO-Epoch 0, Batch 105, Global step 105:
20-03-22 22:33-INFO-training batch loss: 0.1215; avg_loss: 1.2338
20-03-22 22:33-INFO-training batch acc: 0.9609; avg_acc: 0.7798
20-03-22 22:33-INFO-
20-03-22 22:33-INFO-Epoch 0, Batch 106, Global step 106:
20-03-22 22:33-INFO-training batch loss: 0.2233; avg_loss: 1.2243
20-03-22 22:33-INFO-training batch acc: 0.9531; avg_acc: 0.7815
20-03-22 22:33-INFO-
20-03-22 22:33-INFO-Epoch 0, Batch 107, Global step 107:
20-03-22 22:33-INFO-training batch loss: 0.0707; avg_loss: 1.2135
20-03-22 22:33-INFO-training batch acc: 0.9844; avg_acc: 0.7834
20-03-22 22:33-INFO-
20-03-22 22:33-INFO-Epoch 0, Batch 108, Global step 108:
20-03-22 22:33-INFO-training batch loss: 0.1671; avg_loss: 1.2038
20-03-22 22:33-INFO-training batch acc: 0.9453; avg_acc: 0.7849
20-03-22 22:33-INFO-
20-03-22 22:33-INFO-Epoch 0, Batch 109, Global step 109:
20-03-22 22:33-INFO-training batch loss: 0.0903; avg_loss: 1.1936
20-03-22 22:33-INFO-training batch acc: 0.9688; avg_acc: 0.7866
20-03-22 22:33-INFO-
20-03-22 22:33-INFO-Epoch 0, Batch 110, Global step 110:
20-03-22 22:33-INFO-training batch loss: 0.0844; avg_loss: 1.1835
20-03-22 22:33-INFO-training batch acc: 0.9922; avg_acc: 0.7884
20-03-22 22:33-INFO-
20-03-22 22:33-INFO-Epoch 0, Batch 111, Global step 111:
20-03-22 22:33-INFO-training batch loss: 0.0842; avg_loss: 1.1736
20-03-22 22:33-INFO-training batch acc: 0.9844; avg_acc: 0.7902
20-03-22 22:33-INFO-
20-03-22 22:33-INFO-Epoch 0, Batch 112, Global step 112:
20-03-22 22:33-INFO-training batch loss: 0.0992; avg_loss: 1.1640
20-03-22 22:33-INFO-training batch acc: 0.9766; avg_acc: 0.7919
20-03-22 22:33-INFO-
20-03-22 22:33-INFO-Epoch 0, Batch 113, Global step 113:
20-03-22 22:33-INFO-training batch loss: 0.0450; avg_loss: 1.1541
20-03-22 22:33-INFO-training batch acc: 1.0000; avg_acc: 0.7937
20-03-22 22:33-INFO-
20-03-22 22:33-INFO-Epoch 0, Batch 114, Global step 114:
20-03-22 22:33-INFO-training batch loss: 0.0852; avg_loss: 1.1448
20-03-22 22:33-INFO-training batch acc: 0.9844; avg_acc: 0.7954
20-03-22 22:33-INFO-
20-03-22 22:34-INFO-Epoch 0, Batch 115, Global step 115:
20-03-22 22:34-INFO-training batch loss: 0.1537; avg_loss: 1.1361
20-03-22 22:34-INFO-training batch acc: 0.9688; avg_acc: 0.7969
20-03-22 22:34-INFO-
20-03-22 22:34-INFO-Epoch 0, Batch 116, Global step 116:
20-03-22 22:34-INFO-training batch loss: 0.0745; avg_loss: 1.1270
20-03-22 22:34-INFO-training batch acc: 0.9844; avg_acc: 0.7985
20-03-22 22:34-INFO-
20-03-22 22:34-INFO-Epoch 0, Batch 117, Global step 117:
20-03-22 22:34-INFO-training batch loss: 0.0564; avg_loss: 1.1178
20-03-22 22:34-INFO-training batch acc: 0.9844; avg_acc: 0.8001
20-03-22 22:34-INFO-
20-03-22 22:34-INFO-Epoch 0, Batch 118, Global step 118:
20-03-22 22:34-INFO-training batch loss: 0.0426; avg_loss: 1.1087
20-03-22 22:34-INFO-training batch acc: 0.9922; avg_acc: 0.8017
20-03-22 22:34-INFO-
20-03-22 22:34-INFO-Epoch 0, Batch 119, Global step 119:
20-03-22 22:34-INFO-training batch loss: 0.1822; avg_loss: 1.1009
20-03-22 22:34-INFO-training batch acc: 0.9531; avg_acc: 0.8030
20-03-22 22:34-INFO-
20-03-22 22:34-INFO-Epoch 0, Batch 120, Global step 120:
20-03-22 22:34-INFO-training batch loss: 0.0727; avg_loss: 1.0924
20-03-22 22:34-INFO-training batch acc: 0.9766; avg_acc: 0.8044
20-03-22 22:34-INFO-
20-03-22 22:34-INFO-Epoch 0, Batch 121, Global step 121:
20-03-22 22:34-INFO-training batch loss: 0.1167; avg_loss: 1.0843
20-03-22 22:34-INFO-training batch acc: 0.9844; avg_acc: 0.8059
20-03-22 22:34-INFO-
20-03-22 22:34-INFO-Epoch 0, Batch 122, Global step 122:
20-03-22 22:34-INFO-training batch loss: 0.0738; avg_loss: 1.0760
20-03-22 22:34-INFO-training batch acc: 0.9844; avg_acc: 0.8074
20-03-22 22:34-INFO-
20-03-22 22:34-INFO-Epoch 0, Batch 123, Global step 123:
20-03-22 22:34-INFO-training batch loss: 0.1700; avg_loss: 1.0687
20-03-22 22:34-INFO-training batch acc: 0.9531; avg_acc: 0.8086
20-03-22 22:34-INFO-
20-03-22 22:34-INFO-Epoch 0, Batch 124, Global step 124:
20-03-22 22:34-INFO-training batch loss: 0.0868; avg_loss: 1.0607
20-03-22 22:34-INFO-training batch acc: 0.9844; avg_acc: 0.8100
20-03-22 22:34-INFO-
20-03-22 22:34-INFO-Epoch 0, Batch 125, Global step 125:
20-03-22 22:34-INFO-training batch loss: 0.0401; avg_loss: 1.0526
20-03-22 22:34-INFO-training batch acc: 0.9922; avg_acc: 0.8114
20-03-22 22:34-INFO-
20-03-22 22:34-INFO-Epoch 0, Batch 126, Global step 126:
20-03-22 22:34-INFO-training batch loss: 0.0430; avg_loss: 1.0446
20-03-22 22:34-INFO-training batch acc: 0.9922; avg_acc: 0.8129
20-03-22 22:34-INFO-
20-03-22 22:34-INFO-Epoch 0, Batch 127, Global step 127:
20-03-22 22:34-INFO-training batch loss: 0.0614; avg_loss: 1.0368
20-03-22 22:34-INFO-training batch acc: 0.9844; avg_acc: 0.8142
20-03-22 22:34-INFO-
20-03-22 22:34-INFO-Epoch 0, Batch 128, Global step 128:
20-03-22 22:34-INFO-training batch loss: 0.0455; avg_loss: 1.0291
20-03-22 22:34-INFO-training batch acc: 0.9844; avg_acc: 0.8156
20-03-22 22:34-INFO-
20-03-22 22:34-INFO-Epoch 0, Batch 129, Global step 129:
20-03-22 22:34-INFO-training batch loss: 0.2038; avg_loss: 1.0227
20-03-22 22:34-INFO-training batch acc: 0.9609; avg_acc: 0.8167
20-03-22 22:34-INFO-
20-03-22 22:34-INFO-Epoch 0, Batch 130, Global step 130:
20-03-22 22:34-INFO-training batch loss: 0.0615; avg_loss: 1.0153
20-03-22 22:34-INFO-training batch acc: 0.9844; avg_acc: 0.8180
20-03-22 22:34-INFO-
20-03-22 22:34-INFO-Epoch 0, Batch 131, Global step 131:
20-03-22 22:34-INFO-training batch loss: 0.1367; avg_loss: 1.0086
20-03-22 22:34-INFO-training batch acc: 0.9766; avg_acc: 0.8192
20-03-22 22:34-INFO-
20-03-22 22:34-INFO-Epoch 0, Batch 132, Global step 132:
20-03-22 22:34-INFO-training batch loss: 0.1077; avg_loss: 1.0018
20-03-22 22:34-INFO-training batch acc: 0.9766; avg_acc: 0.8204
20-03-22 22:34-INFO-
20-03-22 22:34-INFO-Epoch 0, Batch 133, Global step 133:
20-03-22 22:34-INFO-training batch loss: 0.0805; avg_loss: 0.9948
20-03-22 22:34-INFO-training batch acc: 0.9766; avg_acc: 0.8215
20-03-22 22:34-INFO-
20-03-22 22:34-INFO-Epoch 0, Batch 134, Global step 134:
20-03-22 22:34-INFO-training batch loss: 0.0382; avg_loss: 0.9877
20-03-22 22:34-INFO-training batch acc: 0.9922; avg_acc: 0.8228
20-03-22 22:34-INFO-
20-03-22 22:34-INFO-Epoch 0, Batch 135, Global step 135:
20-03-22 22:34-INFO-training batch loss: 0.0898; avg_loss: 0.9810
20-03-22 22:34-INFO-training batch acc: 0.9766; avg_acc: 0.8240
20-03-22 22:34-INFO-
20-03-22 22:34-INFO-Epoch 0, Batch 136, Global step 136:
20-03-22 22:34-INFO-training batch loss: 0.1613; avg_loss: 0.9750
20-03-22 22:34-INFO-training batch acc: 0.9766; avg_acc: 0.8251
20-03-22 22:34-INFO-
20-03-22 22:34-INFO-Epoch 0, Batch 137, Global step 137:
20-03-22 22:34-INFO-training batch loss: 0.1004; avg_loss: 0.9686
20-03-22 22:34-INFO-training batch acc: 0.9688; avg_acc: 0.8261
20-03-22 22:34-INFO-
20-03-22 22:34-INFO-Epoch 0, Batch 138, Global step 138:
20-03-22 22:34-INFO-training batch loss: 0.1441; avg_loss: 0.9627
20-03-22 22:34-INFO-training batch acc: 0.9609; avg_acc: 0.8271
20-03-22 22:34-INFO-
20-03-22 22:34-INFO-Epoch 0, Batch 139, Global step 139:
20-03-22 22:34-INFO-training batch loss: 0.0505; avg_loss: 0.9561
20-03-22 22:34-INFO-training batch acc: 0.9844; avg_acc: 0.8282
20-03-22 22:34-INFO-
20-03-22 22:34-INFO-Epoch 0, Batch 140, Global step 140:
20-03-22 22:34-INFO-training batch loss: 0.0446; avg_loss: 0.9496
20-03-22 22:34-INFO-training batch acc: 0.9922; avg_acc: 0.8294
20-03-22 22:34-INFO-
20-03-22 22:35-INFO-Epoch 0, Batch 141, Global step 141:
20-03-22 22:35-INFO-training batch loss: 0.1519; avg_loss: 0.9439
20-03-22 22:35-INFO-training batch acc: 0.9609; avg_acc: 0.8303
20-03-22 22:35-INFO-
20-03-22 22:35-INFO-Epoch 0, Batch 142, Global step 142:
20-03-22 22:35-INFO-training batch loss: 0.0386; avg_loss: 0.9375
20-03-22 22:35-INFO-training batch acc: 1.0000; avg_acc: 0.8315
20-03-22 22:35-INFO-
20-03-22 22:35-INFO-Epoch 0, Batch 143, Global step 143:
20-03-22 22:35-INFO-training batch loss: 0.0516; avg_loss: 0.9314
20-03-22 22:35-INFO-training batch acc: 0.9844; avg_acc: 0.8326
20-03-22 22:35-INFO-
20-03-22 22:35-INFO-Epoch 0, Batch 144, Global step 144:
20-03-22 22:35-INFO-training batch loss: 0.1307; avg_loss: 0.9258
20-03-22 22:35-INFO-training batch acc: 0.9688; avg_acc: 0.8336
20-03-22 22:35-INFO-
20-03-22 22:35-INFO-Epoch 0, Batch 145, Global step 145:
20-03-22 22:35-INFO-training batch loss: 0.0694; avg_loss: 0.9199
20-03-22 22:35-INFO-training batch acc: 0.9766; avg_acc: 0.8345
20-03-22 22:35-INFO-
20-03-22 22:35-INFO-Epoch 0, Batch 146, Global step 146:
20-03-22 22:35-INFO-training batch loss: 0.0873; avg_loss: 0.9142
20-03-22 22:35-INFO-training batch acc: 0.9844; avg_acc: 0.8356
20-03-22 22:35-INFO-
20-03-22 22:35-INFO-Epoch 0, Batch 147, Global step 147:
20-03-22 22:35-INFO-training batch loss: 0.0704; avg_loss: 0.9084
20-03-22 22:35-INFO-training batch acc: 0.9922; avg_acc: 0.8366
20-03-22 22:35-INFO-
20-03-22 22:35-INFO-Epoch 0, Batch 148, Global step 148:
20-03-22 22:35-INFO-training batch loss: 0.0744; avg_loss: 0.9028
20-03-22 22:35-INFO-training batch acc: 0.9766; avg_acc: 0.8376
20-03-22 22:35-INFO-
20-03-22 22:35-INFO-Epoch 0, Batch 149, Global step 149:
20-03-22 22:35-INFO-training batch loss: 0.0891; avg_loss: 0.8973
20-03-22 22:35-INFO-training batch acc: 0.9844; avg_acc: 0.8386
20-03-22 22:35-INFO-
20-03-22 22:35-INFO-Epoch 0, Batch 150, Global step 150:
20-03-22 22:35-INFO-training batch loss: 0.0611; avg_loss: 0.8918
20-03-22 22:35-INFO-training batch acc: 0.9609; avg_acc: 0.8394
20-03-22 22:35-INFO-
20-03-22 22:35-INFO-Epoch 0, Batch 151, Global step 151:
20-03-22 22:35-INFO-training batch loss: 0.1081; avg_loss: 0.8866
20-03-22 22:35-INFO-training batch acc: 0.9766; avg_acc: 0.8403
20-03-22 22:35-INFO-
20-03-22 22:35-INFO-Epoch 0, Batch 152, Global step 152:
20-03-22 22:35-INFO-training batch loss: 0.1124; avg_loss: 0.8815
20-03-22 22:35-INFO-training batch acc: 0.9844; avg_acc: 0.8412
20-03-22 22:35-INFO-
20-03-22 22:35-INFO-Epoch 0, Batch 153, Global step 153:
20-03-22 22:35-INFO-training batch loss: 0.1868; avg_loss: 0.8769
20-03-22 22:35-INFO-training batch acc: 0.9609; avg_acc: 0.8420
20-03-22 22:35-INFO-
20-03-22 22:35-INFO-Epoch 0, Batch 154, Global step 154:
20-03-22 22:35-INFO-training batch loss: 0.2115; avg_loss: 0.8726
20-03-22 22:35-INFO-training batch acc: 0.9531; avg_acc: 0.8427
20-03-22 22:35-INFO-
20-03-22 22:35-INFO-Epoch 0, Batch 155, Global step 155:
20-03-22 22:35-INFO-training batch loss: 0.1334; avg_loss: 0.8679
20-03-22 22:35-INFO-training batch acc: 0.9766; avg_acc: 0.8436
20-03-22 22:35-INFO-
20-03-22 22:35-INFO-Epoch 0, Batch 156, Global step 156:
20-03-22 22:35-INFO-training batch loss: 0.0706; avg_loss: 0.8627
20-03-22 22:35-INFO-training batch acc: 0.9922; avg_acc: 0.8446
20-03-22 22:35-INFO-
20-03-22 22:35-INFO-Epoch 0, Batch 157, Global step 157:
20-03-22 22:35-INFO-training batch loss: 0.0481; avg_loss: 0.8576
20-03-22 22:35-INFO-training batch acc: 0.9922; avg_acc: 0.8455
20-03-22 22:35-INFO-
20-03-22 22:35-INFO-Epoch 0, Batch 158, Global step 158:
20-03-22 22:35-INFO-training batch loss: 0.0647; avg_loss: 0.8525
20-03-22 22:35-INFO-training batch acc: 0.9844; avg_acc: 0.8464
20-03-22 22:35-INFO-
20-03-22 22:35-INFO-Epoch 0, Batch 159, Global step 159:
20-03-22 22:35-INFO-training batch loss: 0.0652; avg_loss: 0.8476
20-03-22 22:35-INFO-training batch acc: 0.9922; avg_acc: 0.8473
20-03-22 22:35-INFO-
20-03-22 22:35-INFO-Epoch 0, Batch 160, Global step 160:
20-03-22 22:35-INFO-training batch loss: 0.0991; avg_loss: 0.8429
20-03-22 22:35-INFO-training batch acc: 0.9844; avg_acc: 0.8481
20-03-22 22:35-INFO-
20-03-22 22:35-INFO-Epoch 0, Batch 161, Global step 161:
20-03-22 22:35-INFO-training batch loss: 0.1143; avg_loss: 0.8384
20-03-22 22:35-INFO-training batch acc: 0.9609; avg_acc: 0.8488
20-03-22 22:35-INFO-
20-03-22 22:35-INFO-Epoch 0, Batch 162, Global step 162:
20-03-22 22:35-INFO-training batch loss: 0.0665; avg_loss: 0.8336
20-03-22 22:35-INFO-training batch acc: 0.9688; avg_acc: 0.8496
20-03-22 22:35-INFO-
20-03-22 22:35-INFO-Epoch 0, Batch 163, Global step 163:
20-03-22 22:35-INFO-training batch loss: 0.0758; avg_loss: 0.8290
20-03-22 22:35-INFO-training batch acc: 0.9609; avg_acc: 0.8503
20-03-22 22:35-INFO-
20-03-22 22:35-INFO-Epoch 0, Batch 164, Global step 164:
20-03-22 22:35-INFO-training batch loss: 0.0555; avg_loss: 0.8243
20-03-22 22:35-INFO-training batch acc: 0.9844; avg_acc: 0.8511
20-03-22 22:35-INFO-
20-03-22 22:35-INFO-Epoch 0, Batch 165, Global step 165:
20-03-22 22:35-INFO-training batch loss: 0.0424; avg_loss: 0.8195
20-03-22 22:35-INFO-training batch acc: 0.9844; avg_acc: 0.8519
20-03-22 22:35-INFO-
20-03-22 22:35-INFO-Epoch 0, Batch 166, Global step 166:
20-03-22 22:35-INFO-training batch loss: 0.0333; avg_loss: 0.8148
20-03-22 22:35-INFO-training batch acc: 0.9922; avg_acc: 0.8527
20-03-22 22:35-INFO-
20-03-22 22:36-INFO-Epoch 0, Batch 167, Global step 167:
20-03-22 22:36-INFO-training batch loss: 0.0314; avg_loss: 0.8101
20-03-22 22:36-INFO-training batch acc: 0.9922; avg_acc: 0.8536
20-03-22 22:36-INFO-
20-03-22 22:36-INFO-Epoch 0, Batch 168, Global step 168:
20-03-22 22:36-INFO-training batch loss: 0.0725; avg_loss: 0.8057
20-03-22 22:36-INFO-training batch acc: 0.9766; avg_acc: 0.8543
20-03-22 22:36-INFO-
20-03-22 22:36-INFO-Epoch 0, Batch 169, Global step 169:
20-03-22 22:36-INFO-training batch loss: 0.0777; avg_loss: 0.8014
20-03-22 22:36-INFO-training batch acc: 0.9766; avg_acc: 0.8550
20-03-22 22:36-INFO-
20-03-22 22:36-INFO-Epoch 0, Batch 170, Global step 170:
20-03-22 22:36-INFO-training batch loss: 0.1109; avg_loss: 0.7973
20-03-22 22:36-INFO-training batch acc: 0.9844; avg_acc: 0.8558
20-03-22 22:36-INFO-
20-03-22 22:36-INFO-Epoch 0, Batch 171, Global step 171:
20-03-22 22:36-INFO-training batch loss: 0.0303; avg_loss: 0.7928
20-03-22 22:36-INFO-training batch acc: 0.9922; avg_acc: 0.8566
20-03-22 22:36-INFO-
20-03-22 22:36-INFO-Epoch 0, Batch 172, Global step 172:
20-03-22 22:36-INFO-training batch loss: 0.1276; avg_loss: 0.7890
20-03-22 22:36-INFO-training batch acc: 0.9844; avg_acc: 0.8573
20-03-22 22:36-INFO-
20-03-22 22:36-INFO-Epoch 0, Batch 173, Global step 173:
20-03-22 22:36-INFO-training batch loss: 0.1318; avg_loss: 0.7852
20-03-22 22:36-INFO-training batch acc: 0.9609; avg_acc: 0.8579
20-03-22 22:36-INFO-
20-03-22 22:36-INFO-Epoch 0, Batch 174, Global step 174:
20-03-22 22:36-INFO-training batch loss: 0.0254; avg_loss: 0.7808
20-03-22 22:36-INFO-training batch acc: 0.9922; avg_acc: 0.8587
20-03-22 22:36-INFO-
20-03-22 22:36-INFO-Epoch 0, Batch 175, Global step 175:
20-03-22 22:36-INFO-training batch loss: 0.0262; avg_loss: 0.7765
20-03-22 22:36-INFO-training batch acc: 0.9922; avg_acc: 0.8595
20-03-22 22:36-INFO-
20-03-22 22:36-INFO-Epoch 0, Batch 176, Global step 176:
20-03-22 22:36-INFO-training batch loss: 0.0701; avg_loss: 0.7725
20-03-22 22:36-INFO-training batch acc: 0.9766; avg_acc: 0.8601
20-03-22 22:36-INFO-
20-03-22 22:36-INFO-Epoch 0, Batch 177, Global step 177:
20-03-22 22:36-INFO-training batch loss: 0.0523; avg_loss: 0.7684
20-03-22 22:36-INFO-training batch acc: 0.9766; avg_acc: 0.8608
20-03-22 22:36-INFO-
20-03-22 22:36-INFO-Epoch 0, Batch 178, Global step 178:
20-03-22 22:36-INFO-training batch loss: 0.0563; avg_loss: 0.7644
20-03-22 22:36-INFO-training batch acc: 0.9688; avg_acc: 0.8614
20-03-22 22:36-INFO-
20-03-22 22:36-INFO-Epoch 0, Batch 179, Global step 179:
20-03-22 22:36-INFO-training batch loss: 0.0297; avg_loss: 0.7603
20-03-22 22:36-INFO-training batch acc: 0.9922; avg_acc: 0.8621
20-03-22 22:36-INFO-
20-03-22 22:36-INFO-Epoch 0, Batch 180, Global step 180:
20-03-22 22:36-INFO-training batch loss: 0.0617; avg_loss: 0.7564
20-03-22 22:36-INFO-training batch acc: 0.9922; avg_acc: 0.8628
20-03-22 22:36-INFO-
20-03-22 22:36-INFO-Epoch 0, Batch 181, Global step 181:
20-03-22 22:36-INFO-training batch loss: 0.1332; avg_loss: 0.7530
20-03-22 22:36-INFO-training batch acc: 0.9375; avg_acc: 0.8633
20-03-22 22:36-INFO-
20-03-22 22:36-INFO-Epoch 0, Batch 182, Global step 182:
20-03-22 22:36-INFO-training batch loss: 0.1204; avg_loss: 0.7495
20-03-22 22:36-INFO-training batch acc: 0.9688; avg_acc: 0.8638
20-03-22 22:36-INFO-
20-03-22 22:36-INFO-Epoch 0, Batch 183, Global step 183:
20-03-22 22:36-INFO-training batch loss: 0.0556; avg_loss: 0.7457
20-03-22 22:36-INFO-training batch acc: 0.9922; avg_acc: 0.8645
20-03-22 22:36-INFO-
20-03-22 22:36-INFO-Epoch 0, Batch 184, Global step 184:
20-03-22 22:36-INFO-training batch loss: 0.0918; avg_loss: 0.7422
20-03-22 22:36-INFO-training batch acc: 0.9922; avg_acc: 0.8652
20-03-22 22:36-INFO-
20-03-22 22:36-INFO-Epoch 0, Batch 185, Global step 185:
20-03-22 22:36-INFO-training batch loss: 0.0150; avg_loss: 0.7382
20-03-22 22:36-INFO-training batch acc: 0.9922; avg_acc: 0.8659
20-03-22 22:36-INFO-
20-03-22 22:36-INFO-Epoch 0, Batch 186, Global step 186:
20-03-22 22:36-INFO-training batch loss: 0.0220; avg_loss: 0.7344
20-03-22 22:36-INFO-training batch acc: 0.9922; avg_acc: 0.8666
20-03-22 22:36-INFO-
20-03-22 22:36-INFO-Epoch 0, Batch 187, Global step 187:
20-03-22 22:36-INFO-training batch loss: 0.1272; avg_loss: 0.7311
20-03-22 22:36-INFO-training batch acc: 0.9609; avg_acc: 0.8671
20-03-22 22:36-INFO-
20-03-22 22:36-INFO-Epoch 0, Batch 188, Global step 188:
20-03-22 22:36-INFO-training batch loss: 0.0613; avg_loss: 0.7276
20-03-22 22:36-INFO-training batch acc: 0.9844; avg_acc: 0.8677
20-03-22 22:36-INFO-
20-03-22 22:36-INFO-Epoch 0, Batch 189, Global step 189:
20-03-22 22:36-INFO-training batch loss: 0.0807; avg_loss: 0.7242
20-03-22 22:36-INFO-training batch acc: 0.9844; avg_acc: 0.8683
20-03-22 22:36-INFO-
20-03-22 22:36-INFO-Epoch 0, Batch 190, Global step 190:
20-03-22 22:36-INFO-training batch loss: 0.1133; avg_loss: 0.7209
20-03-22 22:36-INFO-training batch acc: 0.9688; avg_acc: 0.8689
20-03-22 22:36-INFO-
20-03-22 22:36-INFO-Epoch 0, Batch 191, Global step 191:
20-03-22 22:36-INFO-training batch loss: 0.0528; avg_loss: 0.7174
20-03-22 22:36-INFO-training batch acc: 0.9922; avg_acc: 0.8695
20-03-22 22:36-INFO-
20-03-22 22:36-INFO-Epoch 0, Batch 192, Global step 192:
20-03-22 22:36-INFO-training batch loss: 0.0635; avg_loss: 0.7140
20-03-22 22:36-INFO-training batch acc: 0.9922; avg_acc: 0.8702
20-03-22 22:36-INFO-
20-03-22 22:37-INFO-Epoch 0, Batch 193, Global step 193:
20-03-22 22:37-INFO-training batch loss: 0.0856; avg_loss: 0.7108
20-03-22 22:37-INFO-training batch acc: 0.9688; avg_acc: 0.8707
20-03-22 22:37-INFO-
20-03-22 22:37-INFO-Epoch 0, Batch 194, Global step 194:
20-03-22 22:37-INFO-training batch loss: 0.0597; avg_loss: 0.7074
20-03-22 22:37-INFO-training batch acc: 0.9922; avg_acc: 0.8713
20-03-22 22:37-INFO-
20-03-22 22:37-INFO-Epoch 0, Batch 195, Global step 195:
20-03-22 22:37-INFO-training batch loss: 0.0131; avg_loss: 0.7039
20-03-22 22:37-INFO-training batch acc: 1.0000; avg_acc: 0.8720
20-03-22 22:37-INFO-
20-03-22 22:37-INFO-Epoch 0, Batch 196, Global step 196:
20-03-22 22:37-INFO-training batch loss: 0.0891; avg_loss: 0.7007
20-03-22 22:37-INFO-training batch acc: 0.9922; avg_acc: 0.8726
20-03-22 22:37-INFO-
20-03-22 22:37-INFO-Epoch 0, Batch 197, Global step 197:
20-03-22 22:37-INFO-training batch loss: 0.0959; avg_loss: 0.6977
20-03-22 22:37-INFO-training batch acc: 0.9688; avg_acc: 0.8731
20-03-22 22:37-INFO-
20-03-22 22:37-INFO-Epoch 0, Batch 198, Global step 198:
20-03-22 22:37-INFO-training batch loss: 0.0748; avg_loss: 0.6945
20-03-22 22:37-INFO-training batch acc: 0.9766; avg_acc: 0.8736
20-03-22 22:37-INFO-
20-03-22 22:37-INFO-Epoch 0, Batch 199, Global step 199:
20-03-22 22:37-INFO-training batch loss: 0.0678; avg_loss: 0.6914
20-03-22 22:37-INFO-training batch acc: 0.9844; avg_acc: 0.8741
20-03-22 22:37-INFO-
20-03-22 22:37-INFO-Epoch 0, Batch 200, Global step 200:
20-03-22 22:37-INFO-training batch loss: 0.1826; avg_loss: 0.6888
20-03-22 22:37-INFO-training batch acc: 0.9531; avg_acc: 0.8745
20-03-22 22:37-INFO-
20-03-22 22:37-INFO-Epoch 0, Batch 201, Global step 201:
20-03-22 22:37-INFO-training batch loss: 0.0659; avg_loss: 0.6857
20-03-22 22:37-INFO-training batch acc: 0.9844; avg_acc: 0.8751
20-03-22 22:37-INFO-
20-03-22 22:37-INFO-Epoch 0, Batch 202, Global step 202:
20-03-22 22:37-INFO-training batch loss: 0.1108; avg_loss: 0.6829
20-03-22 22:37-INFO-training batch acc: 0.9609; avg_acc: 0.8755
20-03-22 22:37-INFO-
20-03-22 22:37-INFO-Epoch 0, Batch 203, Global step 203:
20-03-22 22:37-INFO-training batch loss: 0.0631; avg_loss: 0.6798
20-03-22 22:37-INFO-training batch acc: 0.9844; avg_acc: 0.8760
20-03-22 22:37-INFO-
20-03-22 22:37-INFO-Epoch 0, Batch 204, Global step 204:
20-03-22 22:37-INFO-training batch loss: 0.1322; avg_loss: 0.6771
20-03-22 22:37-INFO-training batch acc: 0.9688; avg_acc: 0.8765
20-03-22 22:37-INFO-
20-03-22 22:37-INFO-Epoch 0, Batch 205, Global step 205:
20-03-22 22:37-INFO-training batch loss: 0.0601; avg_loss: 0.6741
20-03-22 22:37-INFO-training batch acc: 0.9922; avg_acc: 0.8771
20-03-22 22:37-INFO-
20-03-22 22:37-INFO-Epoch 0, Batch 206, Global step 206:
20-03-22 22:37-INFO-training batch loss: 0.0269; avg_loss: 0.6710
20-03-22 22:37-INFO-training batch acc: 0.9922; avg_acc: 0.8776
20-03-22 22:37-INFO-
20-03-22 22:37-INFO-Epoch 0, Batch 207, Global step 207:
20-03-22 22:37-INFO-training batch loss: 0.0873; avg_loss: 0.6682
20-03-22 22:37-INFO-training batch acc: 0.9766; avg_acc: 0.8781
20-03-22 22:37-INFO-
20-03-22 22:37-INFO-Epoch 0, Batch 208, Global step 208:
20-03-22 22:37-INFO-training batch loss: 0.0929; avg_loss: 0.6654
20-03-22 22:37-INFO-training batch acc: 0.9609; avg_acc: 0.8785
20-03-22 22:37-INFO-
20-03-22 22:37-INFO-Epoch 0, Batch 209, Global step 209:
20-03-22 22:37-INFO-training batch loss: 0.1790; avg_loss: 0.6631
20-03-22 22:37-INFO-training batch acc: 0.9609; avg_acc: 0.8789
20-03-22 22:37-INFO-
20-03-22 22:37-INFO-Epoch 0, Batch 210, Global step 210:
20-03-22 22:37-INFO-training batch loss: 0.0550; avg_loss: 0.6602
20-03-22 22:37-INFO-training batch acc: 0.9844; avg_acc: 0.8794
20-03-22 22:37-INFO-
20-03-22 22:37-INFO-Epoch 0, Batch 211, Global step 211:
20-03-22 22:37-INFO-training batch loss: 0.0547; avg_loss: 0.6573
20-03-22 22:37-INFO-training batch acc: 0.9844; avg_acc: 0.8799
20-03-22 22:37-INFO-
20-03-22 22:37-INFO-Epoch 0, Batch 212, Global step 212:
20-03-22 22:37-INFO-training batch loss: 0.0510; avg_loss: 0.6544
20-03-22 22:37-INFO-training batch acc: 0.9844; avg_acc: 0.8804
20-03-22 22:37-INFO-
20-03-22 22:37-INFO-Epoch 0, Batch 213, Global step 213:
20-03-22 22:37-INFO-training batch loss: 0.1620; avg_loss: 0.6521
20-03-22 22:37-INFO-training batch acc: 0.9609; avg_acc: 0.8808
20-03-22 22:37-INFO-
20-03-22 22:37-INFO-Epoch 0, Batch 214, Global step 214:
20-03-22 22:37-INFO-training batch loss: 0.0397; avg_loss: 0.6493
20-03-22 22:37-INFO-training batch acc: 0.9922; avg_acc: 0.8813
20-03-22 22:37-INFO-
20-03-22 22:37-INFO-Epoch 0, Batch 215, Global step 215:
20-03-22 22:37-INFO-training batch loss: 0.0927; avg_loss: 0.6467
20-03-22 22:37-INFO-training batch acc: 0.9609; avg_acc: 0.8816
20-03-22 22:37-INFO-
20-03-22 22:37-INFO-Epoch 0, Batch 216, Global step 216:
20-03-22 22:37-INFO-training batch loss: 0.0763; avg_loss: 0.6440
20-03-22 22:37-INFO-training batch acc: 0.9922; avg_acc: 0.8822
20-03-22 22:37-INFO-
20-03-22 22:37-INFO-Epoch 0, Batch 217, Global step 217:
20-03-22 22:37-INFO-training batch loss: 0.0468; avg_loss: 0.6413
20-03-22 22:37-INFO-training batch acc: 0.9922; avg_acc: 0.8827
20-03-22 22:37-INFO-
20-03-22 22:37-INFO-Epoch 0, Batch 218, Global step 218:
20-03-22 22:37-INFO-training batch loss: 0.0985; avg_loss: 0.6388
20-03-22 22:37-INFO-training batch acc: 0.9609; avg_acc: 0.8830
20-03-22 22:37-INFO-
20-03-22 22:38-INFO-Epoch 0, Batch 219, Global step 219:
20-03-22 22:38-INFO-training batch loss: 0.0432; avg_loss: 0.6361
20-03-22 22:38-INFO-training batch acc: 0.9844; avg_acc: 0.8835
20-03-22 22:38-INFO-
20-03-22 22:38-INFO-Epoch 0, Batch 220, Global step 220:
20-03-22 22:38-INFO-training batch loss: 0.1185; avg_loss: 0.6337
20-03-22 22:38-INFO-training batch acc: 0.9766; avg_acc: 0.8839
20-03-22 22:38-INFO-
20-03-22 22:38-INFO-Epoch 0, Batch 221, Global step 221:
20-03-22 22:38-INFO-training batch loss: 0.0252; avg_loss: 0.6310
20-03-22 22:38-INFO-training batch acc: 0.9922; avg_acc: 0.8844
20-03-22 22:38-INFO-
20-03-22 22:38-INFO-Epoch 0, Batch 222, Global step 222:
20-03-22 22:38-INFO-training batch loss: 0.0646; avg_loss: 0.6284
20-03-22 22:38-INFO-training batch acc: 0.9688; avg_acc: 0.8848
20-03-22 22:38-INFO-
20-03-22 22:38-INFO-Epoch 0, Batch 223, Global step 223:
20-03-22 22:38-INFO-training batch loss: 0.1597; avg_loss: 0.6263
20-03-22 22:38-INFO-training batch acc: 0.9531; avg_acc: 0.8851
20-03-22 22:38-INFO-
20-03-22 22:38-INFO-Epoch 0, Batch 224, Global step 224:
20-03-22 22:38-INFO-training batch loss: 0.0646; avg_loss: 0.6238
20-03-22 22:38-INFO-training batch acc: 0.9766; avg_acc: 0.8855
20-03-22 22:38-INFO-
20-03-22 22:38-INFO-Epoch 0, Batch 225, Global step 225:
20-03-22 22:38-INFO-training batch loss: 0.0577; avg_loss: 0.6213
20-03-22 22:38-INFO-training batch acc: 0.9844; avg_acc: 0.8859
20-03-22 22:38-INFO-
20-03-22 22:38-INFO-Epoch 0, Batch 226, Global step 226:
20-03-22 22:38-INFO-training batch loss: 0.1575; avg_loss: 0.6192
20-03-22 22:38-INFO-training batch acc: 0.9688; avg_acc: 0.8863
20-03-22 22:38-INFO-
20-03-22 22:38-INFO-Epoch 0, Batch 227, Global step 227:
20-03-22 22:38-INFO-training batch loss: 0.0491; avg_loss: 0.6167
20-03-22 22:38-INFO-training batch acc: 0.9844; avg_acc: 0.8867
20-03-22 22:38-INFO-
20-03-22 22:38-INFO-Epoch 0, Batch 228, Global step 228:
20-03-22 22:38-INFO-training batch loss: 0.0190; avg_loss: 0.6141
20-03-22 22:38-INFO-training batch acc: 0.9922; avg_acc: 0.8872
20-03-22 22:38-INFO-
20-03-22 22:38-INFO-Epoch 0, Batch 229, Global step 229:
20-03-22 22:38-INFO-training batch loss: 0.0421; avg_loss: 0.6116
20-03-22 22:38-INFO-training batch acc: 0.9922; avg_acc: 0.8877
20-03-22 22:38-INFO-
20-03-22 22:38-INFO-Epoch 0, Batch 230, Global step 230:
20-03-22 22:38-INFO-training batch loss: 0.0854; avg_loss: 0.6093
20-03-22 22:38-INFO-training batch acc: 0.9844; avg_acc: 0.8881
20-03-22 22:38-INFO-
20-03-22 22:38-INFO-Epoch 0, Batch 231, Global step 231:
20-03-22 22:38-INFO-training batch loss: 0.1735; avg_loss: 0.6074
20-03-22 22:38-INFO-training batch acc: 0.9609; avg_acc: 0.8884
20-03-22 22:38-INFO-
20-03-22 22:38-INFO-Epoch 0, Batch 232, Global step 232:
20-03-22 22:38-INFO-training batch loss: 0.0436; avg_loss: 0.6050
20-03-22 22:38-INFO-training batch acc: 0.9922; avg_acc: 0.8888
20-03-22 22:38-INFO-
20-03-22 22:38-INFO-Epoch 0, Batch 233, Global step 233:
20-03-22 22:38-INFO-training batch loss: 0.0778; avg_loss: 0.6027
20-03-22 22:38-INFO-training batch acc: 0.9844; avg_acc: 0.8893
20-03-22 22:38-INFO-
20-03-22 22:38-INFO-Epoch 0, Batch 234, Global step 234:
20-03-22 22:38-INFO-training batch loss: 0.0858; avg_loss: 0.6005
20-03-22 22:38-INFO-training batch acc: 0.9922; avg_acc: 0.8897
20-03-22 22:38-INFO-
20-03-22 22:38-INFO-Epoch 0, Batch 235, Global step 235:
20-03-22 22:38-INFO-training batch loss: 0.0907; avg_loss: 0.5984
20-03-22 22:38-INFO-training batch acc: 0.9609; avg_acc: 0.8900
20-03-22 22:38-INFO-
20-03-22 22:38-INFO-Epoch 0, Batch 236, Global step 236:
20-03-22 22:38-INFO-training batch loss: 0.0724; avg_loss: 0.5961
20-03-22 22:38-INFO-training batch acc: 0.9688; avg_acc: 0.8903
20-03-22 22:38-INFO-
20-03-22 22:38-INFO-Epoch 0, Batch 237, Global step 237:
20-03-22 22:38-INFO-training batch loss: 0.0752; avg_loss: 0.5939
20-03-22 22:38-INFO-training batch acc: 0.9844; avg_acc: 0.8907
20-03-22 22:38-INFO-
20-03-22 22:38-INFO-Epoch 0, Batch 238, Global step 238:
20-03-22 22:38-INFO-training batch loss: 0.0863; avg_loss: 0.5918
20-03-22 22:38-INFO-training batch acc: 0.9688; avg_acc: 0.8911
20-03-22 22:38-INFO-
20-03-22 22:38-INFO-Epoch 0, Batch 239, Global step 239:
20-03-22 22:38-INFO-training batch loss: 0.0731; avg_loss: 0.5896
20-03-22 22:38-INFO-training batch acc: 0.9766; avg_acc: 0.8914
20-03-22 22:38-INFO-
20-03-22 22:38-INFO-Epoch 0, Batch 240, Global step 240:
20-03-22 22:38-INFO-training batch loss: 0.0881; avg_loss: 0.5875
20-03-22 22:38-INFO-training batch acc: 0.9766; avg_acc: 0.8918
20-03-22 22:38-INFO-
20-03-22 22:38-INFO-Epoch 0, Batch 241, Global step 241:
20-03-22 22:38-INFO-training batch loss: 0.0703; avg_loss: 0.5854
20-03-22 22:38-INFO-training batch acc: 0.9688; avg_acc: 0.8921
20-03-22 22:38-INFO-
20-03-22 22:38-INFO-Epoch 0, Batch 242, Global step 242:
20-03-22 22:38-INFO-training batch loss: 0.0405; avg_loss: 0.5831
20-03-22 22:38-INFO-training batch acc: 0.9844; avg_acc: 0.8925
20-03-22 22:38-INFO-
20-03-22 22:38-INFO-Epoch 0, Batch 243, Global step 243:
20-03-22 22:38-INFO-training batch loss: 0.0415; avg_loss: 0.5809
20-03-22 22:38-INFO-training batch acc: 0.9922; avg_acc: 0.8929
20-03-22 22:38-INFO-
20-03-22 22:38-INFO-Epoch 0, Batch 244, Global step 244:
20-03-22 22:38-INFO-training batch loss: 0.0566; avg_loss: 0.5788
20-03-22 22:38-INFO-training batch acc: 0.9844; avg_acc: 0.8933
20-03-22 22:38-INFO-
20-03-22 22:38-INFO-Epoch 0, Batch 245, Global step 245:
20-03-22 22:38-INFO-training batch loss: 0.0284; avg_loss: 0.5765
20-03-22 22:38-INFO-training batch acc: 1.0000; avg_acc: 0.8937
20-03-22 22:38-INFO-
20-03-22 22:39-INFO-Epoch 0, Batch 246, Global step 246:
20-03-22 22:39-INFO-training batch loss: 0.0270; avg_loss: 0.5743
20-03-22 22:39-INFO-training batch acc: 0.9922; avg_acc: 0.8941
20-03-22 22:39-INFO-
20-03-22 22:39-INFO-Epoch 0, Batch 247, Global step 247:
20-03-22 22:39-INFO-training batch loss: 0.0235; avg_loss: 0.5721
20-03-22 22:39-INFO-training batch acc: 1.0000; avg_acc: 0.8945
20-03-22 22:39-INFO-
20-03-22 22:39-INFO-Epoch 0, Batch 248, Global step 248:
20-03-22 22:39-INFO-training batch loss: 0.0932; avg_loss: 0.5701
20-03-22 22:39-INFO-training batch acc: 0.9844; avg_acc: 0.8949
20-03-22 22:39-INFO-
20-03-22 22:39-INFO-Epoch 0, Batch 249, Global step 249:
20-03-22 22:39-INFO-training batch loss: 0.0177; avg_loss: 0.5679
20-03-22 22:39-INFO-training batch acc: 1.0000; avg_acc: 0.8953
20-03-22 22:39-INFO-
20-03-22 22:39-INFO-Epoch 0, Batch 250, Global step 250:
20-03-22 22:39-INFO-training batch loss: 0.1024; avg_loss: 0.5660
20-03-22 22:39-INFO-training batch acc: 0.9766; avg_acc: 0.8956
20-03-22 22:39-INFO-
20-03-22 22:39-INFO-Epoch 0, Batch 251, Global step 251:
20-03-22 22:39-INFO-training batch loss: 0.0758; avg_loss: 0.5641
20-03-22 22:39-INFO-training batch acc: 0.9766; avg_acc: 0.8959
20-03-22 22:39-INFO-
20-03-22 22:39-INFO-Epoch 0, Batch 252, Global step 252:
20-03-22 22:39-INFO-training batch loss: 0.0349; avg_loss: 0.5620
20-03-22 22:39-INFO-training batch acc: 0.9922; avg_acc: 0.8963
20-03-22 22:39-INFO-
20-03-22 22:39-INFO-Epoch 0, Batch 253, Global step 253:
20-03-22 22:39-INFO-training batch loss: 0.0481; avg_loss: 0.5600
20-03-22 22:39-INFO-training batch acc: 0.9922; avg_acc: 0.8967
20-03-22 22:39-INFO-
20-03-22 22:39-INFO-Epoch 0, Batch 254, Global step 254:
20-03-22 22:39-INFO-training batch loss: 0.0525; avg_loss: 0.5580
20-03-22 22:39-INFO-training batch acc: 0.9844; avg_acc: 0.8971
20-03-22 22:39-INFO-
20-03-22 22:39-INFO-Epoch 0, Batch 255, Global step 255:
20-03-22 22:39-INFO-training batch loss: 0.0355; avg_loss: 0.5559
20-03-22 22:39-INFO-training batch acc: 0.9844; avg_acc: 0.8974
20-03-22 22:39-INFO-
20-03-22 22:39-INFO-Epoch 0, Batch 256, Global step 256:
20-03-22 22:39-INFO-training batch loss: 0.0753; avg_loss: 0.5540
20-03-22 22:39-INFO-training batch acc: 0.9688; avg_acc: 0.8977
20-03-22 22:39-INFO-
20-03-22 22:39-INFO-Epoch 0, Batch 257, Global step 257:
20-03-22 22:39-INFO-training batch loss: 0.0464; avg_loss: 0.5521
20-03-22 22:39-INFO-training batch acc: 0.9922; avg_acc: 0.8980
20-03-22 22:39-INFO-
20-03-22 22:39-INFO-Epoch 0, Batch 258, Global step 258:
20-03-22 22:39-INFO-training batch loss: 0.0265; avg_loss: 0.5500
20-03-22 22:39-INFO-training batch acc: 1.0000; avg_acc: 0.8984
20-03-22 22:39-INFO-
20-03-22 22:39-INFO-Epoch 0, Batch 259, Global step 259:
20-03-22 22:39-INFO-training batch loss: 0.0280; avg_loss: 0.5480
20-03-22 22:39-INFO-training batch acc: 0.9922; avg_acc: 0.8988
20-03-22 22:39-INFO-
20-03-22 22:39-INFO-Epoch 0, Batch 260, Global step 260:
20-03-22 22:39-INFO-training batch loss: 0.0241; avg_loss: 0.5460
20-03-22 22:39-INFO-training batch acc: 0.9922; avg_acc: 0.8992
20-03-22 22:39-INFO-
20-03-22 22:39-INFO-Epoch 0, Batch 261, Global step 261:
20-03-22 22:39-INFO-training batch loss: 0.0244; avg_loss: 0.5440
20-03-22 22:39-INFO-training batch acc: 1.0000; avg_acc: 0.8995
20-03-22 22:39-INFO-
20-03-22 22:39-INFO-Epoch 0, Batch 262, Global step 262:
20-03-22 22:39-INFO-training batch loss: 0.0625; avg_loss: 0.5422
20-03-22 22:39-INFO-training batch acc: 0.9844; avg_acc: 0.8999
20-03-22 22:39-INFO-
20-03-22 22:39-INFO-Epoch 0, Batch 263, Global step 263:
20-03-22 22:39-INFO-training batch loss: 0.0475; avg_loss: 0.5403
20-03-22 22:39-INFO-training batch acc: 0.9922; avg_acc: 0.9002
20-03-22 22:39-INFO-
20-03-22 22:39-INFO-Epoch 0, Batch 264, Global step 264:
20-03-22 22:39-INFO-training batch loss: 0.0205; avg_loss: 0.5383
20-03-22 22:39-INFO-training batch acc: 1.0000; avg_acc: 0.9006
20-03-22 22:39-INFO-
20-03-22 22:39-INFO-Epoch 0, Batch 265, Global step 265:
20-03-22 22:39-INFO-training batch loss: 0.0929; avg_loss: 0.5366
20-03-22 22:39-INFO-training batch acc: 0.9844; avg_acc: 0.9009
20-03-22 22:39-INFO-
20-03-22 22:39-INFO-Epoch 0, Batch 266, Global step 266:
20-03-22 22:39-INFO-training batch loss: 0.0612; avg_loss: 0.5348
20-03-22 22:39-INFO-training batch acc: 0.9688; avg_acc: 0.9012
20-03-22 22:39-INFO-
20-03-22 22:39-INFO-Epoch 0, Batch 267, Global step 267:
20-03-22 22:39-INFO-training batch loss: 0.0373; avg_loss: 0.5330
20-03-22 22:39-INFO-training batch acc: 0.9844; avg_acc: 0.9015
20-03-22 22:39-INFO-
20-03-22 22:39-INFO-Epoch 0, Batch 268, Global step 268:
20-03-22 22:39-INFO-training batch loss: 0.0179; avg_loss: 0.5311
20-03-22 22:39-INFO-training batch acc: 1.0000; avg_acc: 0.9018
20-03-22 22:39-INFO-
20-03-22 22:39-INFO-Epoch 0, Batch 269, Global step 269:
20-03-22 22:39-INFO-training batch loss: 0.0046; avg_loss: 0.5291
20-03-22 22:39-INFO-training batch acc: 1.0000; avg_acc: 0.9022
20-03-22 22:39-INFO-
20-03-22 22:39-INFO-Epoch 0, Batch 270, Global step 270:
20-03-22 22:39-INFO-training batch loss: 0.0058; avg_loss: 0.5272
20-03-22 22:39-INFO-training batch acc: 1.0000; avg_acc: 0.9026
20-03-22 22:39-INFO-
20-03-22 22:40-INFO-Epoch 0, Batch 271, Global step 271:
20-03-22 22:40-INFO-training batch loss: 0.0693; avg_loss: 0.5255
20-03-22 22:40-INFO-training batch acc: 0.9688; avg_acc: 0.9028
20-03-22 22:40-INFO-
20-03-22 22:40-INFO-Epoch 0, Batch 272, Global step 272:
20-03-22 22:40-INFO-training batch loss: 0.0967; avg_loss: 0.5239
20-03-22 22:40-INFO-training batch acc: 0.9688; avg_acc: 0.9031
20-03-22 22:40-INFO-
20-03-22 22:40-INFO-Epoch 0, Batch 273, Global step 273:
20-03-22 22:40-INFO-training batch loss: 0.0406; avg_loss: 0.5221
20-03-22 22:40-INFO-training batch acc: 0.9766; avg_acc: 0.9033
20-03-22 22:40-INFO-
20-03-22 22:40-INFO-Epoch 0, Batch 274, Global step 274:
20-03-22 22:40-INFO-training batch loss: 0.0340; avg_loss: 0.5203
20-03-22 22:40-INFO-training batch acc: 0.9922; avg_acc: 0.9037
20-03-22 22:40-INFO-
20-03-22 22:40-INFO-Epoch 0, Batch 275, Global step 275:
20-03-22 22:40-INFO-training batch loss: 0.0409; avg_loss: 0.5186
20-03-22 22:40-INFO-training batch acc: 0.9844; avg_acc: 0.9039
20-03-22 22:40-INFO-
20-03-22 22:40-INFO-Epoch 0, Batch 276, Global step 276:
20-03-22 22:40-INFO-training batch loss: 0.0487; avg_loss: 0.5169
20-03-22 22:40-INFO-training batch acc: 0.9844; avg_acc: 0.9042
20-03-22 22:40-INFO-
20-03-22 22:40-INFO-Epoch 0, Batch 277, Global step 277:
20-03-22 22:40-INFO-training batch loss: 0.0404; avg_loss: 0.5152
20-03-22 22:40-INFO-training batch acc: 0.9922; avg_acc: 0.9046
20-03-22 22:40-INFO-
20-03-22 22:40-INFO-Epoch 0, Batch 278, Global step 278:
20-03-22 22:40-INFO-training batch loss: 0.0176; avg_loss: 0.5134
20-03-22 22:40-INFO-training batch acc: 1.0000; avg_acc: 0.9049
20-03-22 22:40-INFO-
20-03-22 22:40-INFO-Epoch 0, Batch 279, Global step 279:
20-03-22 22:40-INFO-training batch loss: 0.0223; avg_loss: 0.5116
20-03-22 22:40-INFO-training batch acc: 0.9922; avg_acc: 0.9052
20-03-22 22:40-INFO-
20-03-22 22:40-INFO-Epoch 0, Batch 280, Global step 280:
20-03-22 22:40-INFO-training batch loss: 0.0287; avg_loss: 0.5099
20-03-22 22:40-INFO-training batch acc: 0.9844; avg_acc: 0.9055
20-03-22 22:40-INFO-
20-03-22 22:40-INFO-Epoch 0, Batch 281, Global step 281:
20-03-22 22:40-INFO-training batch loss: 0.0029; avg_loss: 0.5081
20-03-22 22:40-INFO-training batch acc: 1.0000; avg_acc: 0.9058
20-03-22 22:40-INFO-
20-03-22 22:40-INFO-Epoch 0, Batch 282, Global step 282:
20-03-22 22:40-INFO-training batch loss: 0.0075; avg_loss: 0.5063
20-03-22 22:40-INFO-training batch acc: 1.0000; avg_acc: 0.9062
20-03-22 22:40-INFO-
20-03-22 22:40-INFO-Epoch 0, Batch 283, Global step 283:
20-03-22 22:40-INFO-training batch loss: 0.0082; avg_loss: 0.5046
20-03-22 22:40-INFO-training batch acc: 0.9922; avg_acc: 0.9065
20-03-22 22:40-INFO-
20-03-22 22:40-INFO-Epoch 0, Batch 284, Global step 284:
20-03-22 22:40-INFO-training batch loss: 0.0070; avg_loss: 0.5028
20-03-22 22:40-INFO-training batch acc: 1.0000; avg_acc: 0.9068
20-03-22 22:40-INFO-
20-03-22 22:40-INFO-Epoch 0, training batch loss: 0.0070; avg_loss: 0.5028
20-03-22 22:40-INFO-Epoch 0, training batch accuracy: 1.0000; avg_accuracy: 0.9068
20-03-22 22:40-INFO-
